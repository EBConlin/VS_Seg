{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27933df7-8a32-46a9-a2bd-e5f0d762e2a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3840997287.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 31\u001b[0;36m\u001b[0m\n\u001b[0;31m    model.load_state_dict(torch.load(os.path.join(, model_path)))\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import argparse\n",
    "import monai\n",
    "\n",
    "from params.VSparams import VSparams\n",
    "\n",
    "def create_mask(model_path,n=1,t=\"T1\"):\n",
    "    # read parsed arguments\n",
    "    parser = argparse.ArgumentParser(description=\"Train the model\")\n",
    "\n",
    "    # initialize parameters\n",
    "    p = VSparams(parser)\n",
    "    \n",
    "    train_files, val_files, test_files = p.load_samples(n,t)\n",
    "\n",
    "    # define the transforms\n",
    "    train_transforms, val_transforms, test_transforms = p.get_transforms()\n",
    "    \n",
    "    # Set deterministic training for reproducibility\n",
    "    monai.utils.set_determinism(seed=0)\n",
    "    \n",
    "    # cache and load validation data\n",
    "    test_loader = p.cache_transformed_test_data(test_files, test_transforms)\n",
    "    \n",
    "    # create UNet\n",
    "    model = p.set_and_get_model()\n",
    "    \n",
    "    # load the trained state of the model\n",
    "    model.load_state_dict(torch.load(os.path.join(, model_path)))\n",
    "    \n",
    "    # run inference and create figures in figures folder\n",
    "    model.eval()  # activate evaluation mode of model\n",
    "\n",
    "    model_segmentation = model\n",
    "    final_outputs = []\n",
    "    with torch.no_grad():  # turns off PyTorch's auto grad for better performance\n",
    "    for i, data in enumerate(test_loader):\n",
    "\n",
    "        outputs = sliding_window_inference(\n",
    "            inputs=data[\"image\"].to(self.device),\n",
    "            roi_size=self.sliding_window_inferer_roi_size,\n",
    "            sw_batch_size=1,\n",
    "            predictor=model_segmentation,\n",
    "            mode=\"gaussian\",\n",
    "        )\n",
    "\n",
    "        full_outputs.append(outputs)\n",
    "\n",
    "\n",
    "    return full_outputs\n",
    "\n",
    "\n",
    "monai.config.print_config()\n",
    "mask = create_mask(\"/projectnb/cs585bp/students/econlin/VS_Seg-master/MODEL/best_metric_model.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413081c-fe47-4fca-b127-f292b4b257fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Specify the path to the script\n",
    "script_path = \"VS_inference.py\"\n",
    "\n",
    "# Create a module spec\n",
    "spec = importlib.util.spec_from_file_location(\"script_module\", script_path)\n",
    "# Create a module based on the spec\n",
    "script_module = importlib.util.module_from_spec(spec)\n",
    "# Add the module to sys.modules\n",
    "sys.modules[\"script_module\"] = script_module\n",
    "# Execute the module\n",
    "spec.loader.exec_module(script_module)\n",
    "\n",
    "# Now you can access variables from the script\n",
    "result_variable = script_module.your_variable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c5d7b2b-988a-4653-a48d-f1c095695ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"results.pkl\",\"rb\") as f:\n",
    "    original = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f46a2517-757a-4097-bcec-e86e98e1573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_safe_bounds(c,margin,axis):\n",
    "    if c-margin<0:\n",
    "        indices = (0,96)\n",
    "    elif c+margin > axis-1:\n",
    "        indices = (axis-97,axis-1)\n",
    "    else:\n",
    "        indices = (c-48,c+48)\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def extract_patch(volume, center, margin=48):\n",
    "    \"\"\"\n",
    "    Extract a 96x96x96 patch centered at (x, y, z) from a 3D volume.\n",
    "    Pads with zeros if the patch goes out of bounds.\n",
    "    \n",
    "    Args:\n",
    "        volume (np.ndarray): 3D array with shape (Z, Y, X)\n",
    "        center (tuple): (x, y, z) center coordinate\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Patch of shape (96, 96, 96)\n",
    "    \"\"\"\n",
    "    x_max,y_max,z_max = volume.shape\n",
    "    \n",
    "    x,y,z = center\n",
    "\n",
    "    (x1,x2) = get_safe_bounds(x,margin,x_max)\n",
    "    (y1,y2) = get_safe_bounds(y,margin,y_max)\n",
    "    (z1,z2) = get_safe_bounds(z,margin,z_max)\n",
    "\n",
    "    bounds = ((x1,x2),(y1,y2),(z1,z2))\n",
    "    return (volume[y1:y2,x1:x2,z1:x2],bounds)\n",
    "results = original[0].cpu().numpy()\n",
    "c = results[0,1]\n",
    "c[c<0]=0\n",
    "\n",
    "max_slice_idx = np.argmax(np.sum(c,axis=(0,1)),axis=-1)\n",
    "max_slice = c[:,:,max_slice_idx]\n",
    "\n",
    "max_idx = np.argmax(max_slice)\n",
    "\n",
    "# Convert flat index to 2D coordinates (row, column)\n",
    "y, x = np.unravel_index(max_idx, max_slice.shape)\n",
    "\n",
    "patch,bounds = extract_patch(results[0,0],(x,y,max_slice_idx))\n",
    "\n",
    "data_bounds[i] = bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "63392ee5-964b-4c9e-82c1-f1984b2385ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x152ce1964400>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test[:,:,max_slice_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b473328-2a99-4363-976e-5ddeb25166ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "def read_nifti_file(file_path):\n",
    "    \"\"\"\n",
    "    Custom function to read NIfTI files since monai.data.read_nifti is not available in MONAI 0.4.0\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NIfTI file\n",
    "        \n",
    "    Returns:\n",
    "        numpy array containing the data\n",
    "    \"\"\"\n",
    "    nifti_img = nib.load(file_path)\n",
    "    return nifti_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "158bbbf8-9740-4ab0-a9d9-807ddafd2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/t2_masks/0_509_510_119_mask.nii.gz\"\n",
    "\n",
    "results = read_nifti_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1af92cd5-cce6-4a3e-8a15-bdb1f1dd2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data_bounds.pkl\",\"rb\") as f:\n",
    "    r = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5acd6d9-382a-466d-87dd-66e6cdc65607",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SimpleITK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-083a0dc1974c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleITK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msitk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SimpleITK'"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5656cc0d-fa2f-4e85-a8a8-95bbe05f1a03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-33d3789e08fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Convert flat index to 2D coordinates (row, column)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_slice_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"results.pkl\",\"rb\") as f:\n",
    "    r = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "patient_mask = r[0].cpu().numpy()\n",
    "                \n",
    "c = patient_mask[0]\n",
    "c[c<0]=0\n",
    "max_slice_idx = np.argmax(np.sum(c,axis=(0,1)),axis=-1)\n",
    "max_slice = c[:,:,max_slice_idx]\n",
    "\n",
    "max_idx = np.argmax(max_slice)\n",
    "\n",
    "# Convert flat index to 2D coordinates (row, column)\n",
    "y, x = np.unravel_index(max_idx, max_slice.shape)\n",
    "\n",
    "patch = extract_patch(c,x,y,max_slice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8cf28c-43af-4e61-bc9c-090719fccaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/cs585bp/students/econlin/.conda/envs/shapey/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/projectnb/cs585bp/students/econlin/.conda/envs/shapey/lib/python3.6/site-packages/torch/cuda/__init__.py:143: UserWarning: \n",
      "NVIDIA L40S with CUDA capability sm_89 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA L40S GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a20cfb9a-e871-4e15-8d87-864c40644421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x152ccabe36a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "b = r[0].cpu().numpy()\n",
    "avg = np.mean(b[0,0],axis=-1)\n",
    "plt.imshow(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9d4333-d117-46cb-adb3-f8365450bc45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m one_hot\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHardnessWeightedDiceLoss\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, smooth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, hardness_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d98dc-93c3-4771-897f-5dd58cdf2a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|| 176/176 [00:07<00:00, 22.64it/s]\n",
      "Loading dataset: 100%|| 20/20 [00:00<00:00, 21.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/300\n",
      "1/88, train_loss: 0.7001\n",
      "2/88, train_loss: 0.6695\n",
      "3/88, train_loss: 0.6527\n",
      "4/88, train_loss: 0.6380\n",
      "5/88, train_loss: 0.6299\n",
      "6/88, train_loss: 0.6207\n",
      "7/88, train_loss: 0.6127\n",
      "8/88, train_loss: 0.6086\n",
      "9/88, train_loss: 0.6055\n",
      "10/88, train_loss: 0.6014\n",
      "11/88, train_loss: 0.5990\n",
      "12/88, train_loss: 0.5963\n",
      "13/88, train_loss: 0.5951\n",
      "14/88, train_loss: 0.5936\n",
      "15/88, train_loss: 0.5925\n",
      "16/88, train_loss: 0.5907\n",
      "17/88, train_loss: 0.5902\n",
      "18/88, train_loss: 0.5899\n",
      "19/88, train_loss: 0.5882\n",
      "20/88, train_loss: 0.5874\n",
      "21/88, train_loss: 0.5866\n",
      "22/88, train_loss: 0.5864\n",
      "23/88, train_loss: 0.5864\n",
      "24/88, train_loss: 0.5854\n",
      "25/88, train_loss: 0.5851\n",
      "26/88, train_loss: 0.5835\n",
      "27/88, train_loss: 0.5847\n",
      "28/88, train_loss: 0.5828\n",
      "29/88, train_loss: 0.5825\n",
      "30/88, train_loss: 0.5830\n",
      "31/88, train_loss: 0.5816\n",
      "32/88, train_loss: 0.5817\n",
      "33/88, train_loss: 0.5809\n",
      "34/88, train_loss: 0.5803\n",
      "35/88, train_loss: 0.5801\n",
      "36/88, train_loss: 0.5795\n",
      "37/88, train_loss: 0.5786\n",
      "38/88, train_loss: 0.5782\n",
      "39/88, train_loss: 0.5776\n",
      "40/88, train_loss: 0.5777\n",
      "41/88, train_loss: 0.5768\n",
      "42/88, train_loss: 0.5766\n",
      "43/88, train_loss: 0.5760\n",
      "44/88, train_loss: 0.5753\n",
      "45/88, train_loss: 0.5753\n",
      "46/88, train_loss: 0.5747\n",
      "47/88, train_loss: 0.5741\n",
      "48/88, train_loss: 0.5736\n",
      "49/88, train_loss: 0.5735\n",
      "50/88, train_loss: 0.5728\n",
      "51/88, train_loss: 0.5723\n",
      "52/88, train_loss: 0.5721\n",
      "53/88, train_loss: 0.5720\n",
      "54/88, train_loss: 0.5713\n",
      "55/88, train_loss: 0.5707\n",
      "56/88, train_loss: 0.5701\n",
      "57/88, train_loss: 0.5695\n",
      "58/88, train_loss: 0.5691\n",
      "59/88, train_loss: 0.5691\n",
      "60/88, train_loss: 0.5686\n",
      "61/88, train_loss: 0.5679\n",
      "62/88, train_loss: 0.5670\n",
      "63/88, train_loss: 0.5669\n",
      "64/88, train_loss: 0.5658\n",
      "65/88, train_loss: 0.5660\n",
      "66/88, train_loss: 0.5655\n",
      "67/88, train_loss: 0.5644\n",
      "68/88, train_loss: 0.5643\n",
      "69/88, train_loss: 0.5640\n",
      "70/88, train_loss: 0.5633\n",
      "71/88, train_loss: 0.5630\n",
      "72/88, train_loss: 0.5624\n",
      "73/88, train_loss: 0.5617\n",
      "74/88, train_loss: 0.5612\n",
      "75/88, train_loss: 0.5608\n",
      "76/88, train_loss: 0.5603\n",
      "77/88, train_loss: 0.5596\n",
      "78/88, train_loss: 0.5586\n",
      "79/88, train_loss: 0.5579\n",
      "80/88, train_loss: 0.5572\n",
      "81/88, train_loss: 0.5565\n",
      "82/88, train_loss: 0.5551\n",
      "83/88, train_loss: 0.5543\n",
      "84/88, train_loss: 0.5526\n",
      "85/88, train_loss: 0.5516\n",
      "86/88, train_loss: 0.5511\n",
      "87/88, train_loss: 0.5498\n",
      "88/88, train_loss: 0.5488\n",
      "Epoch 1 average loss: 0.5800\n",
      "\n",
      "Epoch 2/300\n",
      "1/88, train_loss: 0.5473\n",
      "2/88, train_loss: 0.5475\n",
      "3/88, train_loss: 0.5464\n",
      "4/88, train_loss: 0.5454\n",
      "5/88, train_loss: 0.5452\n",
      "6/88, train_loss: 0.5428\n",
      "7/88, train_loss: 0.5434\n",
      "8/88, train_loss: 0.5432\n",
      "9/88, train_loss: 0.5424\n",
      "10/88, train_loss: 0.5396\n",
      "11/88, train_loss: 0.5406\n",
      "12/88, train_loss: 0.5371\n",
      "13/88, train_loss: 0.5358\n",
      "14/88, train_loss: 0.5398\n",
      "15/88, train_loss: 0.5395\n",
      "16/88, train_loss: 0.5391\n",
      "17/88, train_loss: 0.5379\n",
      "18/88, train_loss: 0.5350\n",
      "19/88, train_loss: 0.5368\n",
      "20/88, train_loss: 0.5363\n",
      "21/88, train_loss: 0.5362\n",
      "22/88, train_loss: 0.5351\n",
      "23/88, train_loss: 0.5331\n",
      "24/88, train_loss: 0.5342\n",
      "25/88, train_loss: 0.5344\n",
      "26/88, train_loss: 0.5330\n",
      "27/88, train_loss: 0.5337\n",
      "28/88, train_loss: 0.5334\n",
      "29/88, train_loss: 0.5330\n",
      "30/88, train_loss: 0.5329\n",
      "31/88, train_loss: 0.5325\n",
      "32/88, train_loss: 0.5320\n",
      "33/88, train_loss: 0.5318\n",
      "34/88, train_loss: 0.5316\n",
      "35/88, train_loss: 0.5311\n",
      "36/88, train_loss: 0.5308\n",
      "37/88, train_loss: 0.5307\n",
      "38/88, train_loss: 0.5301\n",
      "39/88, train_loss: 0.5291\n",
      "40/88, train_loss: 0.5301\n",
      "41/88, train_loss: 0.5277\n",
      "42/88, train_loss: 0.5294\n",
      "43/88, train_loss: 0.5294\n",
      "44/88, train_loss: 0.5290\n",
      "45/88, train_loss: 0.5286\n",
      "46/88, train_loss: 0.5286\n",
      "47/88, train_loss: 0.5282\n",
      "48/88, train_loss: 0.5286\n",
      "49/88, train_loss: 0.5279\n",
      "50/88, train_loss: 0.5277\n",
      "51/88, train_loss: 0.5260\n",
      "52/88, train_loss: 0.5262\n",
      "53/88, train_loss: 0.5273\n",
      "54/88, train_loss: 0.5270\n",
      "55/88, train_loss: 0.5271\n",
      "56/88, train_loss: 0.5268\n",
      "57/88, train_loss: 0.5266\n",
      "58/88, train_loss: 0.5254\n",
      "59/88, train_loss: 0.5230\n",
      "60/88, train_loss: 0.5259\n",
      "61/88, train_loss: 0.5244\n",
      "62/88, train_loss: 0.5230\n",
      "63/88, train_loss: 0.5267\n",
      "64/88, train_loss: 0.5260\n",
      "65/88, train_loss: 0.5238\n",
      "66/88, train_loss: 0.5248\n",
      "67/88, train_loss: 0.5254\n",
      "68/88, train_loss: 0.5233\n",
      "69/88, train_loss: 0.5233\n",
      "70/88, train_loss: 0.5249\n",
      "71/88, train_loss: 0.5196\n",
      "72/88, train_loss: 0.5244\n",
      "73/88, train_loss: 0.5253\n",
      "74/88, train_loss: 0.5244\n",
      "75/88, train_loss: 0.5247\n",
      "76/88, train_loss: 0.5241\n",
      "77/88, train_loss: 0.5240\n",
      "78/88, train_loss: 0.5229\n",
      "79/88, train_loss: 0.5237\n",
      "80/88, train_loss: 0.5237\n",
      "81/88, train_loss: 0.5235\n",
      "82/88, train_loss: 0.5218\n",
      "83/88, train_loss: 0.5223\n",
      "84/88, train_loss: 0.5231\n",
      "85/88, train_loss: 0.5228\n",
      "86/88, train_loss: 0.5230\n",
      "87/88, train_loss: 0.5227\n",
      "88/88, train_loss: 0.5225\n",
      "Epoch 2 average loss: 0.5305\n",
      "\n",
      "Epoch 3/300\n",
      "1/88, train_loss: 0.5224\n",
      "2/88, train_loss: 0.5226\n",
      "3/88, train_loss: 0.5223\n",
      "4/88, train_loss: 0.5202\n",
      "5/88, train_loss: 0.5221\n",
      "6/88, train_loss: 0.5218\n",
      "7/88, train_loss: 0.5216\n",
      "8/88, train_loss: 0.5218\n",
      "9/88, train_loss: 0.5215\n",
      "10/88, train_loss: 0.5216\n",
      "11/88, train_loss: 0.5213\n",
      "12/88, train_loss: 0.5214\n",
      "13/88, train_loss: 0.5177\n",
      "14/88, train_loss: 0.5198\n",
      "15/88, train_loss: 0.5189\n",
      "16/88, train_loss: 0.5211\n",
      "17/88, train_loss: 0.5218\n",
      "18/88, train_loss: 0.5192\n",
      "19/88, train_loss: 0.5208\n",
      "20/88, train_loss: 0.5209\n",
      "21/88, train_loss: 0.5199\n",
      "22/88, train_loss: 0.5179\n",
      "23/88, train_loss: 0.5172\n",
      "24/88, train_loss: 0.5177\n",
      "25/88, train_loss: 0.5208\n",
      "26/88, train_loss: 0.5156\n",
      "27/88, train_loss: 0.5212\n",
      "28/88, train_loss: 0.5207\n",
      "29/88, train_loss: 0.5206\n",
      "30/88, train_loss: 0.5174\n",
      "31/88, train_loss: 0.5201\n",
      "32/88, train_loss: 0.5198\n",
      "33/88, train_loss: 0.5195\n",
      "34/88, train_loss: 0.5193\n",
      "35/88, train_loss: 0.5194\n",
      "36/88, train_loss: 0.5187\n",
      "37/88, train_loss: 0.5196\n",
      "38/88, train_loss: 0.5194\n",
      "39/88, train_loss: 0.5175\n",
      "40/88, train_loss: 0.5191\n",
      "41/88, train_loss: 0.5191\n",
      "42/88, train_loss: 0.5175\n",
      "43/88, train_loss: 0.5190\n",
      "44/88, train_loss: 0.5190\n",
      "45/88, train_loss: 0.5172\n",
      "46/88, train_loss: 0.5187\n",
      "47/88, train_loss: 0.5186\n",
      "48/88, train_loss: 0.5183\n",
      "49/88, train_loss: 0.5185\n",
      "50/88, train_loss: 0.5139\n",
      "51/88, train_loss: 0.5183\n",
      "52/88, train_loss: 0.5182\n",
      "53/88, train_loss: 0.5115\n",
      "54/88, train_loss: 0.5179\n",
      "55/88, train_loss: 0.5119\n",
      "56/88, train_loss: 0.5178\n",
      "57/88, train_loss: 0.5189\n",
      "58/88, train_loss: 0.5186\n",
      "59/88, train_loss: 0.5181\n",
      "60/88, train_loss: 0.5092\n",
      "61/88, train_loss: 0.5178\n",
      "62/88, train_loss: 0.5176\n",
      "63/88, train_loss: 0.5179\n",
      "64/88, train_loss: 0.5173\n",
      "65/88, train_loss: 0.5167\n",
      "66/88, train_loss: 0.5175\n",
      "67/88, train_loss: 0.5172\n",
      "68/88, train_loss: 0.5163\n",
      "69/88, train_loss: 0.5172\n",
      "70/88, train_loss: 0.5169\n",
      "71/88, train_loss: 0.5168\n",
      "72/88, train_loss: 0.5167\n",
      "73/88, train_loss: 0.5167\n",
      "74/88, train_loss: 0.5098\n",
      "75/88, train_loss: 0.5167\n",
      "76/88, train_loss: 0.5169\n",
      "77/88, train_loss: 0.5171\n",
      "78/88, train_loss: 0.5171\n",
      "79/88, train_loss: 0.5167\n",
      "80/88, train_loss: 0.5155\n",
      "81/88, train_loss: 0.5164\n",
      "82/88, train_loss: 0.5163\n",
      "83/88, train_loss: 0.5161\n",
      "84/88, train_loss: 0.5147\n",
      "85/88, train_loss: 0.5160\n",
      "86/88, train_loss: 0.5139\n",
      "87/88, train_loss: 0.5154\n",
      "88/88, train_loss: 0.5134\n",
      "Epoch 3 average loss: 0.5181\n",
      "\n",
      "Epoch 4/300\n",
      "1/88, train_loss: 0.5157\n",
      "2/88, train_loss: 0.5157\n",
      "3/88, train_loss: 0.5157\n",
      "4/88, train_loss: 0.5158\n",
      "5/88, train_loss: 0.5156\n",
      "6/88, train_loss: 0.5155\n",
      "7/88, train_loss: 0.5146\n",
      "8/88, train_loss: 0.5155\n",
      "9/88, train_loss: 0.5126\n",
      "10/88, train_loss: 0.5148\n",
      "11/88, train_loss: 0.5123\n",
      "12/88, train_loss: 0.5103\n",
      "13/88, train_loss: 0.5065\n",
      "14/88, train_loss: 0.5156\n",
      "15/88, train_loss: 0.5170\n",
      "16/88, train_loss: 0.5162\n",
      "17/88, train_loss: 0.5063\n",
      "18/88, train_loss: 0.5157\n",
      "19/88, train_loss: 0.5154\n",
      "20/88, train_loss: 0.5158\n",
      "21/88, train_loss: 0.5105\n",
      "22/88, train_loss: 0.5150\n",
      "23/88, train_loss: 0.5123\n",
      "24/88, train_loss: 0.5152\n",
      "25/88, train_loss: 0.5149\n",
      "26/88, train_loss: 0.5149\n",
      "27/88, train_loss: 0.5142\n",
      "28/88, train_loss: 0.5149\n",
      "29/88, train_loss: 0.5147\n",
      "30/88, train_loss: 0.5147\n",
      "31/88, train_loss: 0.5146\n",
      "32/88, train_loss: 0.5087\n",
      "33/88, train_loss: 0.5142\n",
      "34/88, train_loss: 0.5124\n",
      "35/88, train_loss: 0.5024\n",
      "36/88, train_loss: 0.5108\n",
      "37/88, train_loss: 0.5155\n",
      "38/88, train_loss: 0.5116\n",
      "39/88, train_loss: 0.5170\n",
      "40/88, train_loss: 0.5151\n",
      "41/88, train_loss: 0.4985\n",
      "42/88, train_loss: 0.5116\n",
      "43/88, train_loss: 0.5085\n",
      "44/88, train_loss: 0.5152\n",
      "45/88, train_loss: 0.5150\n",
      "46/88, train_loss: 0.5133\n",
      "47/88, train_loss: 0.5085\n",
      "48/88, train_loss: 0.5140\n",
      "49/88, train_loss: 0.5137\n",
      "50/88, train_loss: 0.5137\n",
      "51/88, train_loss: 0.5049\n",
      "52/88, train_loss: 0.5094\n",
      "53/88, train_loss: 0.5025\n",
      "54/88, train_loss: 0.5140\n",
      "55/88, train_loss: 0.5141\n",
      "56/88, train_loss: 0.5139\n",
      "57/88, train_loss: 0.5137\n",
      "58/88, train_loss: 0.5138\n",
      "59/88, train_loss: 0.5131\n",
      "60/88, train_loss: 0.5136\n",
      "61/88, train_loss: 0.5131\n",
      "62/88, train_loss: 0.5129\n",
      "63/88, train_loss: 0.5123\n",
      "64/88, train_loss: 0.5129\n",
      "65/88, train_loss: 0.5130\n",
      "66/88, train_loss: 0.5033\n",
      "67/88, train_loss: 0.5125\n",
      "68/88, train_loss: 0.5125\n",
      "69/88, train_loss: 0.5104\n",
      "70/88, train_loss: 0.5125\n",
      "71/88, train_loss: 0.5115\n",
      "72/88, train_loss: 0.5126\n",
      "73/88, train_loss: 0.5126\n",
      "74/88, train_loss: 0.5129\n",
      "75/88, train_loss: 0.5098\n",
      "76/88, train_loss: 0.5065\n",
      "77/88, train_loss: 0.5048\n",
      "78/88, train_loss: 0.5127\n",
      "79/88, train_loss: 0.5121\n",
      "80/88, train_loss: 0.5126\n",
      "81/88, train_loss: 0.5130\n",
      "82/88, train_loss: 0.5123\n",
      "83/88, train_loss: 0.5121\n",
      "84/88, train_loss: 0.5122\n",
      "85/88, train_loss: 0.5120\n",
      "86/88, train_loss: 0.5120\n",
      "87/88, train_loss: 0.5107\n",
      "88/88, train_loss: 0.5118\n",
      "Epoch 4 average loss: 0.5124\n",
      "\n",
      "Epoch 5/300\n",
      "1/88, train_loss: 0.5119\n",
      "2/88, train_loss: 0.5118\n",
      "3/88, train_loss: 0.5007\n",
      "4/88, train_loss: 0.5120\n",
      "5/88, train_loss: 0.5114\n",
      "6/88, train_loss: 0.5117\n",
      "7/88, train_loss: 0.5110\n",
      "8/88, train_loss: 0.5117\n",
      "9/88, train_loss: 0.5116\n",
      "10/88, train_loss: 0.5116\n",
      "11/88, train_loss: 0.5037\n",
      "12/88, train_loss: 0.5115\n",
      "13/88, train_loss: 0.5114\n",
      "14/88, train_loss: 0.5115\n",
      "15/88, train_loss: 0.5045\n",
      "16/88, train_loss: 0.5119\n",
      "17/88, train_loss: 0.5114\n",
      "18/88, train_loss: 0.5113\n",
      "19/88, train_loss: 0.5100\n",
      "20/88, train_loss: 0.5114\n",
      "21/88, train_loss: 0.5028\n",
      "22/88, train_loss: 0.5106\n",
      "23/88, train_loss: 0.5049\n",
      "24/88, train_loss: 0.5049\n",
      "25/88, train_loss: 0.5113\n",
      "26/88, train_loss: 0.5119\n",
      "27/88, train_loss: 0.5037\n",
      "28/88, train_loss: 0.5104\n",
      "29/88, train_loss: 0.5114\n",
      "30/88, train_loss: 0.5113\n",
      "31/88, train_loss: 0.5111\n",
      "32/88, train_loss: 0.5011\n",
      "33/88, train_loss: 0.5084\n",
      "34/88, train_loss: 0.5113\n",
      "35/88, train_loss: 0.5068\n",
      "36/88, train_loss: 0.5112\n",
      "37/88, train_loss: 0.4934\n",
      "38/88, train_loss: 0.5109\n",
      "39/88, train_loss: 0.4992\n",
      "40/88, train_loss: 0.5108\n",
      "41/88, train_loss: 0.4894\n",
      "42/88, train_loss: 0.5122\n",
      "43/88, train_loss: 0.5133\n",
      "44/88, train_loss: 0.5113\n",
      "45/88, train_loss: 0.5110\n",
      "46/88, train_loss: 0.5060\n",
      "47/88, train_loss: 0.5034\n",
      "48/88, train_loss: 0.5124\n",
      "49/88, train_loss: 0.5131\n",
      "50/88, train_loss: 0.5134\n",
      "51/88, train_loss: 0.5114\n",
      "52/88, train_loss: 0.5066\n",
      "53/88, train_loss: 0.5120\n",
      "54/88, train_loss: 0.4961\n",
      "55/88, train_loss: 0.5108\n",
      "56/88, train_loss: 0.5103\n",
      "57/88, train_loss: 0.5106\n",
      "58/88, train_loss: 0.5089\n",
      "59/88, train_loss: 0.5102\n",
      "60/88, train_loss: 0.5099\n",
      "61/88, train_loss: 0.5105\n",
      "62/88, train_loss: 0.5101\n",
      "63/88, train_loss: 0.5105\n",
      "64/88, train_loss: 0.5102\n",
      "65/88, train_loss: 0.4955\n",
      "66/88, train_loss: 0.5087\n",
      "67/88, train_loss: 0.5098\n",
      "68/88, train_loss: 0.5099\n",
      "69/88, train_loss: 0.5024\n",
      "70/88, train_loss: 0.5102\n",
      "71/88, train_loss: 0.4993\n",
      "72/88, train_loss: 0.4913\n",
      "73/88, train_loss: 0.5099\n",
      "74/88, train_loss: 0.5100\n",
      "75/88, train_loss: 0.5104\n",
      "76/88, train_loss: 0.5104\n",
      "77/88, train_loss: 0.5099\n",
      "78/88, train_loss: 0.5101\n",
      "79/88, train_loss: 0.5097\n",
      "80/88, train_loss: 0.5104\n",
      "81/88, train_loss: 0.5024\n",
      "82/88, train_loss: 0.5094\n",
      "83/88, train_loss: 0.5094\n",
      "84/88, train_loss: 0.4927\n",
      "85/88, train_loss: 0.5100\n",
      "86/88, train_loss: 0.4945\n",
      "87/88, train_loss: 0.5104\n",
      "88/88, train_loss: 0.4843\n",
      "Epoch 5 average loss: 0.5078\n",
      "Validation loss: 0.5106\n",
      "Saved new best model\n",
      "Current epoch: 5, current mean dice: 0.0362, best mean dice: 0.0362 at epoch 5\n",
      "\n",
      "Epoch 6/300\n",
      "1/88, train_loss: 0.5104\n",
      "2/88, train_loss: 0.5105\n",
      "3/88, train_loss: 0.5069\n",
      "4/88, train_loss: 0.5124\n",
      "5/88, train_loss: 0.5120\n",
      "6/88, train_loss: 0.5125\n",
      "7/88, train_loss: 0.5022\n",
      "8/88, train_loss: 0.5006\n",
      "9/88, train_loss: 0.5099\n",
      "10/88, train_loss: 0.5091\n",
      "11/88, train_loss: 0.5092\n",
      "12/88, train_loss: 0.4962\n",
      "13/88, train_loss: 0.5090\n",
      "14/88, train_loss: 0.5088\n",
      "15/88, train_loss: 0.5090\n",
      "16/88, train_loss: 0.5087\n",
      "17/88, train_loss: 0.5086\n",
      "18/88, train_loss: 0.5086\n",
      "19/88, train_loss: 0.4910\n",
      "20/88, train_loss: 0.4976\n",
      "21/88, train_loss: 0.5086\n",
      "22/88, train_loss: 0.5032\n",
      "23/88, train_loss: 0.5092\n",
      "24/88, train_loss: 0.5080\n",
      "25/88, train_loss: 0.5099\n",
      "26/88, train_loss: 0.4796\n",
      "27/88, train_loss: 0.4973\n",
      "28/88, train_loss: 0.5097\n",
      "29/88, train_loss: 0.5060\n",
      "30/88, train_loss: 0.5125\n",
      "31/88, train_loss: 0.4937\n",
      "32/88, train_loss: 0.5112\n",
      "33/88, train_loss: 0.5095\n",
      "34/88, train_loss: 0.5089\n",
      "35/88, train_loss: 0.5072\n",
      "36/88, train_loss: 0.5085\n",
      "37/88, train_loss: 0.5084\n",
      "38/88, train_loss: 0.5085\n",
      "39/88, train_loss: 0.5083\n",
      "40/88, train_loss: 0.5083\n",
      "41/88, train_loss: 0.4952\n",
      "42/88, train_loss: 0.5079\n",
      "43/88, train_loss: 0.5060\n",
      "44/88, train_loss: 0.5012\n",
      "45/88, train_loss: 0.5079\n",
      "46/88, train_loss: 0.5081\n",
      "47/88, train_loss: 0.5057\n",
      "48/88, train_loss: 0.4935\n",
      "49/88, train_loss: 0.4954\n",
      "50/88, train_loss: 0.5083\n",
      "51/88, train_loss: 0.5079\n",
      "52/88, train_loss: 0.5084\n",
      "53/88, train_loss: 0.5082\n",
      "54/88, train_loss: 0.5077\n",
      "55/88, train_loss: 0.5084\n",
      "56/88, train_loss: 0.4878\n",
      "57/88, train_loss: 0.5072\n",
      "58/88, train_loss: 0.5075\n",
      "59/88, train_loss: 0.5075\n",
      "60/88, train_loss: 0.5074\n",
      "61/88, train_loss: 0.5080\n",
      "62/88, train_loss: 0.5078\n",
      "63/88, train_loss: 0.4978\n",
      "64/88, train_loss: 0.5073\n",
      "65/88, train_loss: 0.5064\n",
      "66/88, train_loss: 0.5057\n",
      "67/88, train_loss: 0.4954\n",
      "68/88, train_loss: 0.5075\n",
      "69/88, train_loss: 0.4994\n",
      "70/88, train_loss: 0.5066\n",
      "71/88, train_loss: 0.5005\n",
      "72/88, train_loss: 0.5071\n",
      "73/88, train_loss: 0.5071\n",
      "74/88, train_loss: 0.5071\n",
      "75/88, train_loss: 0.5071\n",
      "76/88, train_loss: 0.5071\n",
      "77/88, train_loss: 0.4896\n",
      "78/88, train_loss: 0.4946\n",
      "79/88, train_loss: 0.5074\n",
      "80/88, train_loss: 0.4752\n",
      "81/88, train_loss: 0.5076\n",
      "82/88, train_loss: 0.5086\n",
      "83/88, train_loss: 0.4903\n",
      "84/88, train_loss: 0.4798\n",
      "85/88, train_loss: 0.5072\n",
      "86/88, train_loss: 0.5067\n",
      "87/88, train_loss: 0.5069\n",
      "88/88, train_loss: 0.5065\n",
      "Epoch 6 average loss: 0.5045\n",
      "\n",
      "Epoch 7/300\n",
      "1/88, train_loss: 0.5071\n",
      "2/88, train_loss: 0.5051\n",
      "3/88, train_loss: 0.5070\n",
      "4/88, train_loss: 0.5068\n",
      "5/88, train_loss: 0.5067\n",
      "6/88, train_loss: 0.5066\n",
      "7/88, train_loss: 0.5040\n",
      "8/88, train_loss: 0.5065\n",
      "9/88, train_loss: 0.5049\n",
      "10/88, train_loss: 0.5065\n",
      "11/88, train_loss: 0.4790\n",
      "12/88, train_loss: 0.5065\n",
      "13/88, train_loss: 0.5063\n",
      "14/88, train_loss: 0.5064\n",
      "15/88, train_loss: 0.5055\n",
      "16/88, train_loss: 0.5063\n",
      "17/88, train_loss: 0.5061\n",
      "18/88, train_loss: 0.5062\n",
      "19/88, train_loss: 0.5062\n",
      "20/88, train_loss: 0.5062\n",
      "21/88, train_loss: 0.4968\n",
      "22/88, train_loss: 0.5036\n",
      "23/88, train_loss: 0.4961\n",
      "24/88, train_loss: 0.5061\n",
      "25/88, train_loss: 0.5064\n",
      "26/88, train_loss: 0.4905\n",
      "27/88, train_loss: 0.5064\n",
      "28/88, train_loss: 0.5068\n",
      "29/88, train_loss: 0.4930\n",
      "30/88, train_loss: 0.5070\n",
      "31/88, train_loss: 0.5068\n",
      "32/88, train_loss: 0.5067\n",
      "33/88, train_loss: 0.5043\n",
      "34/88, train_loss: 0.5063\n",
      "35/88, train_loss: 0.5069\n",
      "36/88, train_loss: 0.5050\n",
      "37/88, train_loss: 0.5062\n",
      "38/88, train_loss: 0.5063\n",
      "39/88, train_loss: 0.5062\n",
      "40/88, train_loss: 0.5062\n",
      "41/88, train_loss: 0.4941\n",
      "42/88, train_loss: 0.5062\n",
      "43/88, train_loss: 0.5063\n",
      "44/88, train_loss: 0.4935\n",
      "45/88, train_loss: 0.5066\n",
      "46/88, train_loss: 0.4671\n",
      "47/88, train_loss: 0.5062\n",
      "48/88, train_loss: 0.5062\n",
      "49/88, train_loss: 0.4996\n",
      "50/88, train_loss: 0.5020\n",
      "51/88, train_loss: 0.5063\n",
      "52/88, train_loss: 0.4823\n",
      "53/88, train_loss: 0.4936\n",
      "54/88, train_loss: 0.5009\n",
      "55/88, train_loss: 0.4709\n",
      "56/88, train_loss: 0.5061\n",
      "57/88, train_loss: 0.5048\n",
      "58/88, train_loss: 0.5058\n",
      "59/88, train_loss: 0.5063\n",
      "60/88, train_loss: 0.5058\n",
      "61/88, train_loss: 0.5035\n",
      "62/88, train_loss: 0.4967\n",
      "63/88, train_loss: 0.4847\n",
      "64/88, train_loss: 0.5007\n",
      "65/88, train_loss: 0.5066\n",
      "66/88, train_loss: 0.4867\n",
      "67/88, train_loss: 0.5056\n",
      "68/88, train_loss: 0.5063\n",
      "69/88, train_loss: 0.4934\n",
      "70/88, train_loss: 0.4904\n",
      "71/88, train_loss: 0.4738\n",
      "72/88, train_loss: 0.5069\n",
      "73/88, train_loss: 0.5048\n",
      "74/88, train_loss: 0.4879\n",
      "75/88, train_loss: 0.5037\n",
      "76/88, train_loss: 0.4607\n",
      "77/88, train_loss: 0.5060\n",
      "78/88, train_loss: 0.4911\n",
      "79/88, train_loss: 0.5059\n",
      "80/88, train_loss: 0.5073\n",
      "81/88, train_loss: 0.4860\n",
      "82/88, train_loss: 0.5079\n",
      "83/88, train_loss: 0.5069\n",
      "84/88, train_loss: 0.4969\n",
      "85/88, train_loss: 0.5060\n",
      "86/88, train_loss: 0.5057\n",
      "87/88, train_loss: 0.5056\n",
      "88/88, train_loss: 0.4890\n",
      "Epoch 7 average loss: 0.5008\n",
      "\n",
      "Epoch 8/300\n",
      "1/88, train_loss: 0.5054\n",
      "2/88, train_loss: 0.5053\n",
      "3/88, train_loss: 0.4955\n",
      "4/88, train_loss: 0.5055\n",
      "5/88, train_loss: 0.5058\n",
      "6/88, train_loss: 0.4685\n",
      "7/88, train_loss: 0.5012\n",
      "8/88, train_loss: 0.4877\n",
      "9/88, train_loss: 0.5062\n",
      "10/88, train_loss: 0.4829\n",
      "11/88, train_loss: 0.5077\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5090\n",
      "14/88, train_loss: 0.5089\n",
      "15/88, train_loss: 0.5085\n",
      "16/88, train_loss: 0.5062\n",
      "17/88, train_loss: 0.5059\n",
      "18/88, train_loss: 0.5054\n",
      "19/88, train_loss: 0.5055\n",
      "20/88, train_loss: 0.5055\n",
      "21/88, train_loss: 0.5021\n",
      "22/88, train_loss: 0.4811\n",
      "23/88, train_loss: 0.5042\n",
      "24/88, train_loss: 0.5032\n",
      "25/88, train_loss: 0.5051\n",
      "26/88, train_loss: 0.5054\n",
      "27/88, train_loss: 0.5057\n",
      "28/88, train_loss: 0.5037\n",
      "29/88, train_loss: 0.5056\n",
      "30/88, train_loss: 0.5055\n",
      "31/88, train_loss: 0.5059\n",
      "32/88, train_loss: 0.5047\n",
      "33/88, train_loss: 0.5051\n",
      "34/88, train_loss: 0.5049\n",
      "35/88, train_loss: 0.5047\n",
      "36/88, train_loss: 0.4984\n",
      "37/88, train_loss: 0.5049\n",
      "38/88, train_loss: 0.5049\n",
      "39/88, train_loss: 0.5040\n",
      "40/88, train_loss: 0.5050\n",
      "41/88, train_loss: 0.5045\n",
      "42/88, train_loss: 0.4813\n",
      "43/88, train_loss: 0.5047\n",
      "44/88, train_loss: 0.4797\n",
      "45/88, train_loss: 0.5049\n",
      "46/88, train_loss: 0.5045\n",
      "47/88, train_loss: 0.5048\n",
      "48/88, train_loss: 0.5048\n",
      "49/88, train_loss: 0.4845\n",
      "50/88, train_loss: 0.5043\n",
      "51/88, train_loss: 0.5045\n",
      "52/88, train_loss: 0.5045\n",
      "53/88, train_loss: 0.4661\n",
      "54/88, train_loss: 0.5058\n",
      "55/88, train_loss: 0.4623\n",
      "56/88, train_loss: 0.5055\n",
      "57/88, train_loss: 0.4849\n",
      "58/88, train_loss: 0.4942\n",
      "59/88, train_loss: 0.5005\n",
      "60/88, train_loss: 0.4606\n",
      "61/88, train_loss: 0.4991\n",
      "62/88, train_loss: 0.5045\n",
      "63/88, train_loss: 0.4829\n",
      "64/88, train_loss: 0.5040\n",
      "65/88, train_loss: 0.4859\n",
      "66/88, train_loss: 0.5044\n",
      "67/88, train_loss: 0.4953\n",
      "68/88, train_loss: 0.5044\n",
      "69/88, train_loss: 0.5058\n",
      "70/88, train_loss: 0.5055\n",
      "71/88, train_loss: 0.4675\n",
      "72/88, train_loss: 0.5050\n",
      "73/88, train_loss: 0.5054\n",
      "74/88, train_loss: 0.4601\n",
      "75/88, train_loss: 0.5035\n",
      "76/88, train_loss: 0.5053\n",
      "77/88, train_loss: 0.5046\n",
      "78/88, train_loss: 0.4954\n",
      "79/88, train_loss: 0.5059\n",
      "80/88, train_loss: 0.4901\n",
      "81/88, train_loss: 0.5044\n",
      "82/88, train_loss: 0.4320\n",
      "83/88, train_loss: 0.5044\n",
      "84/88, train_loss: 0.5041\n",
      "85/88, train_loss: 0.4917\n",
      "86/88, train_loss: 0.5042\n",
      "87/88, train_loss: 0.5039\n",
      "88/88, train_loss: 0.4756\n",
      "Epoch 8 average loss: 0.4979\n",
      "\n",
      "Epoch 9/300\n",
      "1/88, train_loss: 0.5042\n",
      "2/88, train_loss: 0.4962\n",
      "3/88, train_loss: 0.5045\n",
      "4/88, train_loss: 0.5003\n",
      "5/88, train_loss: 0.5047\n",
      "6/88, train_loss: 0.5045\n",
      "7/88, train_loss: 0.5042\n",
      "8/88, train_loss: 0.5039\n",
      "9/88, train_loss: 0.4973\n",
      "10/88, train_loss: 0.5048\n",
      "11/88, train_loss: 0.5047\n",
      "12/88, train_loss: 0.5044\n",
      "13/88, train_loss: 0.5042\n",
      "14/88, train_loss: 0.5004\n",
      "15/88, train_loss: 0.5043\n",
      "16/88, train_loss: 0.5025\n",
      "17/88, train_loss: 0.4676\n",
      "18/88, train_loss: 0.5047\n",
      "19/88, train_loss: 0.5042\n",
      "20/88, train_loss: 0.5062\n",
      "21/88, train_loss: 0.5074\n",
      "22/88, train_loss: 0.5055\n",
      "23/88, train_loss: 0.5023\n",
      "24/88, train_loss: 0.4740\n",
      "25/88, train_loss: 0.5037\n",
      "26/88, train_loss: 0.5070\n",
      "27/88, train_loss: 0.5053\n",
      "28/88, train_loss: 0.4767\n",
      "29/88, train_loss: 0.5041\n",
      "30/88, train_loss: 0.5039\n",
      "31/88, train_loss: 0.4332\n",
      "32/88, train_loss: 0.4813\n",
      "33/88, train_loss: 0.5041\n",
      "34/88, train_loss: 0.5041\n",
      "35/88, train_loss: 0.4669\n",
      "36/88, train_loss: 0.4409\n",
      "37/88, train_loss: 0.5037\n",
      "38/88, train_loss: 0.5041\n",
      "39/88, train_loss: 0.5037\n",
      "40/88, train_loss: 0.5043\n",
      "41/88, train_loss: 0.5037\n",
      "42/88, train_loss: 0.5037\n",
      "43/88, train_loss: 0.4948\n",
      "44/88, train_loss: 0.4671\n",
      "45/88, train_loss: 0.4902\n",
      "46/88, train_loss: 0.5039\n",
      "47/88, train_loss: 0.4875\n",
      "48/88, train_loss: 0.5060\n",
      "49/88, train_loss: 0.4985\n",
      "50/88, train_loss: 0.5041\n",
      "51/88, train_loss: 0.4788\n",
      "52/88, train_loss: 0.5039\n",
      "53/88, train_loss: 0.4990\n",
      "54/88, train_loss: 0.5039\n",
      "55/88, train_loss: 0.5034\n",
      "56/88, train_loss: 0.5009\n",
      "57/88, train_loss: 0.5034\n",
      "58/88, train_loss: 0.5035\n",
      "59/88, train_loss: 0.5035\n",
      "60/88, train_loss: 0.5034\n",
      "61/88, train_loss: 0.5027\n",
      "62/88, train_loss: 0.5036\n",
      "63/88, train_loss: 0.5034\n",
      "64/88, train_loss: 0.5034\n",
      "65/88, train_loss: 0.4867\n",
      "66/88, train_loss: 0.5033\n",
      "67/88, train_loss: 0.4931\n",
      "68/88, train_loss: 0.5030\n",
      "69/88, train_loss: 0.5032\n",
      "70/88, train_loss: 0.4781\n",
      "71/88, train_loss: 0.5039\n",
      "72/88, train_loss: 0.4578\n",
      "73/88, train_loss: 0.4872\n",
      "74/88, train_loss: 0.5045\n",
      "75/88, train_loss: 0.5040\n",
      "76/88, train_loss: 0.5046\n",
      "77/88, train_loss: 0.4692\n",
      "78/88, train_loss: 0.5040\n",
      "79/88, train_loss: 0.4611\n",
      "80/88, train_loss: 0.5039\n",
      "81/88, train_loss: 0.5043\n",
      "82/88, train_loss: 0.4680\n",
      "83/88, train_loss: 0.5033\n",
      "84/88, train_loss: 0.5035\n",
      "85/88, train_loss: 0.4510\n",
      "86/88, train_loss: 0.4846\n",
      "87/88, train_loss: 0.4802\n",
      "88/88, train_loss: 0.5013\n",
      "Epoch 9 average loss: 0.4955\n",
      "\n",
      "Epoch 10/300\n",
      "1/88, train_loss: 0.4989\n",
      "2/88, train_loss: 0.5032\n",
      "3/88, train_loss: 0.4930\n",
      "4/88, train_loss: 0.5033\n",
      "5/88, train_loss: 0.4996\n",
      "6/88, train_loss: 0.5036\n",
      "7/88, train_loss: 0.5026\n",
      "8/88, train_loss: 0.5032\n",
      "9/88, train_loss: 0.4710\n",
      "10/88, train_loss: 0.4967\n",
      "11/88, train_loss: 0.5033\n",
      "12/88, train_loss: 0.5035\n",
      "13/88, train_loss: 0.4989\n",
      "14/88, train_loss: 0.5030\n",
      "15/88, train_loss: 0.4574\n",
      "16/88, train_loss: 0.5038\n",
      "17/88, train_loss: 0.5031\n",
      "18/88, train_loss: 0.4911\n",
      "19/88, train_loss: 0.4976\n",
      "20/88, train_loss: 0.5033\n",
      "21/88, train_loss: 0.4952\n",
      "22/88, train_loss: 0.4126\n",
      "23/88, train_loss: 0.4775\n",
      "24/88, train_loss: 0.5034\n",
      "25/88, train_loss: 0.4802\n",
      "26/88, train_loss: 0.5017\n",
      "27/88, train_loss: 0.5021\n",
      "28/88, train_loss: 0.5030\n",
      "29/88, train_loss: 0.5036\n",
      "30/88, train_loss: 0.4646\n",
      "31/88, train_loss: 0.5029\n",
      "32/88, train_loss: 0.4802\n",
      "33/88, train_loss: 0.5031\n",
      "34/88, train_loss: 0.5032\n",
      "35/88, train_loss: 0.5036\n",
      "36/88, train_loss: 0.4562\n",
      "37/88, train_loss: 0.5039\n",
      "38/88, train_loss: 0.4865\n",
      "39/88, train_loss: 0.5030\n",
      "40/88, train_loss: 0.5029\n",
      "41/88, train_loss: 0.5028\n",
      "42/88, train_loss: 0.5027\n",
      "43/88, train_loss: 0.4920\n",
      "44/88, train_loss: 0.4857\n",
      "45/88, train_loss: 0.4863\n",
      "46/88, train_loss: 0.4596\n",
      "47/88, train_loss: 0.5029\n",
      "48/88, train_loss: 0.5032\n",
      "49/88, train_loss: 0.4564\n",
      "50/88, train_loss: 0.5025\n",
      "51/88, train_loss: 0.5027\n",
      "52/88, train_loss: 0.5018\n",
      "53/88, train_loss: 0.4988\n",
      "54/88, train_loss: 0.5028\n",
      "55/88, train_loss: 0.4610\n",
      "56/88, train_loss: 0.5030\n",
      "57/88, train_loss: 0.4839\n",
      "58/88, train_loss: 0.5012\n",
      "59/88, train_loss: 0.5036\n",
      "60/88, train_loss: 0.5019\n",
      "61/88, train_loss: 0.5026\n",
      "62/88, train_loss: 0.4681\n",
      "63/88, train_loss: 0.4513\n",
      "64/88, train_loss: 0.5023\n",
      "65/88, train_loss: 0.5026\n",
      "66/88, train_loss: 0.5030\n",
      "67/88, train_loss: 0.5030\n",
      "68/88, train_loss: 0.4887\n",
      "69/88, train_loss: 0.4732\n",
      "70/88, train_loss: 0.5026\n",
      "71/88, train_loss: 0.4478\n",
      "72/88, train_loss: 0.5023\n",
      "73/88, train_loss: 0.4898\n",
      "74/88, train_loss: 0.5023\n",
      "75/88, train_loss: 0.5024\n",
      "76/88, train_loss: 0.4779\n",
      "77/88, train_loss: 0.4945\n",
      "78/88, train_loss: 0.5030\n",
      "79/88, train_loss: 0.5029\n",
      "80/88, train_loss: 0.5030\n",
      "81/88, train_loss: 0.5026\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.4613\n",
      "84/88, train_loss: 0.5033\n",
      "85/88, train_loss: 0.4938\n",
      "86/88, train_loss: 0.5025\n",
      "87/88, train_loss: 0.5038\n",
      "88/88, train_loss: 0.5028\n",
      "Epoch 10 average loss: 0.4929\n",
      "Validation loss: 0.4997\n",
      "Saved new best model\n",
      "Current epoch: 10, current mean dice: 0.1861, best mean dice: 0.1861 at epoch 10\n",
      "\n",
      "Epoch 11/300\n",
      "1/88, train_loss: 0.5027\n",
      "2/88, train_loss: 0.4427\n",
      "3/88, train_loss: 0.4950\n",
      "4/88, train_loss: 0.5028\n",
      "5/88, train_loss: 0.5022\n",
      "6/88, train_loss: 0.4406\n",
      "7/88, train_loss: 0.5036\n",
      "8/88, train_loss: 0.5025\n",
      "9/88, train_loss: 0.5014\n",
      "10/88, train_loss: 0.5028\n",
      "11/88, train_loss: 0.4960\n",
      "12/88, train_loss: 0.5035\n",
      "13/88, train_loss: 0.4857\n",
      "14/88, train_loss: 0.5025\n",
      "15/88, train_loss: 0.4296\n",
      "16/88, train_loss: 0.5028\n",
      "17/88, train_loss: 0.5025\n",
      "18/88, train_loss: 0.5025\n",
      "19/88, train_loss: 0.4947\n",
      "20/88, train_loss: 0.4878\n",
      "21/88, train_loss: 0.5026\n",
      "22/88, train_loss: 0.5028\n",
      "23/88, train_loss: 0.4808\n",
      "24/88, train_loss: 0.5024\n",
      "25/88, train_loss: 0.4633\n",
      "26/88, train_loss: 0.5028\n",
      "27/88, train_loss: 0.5026\n",
      "28/88, train_loss: 0.4769\n",
      "29/88, train_loss: 0.5035\n",
      "30/88, train_loss: 0.5038\n",
      "31/88, train_loss: 0.5047\n",
      "32/88, train_loss: 0.4762\n",
      "33/88, train_loss: 0.5028\n",
      "34/88, train_loss: 0.4720\n",
      "35/88, train_loss: 0.4942\n",
      "36/88, train_loss: 0.5023\n",
      "37/88, train_loss: 0.5022\n",
      "38/88, train_loss: 0.5022\n",
      "39/88, train_loss: 0.4993\n",
      "40/88, train_loss: 0.5022\n",
      "41/88, train_loss: 0.4534\n",
      "42/88, train_loss: 0.4917\n",
      "43/88, train_loss: 0.4924\n",
      "44/88, train_loss: 0.5007\n",
      "45/88, train_loss: 0.5021\n",
      "46/88, train_loss: 0.4924\n",
      "47/88, train_loss: 0.4965\n",
      "48/88, train_loss: 0.4766\n",
      "49/88, train_loss: 0.5032\n",
      "50/88, train_loss: 0.5027\n",
      "51/88, train_loss: 0.5027\n",
      "52/88, train_loss: 0.5030\n",
      "53/88, train_loss: 0.5026\n",
      "54/88, train_loss: 0.5051\n",
      "55/88, train_loss: 0.5029\n",
      "56/88, train_loss: 0.5037\n",
      "57/88, train_loss: 0.5037\n",
      "58/88, train_loss: 0.5046\n",
      "59/88, train_loss: 0.4559\n",
      "60/88, train_loss: 0.5026\n",
      "61/88, train_loss: 0.4622\n",
      "62/88, train_loss: 0.4502\n",
      "63/88, train_loss: 0.5022\n",
      "64/88, train_loss: 0.4599\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.4917\n",
      "67/88, train_loss: 0.4750\n",
      "68/88, train_loss: 0.4668\n",
      "69/88, train_loss: 0.5026\n",
      "70/88, train_loss: 0.5031\n",
      "71/88, train_loss: 0.5029\n",
      "72/88, train_loss: 0.4937\n",
      "73/88, train_loss: 0.5036\n",
      "74/88, train_loss: 0.4674\n",
      "75/88, train_loss: 0.4535\n",
      "76/88, train_loss: 0.5037\n",
      "77/88, train_loss: 0.5026\n",
      "78/88, train_loss: 0.5031\n",
      "79/88, train_loss: 0.4563\n",
      "80/88, train_loss: 0.5023\n",
      "81/88, train_loss: 0.5023\n",
      "82/88, train_loss: 0.4760\n",
      "83/88, train_loss: 0.4959\n",
      "84/88, train_loss: 0.5022\n",
      "85/88, train_loss: 0.4602\n",
      "86/88, train_loss: 0.5020\n",
      "87/88, train_loss: 0.5020\n",
      "88/88, train_loss: 0.5021\n",
      "Epoch 11 average loss: 0.4914\n",
      "\n",
      "Epoch 12/300\n",
      "1/88, train_loss: 0.5019\n",
      "2/88, train_loss: 0.4700\n",
      "3/88, train_loss: 0.4715\n",
      "4/88, train_loss: 0.4608\n",
      "5/88, train_loss: 0.5020\n",
      "6/88, train_loss: 0.4851\n",
      "7/88, train_loss: 0.4961\n",
      "8/88, train_loss: 0.4376\n",
      "9/88, train_loss: 0.4476\n",
      "10/88, train_loss: 0.5043\n",
      "11/88, train_loss: 0.4857\n",
      "12/88, train_loss: 0.5030\n",
      "13/88, train_loss: 0.4813\n",
      "14/88, train_loss: 0.4524\n",
      "15/88, train_loss: 0.4719\n",
      "16/88, train_loss: 0.4440\n",
      "17/88, train_loss: 0.5021\n",
      "18/88, train_loss: 0.5019\n",
      "19/88, train_loss: 0.5019\n",
      "20/88, train_loss: 0.5007\n",
      "21/88, train_loss: 0.5013\n",
      "22/88, train_loss: 0.5021\n",
      "23/88, train_loss: 0.5021\n",
      "24/88, train_loss: 0.5021\n",
      "25/88, train_loss: 0.5021\n",
      "26/88, train_loss: 0.5018\n",
      "27/88, train_loss: 0.5018\n",
      "28/88, train_loss: 0.4926\n",
      "29/88, train_loss: 0.4976\n",
      "30/88, train_loss: 0.5019\n",
      "31/88, train_loss: 0.5012\n",
      "32/88, train_loss: 0.5018\n",
      "33/88, train_loss: 0.4920\n",
      "34/88, train_loss: 0.5018\n",
      "35/88, train_loss: 0.5018\n",
      "36/88, train_loss: 0.5018\n",
      "37/88, train_loss: 0.4868\n",
      "38/88, train_loss: 0.4895\n",
      "39/88, train_loss: 0.4950\n",
      "40/88, train_loss: 0.4923\n",
      "41/88, train_loss: 0.5020\n",
      "42/88, train_loss: 0.5020\n",
      "43/88, train_loss: 0.5023\n",
      "44/88, train_loss: 0.5021\n",
      "45/88, train_loss: 0.5027\n",
      "46/88, train_loss: 0.4902\n",
      "47/88, train_loss: 0.5023\n",
      "48/88, train_loss: 0.4866\n",
      "49/88, train_loss: 0.5020\n",
      "50/88, train_loss: 0.4662\n",
      "51/88, train_loss: 0.5019\n",
      "52/88, train_loss: 0.5018\n",
      "53/88, train_loss: 0.5018\n",
      "54/88, train_loss: 0.4350\n",
      "55/88, train_loss: 0.5017\n",
      "56/88, train_loss: 0.4835\n",
      "57/88, train_loss: 0.5026\n",
      "58/88, train_loss: 0.5021\n",
      "59/88, train_loss: 0.5024\n",
      "60/88, train_loss: 0.5022\n",
      "61/88, train_loss: 0.3743\n",
      "62/88, train_loss: 0.5020\n",
      "63/88, train_loss: 0.5019\n",
      "64/88, train_loss: 0.5017\n",
      "65/88, train_loss: 0.5016\n",
      "66/88, train_loss: 0.5017\n",
      "67/88, train_loss: 0.4933\n",
      "68/88, train_loss: 0.5016\n",
      "69/88, train_loss: 0.5016\n",
      "70/88, train_loss: 0.5016\n",
      "71/88, train_loss: 0.5016\n",
      "72/88, train_loss: 0.5016\n",
      "73/88, train_loss: 0.5016\n",
      "74/88, train_loss: 0.4978\n",
      "75/88, train_loss: 0.5016\n",
      "76/88, train_loss: 0.4989\n",
      "77/88, train_loss: 0.4475\n",
      "78/88, train_loss: 0.4974\n",
      "79/88, train_loss: 0.5015\n",
      "80/88, train_loss: 0.5015\n",
      "81/88, train_loss: 0.5015\n",
      "82/88, train_loss: 0.5015\n",
      "83/88, train_loss: 0.4686\n",
      "84/88, train_loss: 0.4552\n",
      "85/88, train_loss: 0.5017\n",
      "86/88, train_loss: 0.5018\n",
      "87/88, train_loss: 0.5018\n",
      "88/88, train_loss: 0.4857\n",
      "Epoch 12 average loss: 0.4913\n",
      "\n",
      "Epoch 13/300\n",
      "1/88, train_loss: 0.4879\n",
      "2/88, train_loss: 0.5021\n",
      "3/88, train_loss: 0.4996\n",
      "4/88, train_loss: 0.5018\n",
      "5/88, train_loss: 0.5019\n",
      "6/88, train_loss: 0.4974\n",
      "7/88, train_loss: 0.5024\n",
      "8/88, train_loss: 0.5030\n",
      "9/88, train_loss: 0.4976\n",
      "10/88, train_loss: 0.4581\n",
      "11/88, train_loss: 0.4622\n",
      "12/88, train_loss: 0.5027\n",
      "13/88, train_loss: 0.4483\n",
      "14/88, train_loss: 0.5026\n",
      "15/88, train_loss: 0.5017\n",
      "16/88, train_loss: 0.5017\n",
      "17/88, train_loss: 0.4526\n",
      "18/88, train_loss: 0.5019\n",
      "19/88, train_loss: 0.4992\n",
      "20/88, train_loss: 0.4801\n",
      "21/88, train_loss: 0.4936\n",
      "22/88, train_loss: 0.4949\n",
      "23/88, train_loss: 0.4953\n",
      "24/88, train_loss: 0.5014\n",
      "25/88, train_loss: 0.5015\n",
      "26/88, train_loss: 0.4625\n",
      "27/88, train_loss: 0.5016\n",
      "28/88, train_loss: 0.5020\n",
      "29/88, train_loss: 0.5016\n",
      "30/88, train_loss: 0.5020\n",
      "31/88, train_loss: 0.5016\n",
      "32/88, train_loss: 0.5025\n",
      "33/88, train_loss: 0.5019\n",
      "34/88, train_loss: 0.4915\n",
      "35/88, train_loss: 0.4559\n",
      "36/88, train_loss: 0.4991\n",
      "37/88, train_loss: 0.4451\n",
      "38/88, train_loss: 0.5011\n",
      "39/88, train_loss: 0.5015\n",
      "40/88, train_loss: 0.4076\n",
      "41/88, train_loss: 0.5015\n",
      "42/88, train_loss: 0.5014\n",
      "43/88, train_loss: 0.5021\n",
      "44/88, train_loss: 0.4152\n",
      "45/88, train_loss: 0.5032\n",
      "46/88, train_loss: 0.4753\n",
      "47/88, train_loss: 0.4336\n",
      "48/88, train_loss: 0.4792\n",
      "49/88, train_loss: 0.5024\n",
      "50/88, train_loss: 0.5016\n",
      "51/88, train_loss: 0.4895\n",
      "52/88, train_loss: 0.4090\n",
      "53/88, train_loss: 0.5016\n",
      "54/88, train_loss: 0.5014\n",
      "55/88, train_loss: 0.4496\n",
      "56/88, train_loss: 0.5017\n",
      "57/88, train_loss: 0.4921\n",
      "58/88, train_loss: 0.5013\n",
      "59/88, train_loss: 0.5014\n",
      "60/88, train_loss: 0.4412\n",
      "61/88, train_loss: 0.5022\n",
      "62/88, train_loss: 0.5018\n",
      "63/88, train_loss: 0.5013\n",
      "64/88, train_loss: 0.5032\n",
      "65/88, train_loss: 0.4916\n",
      "66/88, train_loss: 0.4345\n",
      "67/88, train_loss: 0.5021\n",
      "68/88, train_loss: 0.4534\n",
      "69/88, train_loss: 0.5017\n",
      "70/88, train_loss: 0.5014\n",
      "71/88, train_loss: 0.5015\n",
      "72/88, train_loss: 0.5014\n",
      "73/88, train_loss: 0.5015\n",
      "74/88, train_loss: 0.5015\n",
      "75/88, train_loss: 0.5014\n",
      "76/88, train_loss: 0.5013\n",
      "77/88, train_loss: 0.5016\n",
      "78/88, train_loss: 0.5014\n",
      "79/88, train_loss: 0.4117\n",
      "80/88, train_loss: 0.5013\n",
      "81/88, train_loss: 0.4572\n",
      "82/88, train_loss: 0.4671\n",
      "83/88, train_loss: 0.5013\n",
      "84/88, train_loss: 0.4691\n",
      "85/88, train_loss: 0.5013\n",
      "86/88, train_loss: 0.5012\n",
      "87/88, train_loss: 0.5013\n",
      "88/88, train_loss: 0.4646\n",
      "Epoch 13 average loss: 0.4870\n",
      "\n",
      "Epoch 14/300\n",
      "1/88, train_loss: 0.5015\n",
      "2/88, train_loss: 0.4504\n",
      "3/88, train_loss: 0.5014\n",
      "4/88, train_loss: 0.4872\n",
      "5/88, train_loss: 0.4856\n",
      "6/88, train_loss: 0.4903\n",
      "7/88, train_loss: 0.5017\n",
      "8/88, train_loss: 0.5015\n",
      "9/88, train_loss: 0.5013\n",
      "10/88, train_loss: 0.5019\n",
      "11/88, train_loss: 0.4963\n",
      "12/88, train_loss: 0.5014\n",
      "13/88, train_loss: 0.4215\n",
      "14/88, train_loss: 0.5020\n",
      "15/88, train_loss: 0.4572\n",
      "16/88, train_loss: 0.4217\n",
      "17/88, train_loss: 0.4679\n",
      "18/88, train_loss: 0.5013\n",
      "19/88, train_loss: 0.5014\n",
      "20/88, train_loss: 0.4074\n",
      "21/88, train_loss: 0.5014\n",
      "22/88, train_loss: 0.4559\n",
      "23/88, train_loss: 0.5013\n",
      "24/88, train_loss: 0.5011\n",
      "25/88, train_loss: 0.5010\n",
      "26/88, train_loss: 0.4717\n",
      "27/88, train_loss: 0.4273\n",
      "28/88, train_loss: 0.4530\n",
      "29/88, train_loss: 0.4995\n",
      "30/88, train_loss: 0.5015\n",
      "31/88, train_loss: 0.5015\n",
      "32/88, train_loss: 0.4895\n",
      "33/88, train_loss: 0.5012\n",
      "34/88, train_loss: 0.4935\n",
      "35/88, train_loss: 0.5018\n",
      "36/88, train_loss: 0.4403\n",
      "37/88, train_loss: 0.5023\n",
      "38/88, train_loss: 0.5012\n",
      "39/88, train_loss: 0.5017\n",
      "40/88, train_loss: 0.4387\n",
      "41/88, train_loss: 0.5011\n",
      "42/88, train_loss: 0.4504\n",
      "43/88, train_loss: 0.4776\n",
      "44/88, train_loss: 0.4979\n",
      "45/88, train_loss: 0.5012\n",
      "46/88, train_loss: 0.5010\n",
      "47/88, train_loss: 0.4821\n",
      "48/88, train_loss: 0.5011\n",
      "49/88, train_loss: 0.5011\n",
      "50/88, train_loss: 0.5010\n",
      "51/88, train_loss: 0.4997\n",
      "52/88, train_loss: 0.4769\n",
      "53/88, train_loss: 0.4887\n",
      "54/88, train_loss: 0.4896\n",
      "55/88, train_loss: 0.4987\n",
      "56/88, train_loss: 0.5010\n",
      "57/88, train_loss: 0.4991\n",
      "58/88, train_loss: 0.5010\n",
      "59/88, train_loss: 0.5011\n",
      "60/88, train_loss: 0.4771\n",
      "61/88, train_loss: 0.5011\n",
      "62/88, train_loss: 0.5011\n",
      "63/88, train_loss: 0.5015\n",
      "64/88, train_loss: 0.4132\n",
      "65/88, train_loss: 0.4722\n",
      "66/88, train_loss: 0.5011\n",
      "67/88, train_loss: 0.5018\n",
      "68/88, train_loss: 0.4525\n",
      "69/88, train_loss: 0.5015\n",
      "70/88, train_loss: 0.4873\n",
      "71/88, train_loss: 0.5026\n",
      "72/88, train_loss: 0.5027\n",
      "73/88, train_loss: 0.4797\n",
      "74/88, train_loss: 0.4744\n",
      "75/88, train_loss: 0.4287\n",
      "76/88, train_loss: 0.5013\n",
      "77/88, train_loss: 0.5014\n",
      "78/88, train_loss: 0.5013\n",
      "79/88, train_loss: 0.5011\n",
      "80/88, train_loss: 0.4747\n",
      "81/88, train_loss: 0.4999\n",
      "82/88, train_loss: 0.4436\n",
      "83/88, train_loss: 0.5015\n",
      "84/88, train_loss: 0.5020\n",
      "85/88, train_loss: 0.5019\n",
      "86/88, train_loss: 0.5014\n",
      "87/88, train_loss: 0.4423\n",
      "88/88, train_loss: 0.5012\n",
      "Epoch 14 average loss: 0.4856\n",
      "\n",
      "Epoch 15/300\n",
      "1/88, train_loss: 0.5016\n",
      "2/88, train_loss: 0.4823\n",
      "3/88, train_loss: 0.5013\n",
      "4/88, train_loss: 0.4304\n",
      "5/88, train_loss: 0.5012\n",
      "6/88, train_loss: 0.5011\n",
      "7/88, train_loss: 0.5011\n",
      "8/88, train_loss: 0.5010\n",
      "9/88, train_loss: 0.4752\n",
      "10/88, train_loss: 0.5011\n",
      "11/88, train_loss: 0.5015\n",
      "12/88, train_loss: 0.4685\n",
      "13/88, train_loss: 0.4424\n",
      "14/88, train_loss: 0.3704\n",
      "15/88, train_loss: 0.5013\n",
      "16/88, train_loss: 0.4822\n",
      "17/88, train_loss: 0.5033\n",
      "18/88, train_loss: 0.5038\n",
      "19/88, train_loss: 0.5010\n",
      "20/88, train_loss: 0.5044\n",
      "21/88, train_loss: 0.5030\n",
      "22/88, train_loss: 0.5047\n",
      "23/88, train_loss: 0.5034\n",
      "24/88, train_loss: 0.5032\n",
      "25/88, train_loss: 0.4583\n",
      "26/88, train_loss: 0.4533\n",
      "27/88, train_loss: 0.5023\n",
      "28/88, train_loss: 0.5016\n",
      "29/88, train_loss: 0.4950\n",
      "30/88, train_loss: 0.5010\n",
      "31/88, train_loss: 0.5011\n",
      "32/88, train_loss: 0.5011\n",
      "33/88, train_loss: 0.5011\n",
      "34/88, train_loss: 0.5008\n",
      "35/88, train_loss: 0.4471\n",
      "36/88, train_loss: 0.5010\n",
      "37/88, train_loss: 0.4524\n",
      "38/88, train_loss: 0.5011\n",
      "39/88, train_loss: 0.5012\n",
      "40/88, train_loss: 0.5014\n",
      "41/88, train_loss: 0.3917\n",
      "42/88, train_loss: 0.4658\n",
      "43/88, train_loss: 0.4942\n",
      "44/88, train_loss: 0.4363\n",
      "45/88, train_loss: 0.4321\n",
      "46/88, train_loss: 0.4049\n",
      "47/88, train_loss: 0.4849\n",
      "48/88, train_loss: 0.4489\n",
      "49/88, train_loss: 0.5010\n",
      "50/88, train_loss: 0.4649\n",
      "51/88, train_loss: 0.4317\n",
      "52/88, train_loss: 0.4981\n",
      "53/88, train_loss: 0.5012\n",
      "54/88, train_loss: 0.5002\n",
      "55/88, train_loss: 0.4846\n",
      "56/88, train_loss: 0.4138\n",
      "57/88, train_loss: 0.5013\n",
      "58/88, train_loss: 0.4703\n",
      "59/88, train_loss: 0.5011\n",
      "60/88, train_loss: 0.4842\n",
      "61/88, train_loss: 0.4990\n",
      "62/88, train_loss: 0.5015\n",
      "63/88, train_loss: 0.5014\n",
      "64/88, train_loss: 0.5016\n",
      "65/88, train_loss: 0.4920\n",
      "66/88, train_loss: 0.5019\n",
      "67/88, train_loss: 0.4990\n",
      "68/88, train_loss: 0.5019\n",
      "69/88, train_loss: 0.5022\n",
      "70/88, train_loss: 0.5017\n",
      "71/88, train_loss: 0.4854\n",
      "72/88, train_loss: 0.5022\n",
      "73/88, train_loss: 0.4961\n",
      "74/88, train_loss: 0.5026\n",
      "75/88, train_loss: 0.4591\n",
      "76/88, train_loss: 0.5022\n",
      "77/88, train_loss: 0.5019\n",
      "78/88, train_loss: 0.5018\n",
      "79/88, train_loss: 0.5016\n",
      "80/88, train_loss: 0.5023\n",
      "81/88, train_loss: 0.4879\n",
      "82/88, train_loss: 0.5010\n",
      "83/88, train_loss: 0.5010\n",
      "84/88, train_loss: 0.5011\n",
      "85/88, train_loss: 0.4409\n",
      "86/88, train_loss: 0.4813\n",
      "87/88, train_loss: 0.3724\n",
      "88/88, train_loss: 0.4788\n",
      "Epoch 15 average loss: 0.4834\n",
      "Validation loss: 0.4951\n",
      "Current epoch: 15, current mean dice: 0.1814, best mean dice: 0.1861 at epoch 10\n",
      "\n",
      "Epoch 16/300\n",
      "1/88, train_loss: 0.5004\n",
      "2/88, train_loss: 0.4894\n",
      "3/88, train_loss: 0.4470\n",
      "4/88, train_loss: 0.4779\n",
      "5/88, train_loss: 0.4795\n",
      "6/88, train_loss: 0.4715\n",
      "7/88, train_loss: 0.4926\n",
      "8/88, train_loss: 0.4467\n",
      "9/88, train_loss: 0.4838\n",
      "10/88, train_loss: 0.5013\n",
      "11/88, train_loss: 0.5013\n",
      "12/88, train_loss: 0.5012\n",
      "13/88, train_loss: 0.5019\n",
      "14/88, train_loss: 0.4794\n",
      "15/88, train_loss: 0.4041\n",
      "16/88, train_loss: 0.4201\n",
      "17/88, train_loss: 0.5008\n",
      "18/88, train_loss: 0.5012\n",
      "19/88, train_loss: 0.4182\n",
      "20/88, train_loss: 0.5008\n",
      "21/88, train_loss: 0.5008\n",
      "22/88, train_loss: 0.5008\n",
      "23/88, train_loss: 0.4765\n",
      "24/88, train_loss: 0.4769\n",
      "25/88, train_loss: 0.5008\n",
      "26/88, train_loss: 0.4991\n",
      "27/88, train_loss: 0.5008\n",
      "28/88, train_loss: 0.5008\n",
      "29/88, train_loss: 0.5010\n",
      "30/88, train_loss: 0.5009\n",
      "31/88, train_loss: 0.4602\n",
      "32/88, train_loss: 0.5009\n",
      "33/88, train_loss: 0.3936\n",
      "34/88, train_loss: 0.4942\n",
      "35/88, train_loss: 0.4307\n",
      "36/88, train_loss: 0.5011\n",
      "37/88, train_loss: 0.5009\n",
      "38/88, train_loss: 0.5008\n",
      "39/88, train_loss: 0.4357\n",
      "40/88, train_loss: 0.5008\n",
      "41/88, train_loss: 0.4627\n",
      "42/88, train_loss: 0.4916\n",
      "43/88, train_loss: 0.5008\n",
      "44/88, train_loss: 0.5007\n",
      "45/88, train_loss: 0.5008\n",
      "46/88, train_loss: 0.5007\n",
      "47/88, train_loss: 0.5009\n",
      "48/88, train_loss: 0.5008\n",
      "49/88, train_loss: 0.5008\n",
      "50/88, train_loss: 0.5007\n",
      "51/88, train_loss: 0.4484\n",
      "52/88, train_loss: 0.5008\n",
      "53/88, train_loss: 0.4367\n",
      "54/88, train_loss: 0.5008\n",
      "55/88, train_loss: 0.4968\n",
      "56/88, train_loss: 0.5016\n",
      "57/88, train_loss: 0.4973\n",
      "58/88, train_loss: 0.4480\n",
      "59/88, train_loss: 0.5013\n",
      "60/88, train_loss: 0.4145\n",
      "61/88, train_loss: 0.5019\n",
      "62/88, train_loss: 0.5012\n",
      "63/88, train_loss: 0.4625\n",
      "64/88, train_loss: 0.4116\n",
      "65/88, train_loss: 0.5030\n",
      "66/88, train_loss: 0.5026\n",
      "67/88, train_loss: 0.5034\n",
      "68/88, train_loss: 0.4635\n",
      "69/88, train_loss: 0.5046\n",
      "70/88, train_loss: 0.4608\n",
      "71/88, train_loss: 0.5035\n",
      "72/88, train_loss: 0.4767\n",
      "73/88, train_loss: 0.5025\n",
      "74/88, train_loss: 0.5010\n",
      "75/88, train_loss: 0.4426\n",
      "76/88, train_loss: 0.4891\n",
      "77/88, train_loss: 0.5008\n",
      "78/88, train_loss: 0.3916\n",
      "79/88, train_loss: 0.4825\n",
      "80/88, train_loss: 0.5007\n",
      "81/88, train_loss: 0.5007\n",
      "82/88, train_loss: 0.5007\n",
      "83/88, train_loss: 0.5004\n",
      "84/88, train_loss: 0.4678\n",
      "85/88, train_loss: 0.4998\n",
      "86/88, train_loss: 0.5008\n",
      "87/88, train_loss: 0.5007\n",
      "88/88, train_loss: 0.4934\n",
      "Epoch 16 average loss: 0.4827\n",
      "\n",
      "Epoch 17/300\n",
      "1/88, train_loss: 0.3904\n",
      "2/88, train_loss: 0.4653\n",
      "3/88, train_loss: 0.4271\n",
      "4/88, train_loss: 0.4504\n",
      "5/88, train_loss: 0.5008\n",
      "6/88, train_loss: 0.4839\n",
      "7/88, train_loss: 0.5008\n",
      "8/88, train_loss: 0.5010\n",
      "9/88, train_loss: 0.5010\n",
      "10/88, train_loss: 0.4981\n",
      "11/88, train_loss: 0.4296\n",
      "12/88, train_loss: 0.4669\n",
      "13/88, train_loss: 0.4974\n",
      "14/88, train_loss: 0.5011\n",
      "15/88, train_loss: 0.4621\n",
      "16/88, train_loss: 0.4083\n",
      "17/88, train_loss: 0.4332\n",
      "18/88, train_loss: 0.4503\n",
      "19/88, train_loss: 0.5009\n",
      "20/88, train_loss: 0.3703\n",
      "21/88, train_loss: 0.5009\n",
      "22/88, train_loss: 0.3918\n",
      "23/88, train_loss: 0.5012\n",
      "24/88, train_loss: 0.4237\n",
      "25/88, train_loss: 0.5017\n",
      "26/88, train_loss: 0.4551\n",
      "27/88, train_loss: 0.4521\n",
      "28/88, train_loss: 0.5018\n",
      "29/88, train_loss: 0.5022\n",
      "30/88, train_loss: 0.5011\n",
      "31/88, train_loss: 0.5011\n",
      "32/88, train_loss: 0.5015\n",
      "33/88, train_loss: 0.4101\n",
      "34/88, train_loss: 0.5007\n",
      "35/88, train_loss: 0.4316\n",
      "36/88, train_loss: 0.4809\n",
      "37/88, train_loss: 0.4760\n",
      "38/88, train_loss: 0.5006\n",
      "39/88, train_loss: 0.5007\n",
      "40/88, train_loss: 0.4823\n",
      "41/88, train_loss: 0.5006\n",
      "42/88, train_loss: 0.5008\n",
      "43/88, train_loss: 0.5006\n",
      "44/88, train_loss: 0.4999\n",
      "45/88, train_loss: 0.5006\n",
      "46/88, train_loss: 0.4078\n",
      "47/88, train_loss: 0.4843\n",
      "48/88, train_loss: 0.4772\n",
      "49/88, train_loss: 0.5008\n",
      "50/88, train_loss: 0.5008\n",
      "51/88, train_loss: 0.4753\n",
      "52/88, train_loss: 0.4424\n",
      "53/88, train_loss: 0.5006\n",
      "54/88, train_loss: 0.4699\n",
      "55/88, train_loss: 0.5006\n",
      "56/88, train_loss: 0.5006\n",
      "57/88, train_loss: 0.5006\n",
      "58/88, train_loss: 0.4365\n",
      "59/88, train_loss: 0.5005\n",
      "60/88, train_loss: 0.5005\n",
      "61/88, train_loss: 0.5005\n",
      "62/88, train_loss: 0.5005\n",
      "63/88, train_loss: 0.4379\n",
      "64/88, train_loss: 0.5006\n",
      "65/88, train_loss: 0.4413\n",
      "66/88, train_loss: 0.4904\n",
      "67/88, train_loss: 0.5006\n",
      "68/88, train_loss: 0.5007\n",
      "69/88, train_loss: 0.4696\n",
      "70/88, train_loss: 0.4943\n",
      "71/88, train_loss: 0.4089\n",
      "72/88, train_loss: 0.5006\n",
      "73/88, train_loss: 0.5007\n",
      "74/88, train_loss: 0.5009\n",
      "75/88, train_loss: 0.4456\n",
      "76/88, train_loss: 0.5006\n",
      "77/88, train_loss: 0.5006\n",
      "78/88, train_loss: 0.5009\n",
      "79/88, train_loss: 0.4516\n",
      "80/88, train_loss: 0.5005\n",
      "81/88, train_loss: 0.5006\n",
      "82/88, train_loss: 0.4974\n",
      "83/88, train_loss: 0.5005\n",
      "84/88, train_loss: 0.5008\n",
      "85/88, train_loss: 0.5007\n",
      "86/88, train_loss: 0.4343\n",
      "87/88, train_loss: 0.5005\n",
      "88/88, train_loss: 0.5005\n",
      "Epoch 17 average loss: 0.4777\n",
      "\n",
      "Epoch 18/300\n",
      "1/88, train_loss: 0.4794\n",
      "2/88, train_loss: 0.4301\n",
      "3/88, train_loss: 0.4697\n",
      "4/88, train_loss: 0.4460\n",
      "5/88, train_loss: 0.5005\n",
      "6/88, train_loss: 0.4478\n",
      "7/88, train_loss: 0.5005\n",
      "8/88, train_loss: 0.5007\n",
      "9/88, train_loss: 0.4939\n",
      "10/88, train_loss: 0.3864\n",
      "11/88, train_loss: 0.5005\n",
      "12/88, train_loss: 0.4600\n",
      "13/88, train_loss: 0.4844\n",
      "14/88, train_loss: 0.5004\n",
      "15/88, train_loss: 0.3909\n",
      "16/88, train_loss: 0.5006\n",
      "17/88, train_loss: 0.4981\n",
      "18/88, train_loss: 0.3900\n",
      "19/88, train_loss: 0.5004\n",
      "20/88, train_loss: 0.4428\n",
      "21/88, train_loss: 0.5004\n",
      "22/88, train_loss: 0.5006\n",
      "23/88, train_loss: 0.5004\n",
      "24/88, train_loss: 0.5005\n",
      "25/88, train_loss: 0.4965\n",
      "26/88, train_loss: 0.3750\n",
      "27/88, train_loss: 0.4479\n",
      "28/88, train_loss: 0.4404\n",
      "29/88, train_loss: 0.5006\n",
      "30/88, train_loss: 0.5007\n",
      "31/88, train_loss: 0.5008\n",
      "32/88, train_loss: 0.5012\n",
      "33/88, train_loss: 0.5006\n",
      "34/88, train_loss: 0.4917\n",
      "35/88, train_loss: 0.4257\n",
      "36/88, train_loss: 0.5004\n",
      "37/88, train_loss: 0.4271\n",
      "38/88, train_loss: 0.5008\n",
      "39/88, train_loss: 0.4786\n",
      "40/88, train_loss: 0.4177\n",
      "41/88, train_loss: 0.4991\n",
      "42/88, train_loss: 0.5002\n",
      "43/88, train_loss: 0.4644\n",
      "44/88, train_loss: 0.4280\n",
      "45/88, train_loss: 0.3364\n",
      "46/88, train_loss: 0.5011\n",
      "47/88, train_loss: 0.5005\n",
      "48/88, train_loss: 0.5004\n",
      "49/88, train_loss: 0.5005\n",
      "50/88, train_loss: 0.4089\n",
      "51/88, train_loss: 0.4662\n",
      "52/88, train_loss: 0.5004\n",
      "53/88, train_loss: 0.5008\n",
      "54/88, train_loss: 0.4369\n",
      "55/88, train_loss: 0.4119\n",
      "56/88, train_loss: 0.4895\n",
      "57/88, train_loss: 0.5010\n",
      "58/88, train_loss: 0.4590\n",
      "59/88, train_loss: 0.4292\n",
      "60/88, train_loss: 0.5006\n",
      "61/88, train_loss: 0.5005\n",
      "62/88, train_loss: 0.4188\n",
      "63/88, train_loss: 0.5006\n",
      "64/88, train_loss: 0.5005\n",
      "65/88, train_loss: 0.5004\n",
      "66/88, train_loss: 0.5004\n",
      "67/88, train_loss: 0.5004\n",
      "68/88, train_loss: 0.5004\n",
      "69/88, train_loss: 0.4744\n",
      "70/88, train_loss: 0.5004\n",
      "71/88, train_loss: 0.4940\n",
      "72/88, train_loss: 0.5004\n",
      "73/88, train_loss: 0.5004\n",
      "74/88, train_loss: 0.4944\n",
      "75/88, train_loss: 0.4999\n",
      "76/88, train_loss: 0.5004\n",
      "77/88, train_loss: 0.5004\n",
      "78/88, train_loss: 0.4646\n",
      "79/88, train_loss: 0.5006\n",
      "80/88, train_loss: 0.5008\n",
      "81/88, train_loss: 0.5006\n",
      "82/88, train_loss: 0.4088\n",
      "83/88, train_loss: 0.5007\n",
      "84/88, train_loss: 0.4416\n",
      "85/88, train_loss: 0.5013\n",
      "86/88, train_loss: 0.5005\n",
      "87/88, train_loss: 0.5006\n",
      "88/88, train_loss: 0.5012\n",
      "Epoch 18 average loss: 0.4758\n",
      "\n",
      "Epoch 19/300\n",
      "1/88, train_loss: 0.3897\n",
      "2/88, train_loss: 0.4802\n",
      "3/88, train_loss: 0.3494\n",
      "4/88, train_loss: 0.4989\n",
      "5/88, train_loss: 0.4176\n",
      "6/88, train_loss: 0.5004\n",
      "7/88, train_loss: 0.5004\n",
      "8/88, train_loss: 0.5004\n",
      "9/88, train_loss: 0.5004\n",
      "10/88, train_loss: 0.5010\n",
      "11/88, train_loss: 0.4995\n",
      "12/88, train_loss: 0.4949\n",
      "13/88, train_loss: 0.4609\n",
      "14/88, train_loss: 0.4732\n",
      "15/88, train_loss: 0.5004\n",
      "16/88, train_loss: 0.3998\n",
      "17/88, train_loss: 0.5004\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5004\n",
      "20/88, train_loss: 0.5004\n",
      "21/88, train_loss: 0.4834\n",
      "22/88, train_loss: 0.4975\n",
      "23/88, train_loss: 0.3913\n",
      "24/88, train_loss: 0.5004\n",
      "25/88, train_loss: 0.5005\n",
      "26/88, train_loss: 0.4859\n",
      "27/88, train_loss: 0.5011\n",
      "28/88, train_loss: 0.5011\n",
      "29/88, train_loss: 0.5012\n",
      "30/88, train_loss: 0.4670\n",
      "31/88, train_loss: 0.4663\n",
      "32/88, train_loss: 0.5018\n",
      "33/88, train_loss: 0.5017\n",
      "34/88, train_loss: 0.5011\n",
      "35/88, train_loss: 0.4974\n",
      "36/88, train_loss: 0.5016\n",
      "37/88, train_loss: 0.5012\n",
      "38/88, train_loss: 0.5008\n",
      "39/88, train_loss: 0.5007\n",
      "40/88, train_loss: 0.4530\n",
      "41/88, train_loss: 0.5015\n",
      "42/88, train_loss: 0.5006\n",
      "43/88, train_loss: 0.5006\n",
      "44/88, train_loss: 0.4157\n",
      "45/88, train_loss: 0.5004\n",
      "46/88, train_loss: 0.5005\n",
      "47/88, train_loss: 0.5005\n",
      "48/88, train_loss: 0.5005\n",
      "49/88, train_loss: 0.4134\n",
      "50/88, train_loss: 0.5006\n",
      "51/88, train_loss: 0.5006\n",
      "52/88, train_loss: 0.5004\n",
      "53/88, train_loss: 0.5005\n",
      "54/88, train_loss: 0.5007\n",
      "55/88, train_loss: 0.5004\n",
      "56/88, train_loss: 0.5004\n",
      "57/88, train_loss: 0.5006\n",
      "58/88, train_loss: 0.4382\n",
      "59/88, train_loss: 0.5004\n",
      "60/88, train_loss: 0.4423\n",
      "61/88, train_loss: 0.4926\n",
      "62/88, train_loss: 0.4651\n",
      "63/88, train_loss: 0.5009\n",
      "64/88, train_loss: 0.5013\n",
      "65/88, train_loss: 0.4914\n",
      "66/88, train_loss: 0.4168\n",
      "67/88, train_loss: 0.3337\n",
      "68/88, train_loss: 0.4743\n",
      "69/88, train_loss: 0.5004\n",
      "70/88, train_loss: 0.4271\n",
      "71/88, train_loss: 0.5004\n",
      "72/88, train_loss: 0.5004\n",
      "73/88, train_loss: 0.5004\n",
      "74/88, train_loss: 0.4710\n",
      "75/88, train_loss: 0.5004\n",
      "76/88, train_loss: 0.5005\n",
      "77/88, train_loss: 0.5006\n",
      "78/88, train_loss: 0.5005\n",
      "79/88, train_loss: 0.5009\n",
      "80/88, train_loss: 0.3967\n",
      "81/88, train_loss: 0.5007\n",
      "82/88, train_loss: 0.5007\n",
      "83/88, train_loss: 0.4707\n",
      "84/88, train_loss: 0.4317\n",
      "85/88, train_loss: 0.5006\n",
      "86/88, train_loss: 0.4947\n",
      "87/88, train_loss: 0.4723\n",
      "88/88, train_loss: 0.3697\n",
      "Epoch 19 average loss: 0.4791\n",
      "\n",
      "Epoch 20/300\n",
      "1/88, train_loss: 0.5006\n",
      "2/88, train_loss: 0.4387\n",
      "3/88, train_loss: 0.5005\n",
      "4/88, train_loss: 0.5004\n",
      "5/88, train_loss: 0.5006\n",
      "6/88, train_loss: 0.5003\n",
      "7/88, train_loss: 0.5004\n",
      "8/88, train_loss: 0.4149\n",
      "9/88, train_loss: 0.5004\n",
      "10/88, train_loss: 0.4938\n",
      "11/88, train_loss: 0.3933\n",
      "12/88, train_loss: 0.5004\n",
      "13/88, train_loss: 0.4898\n",
      "14/88, train_loss: 0.5003\n",
      "15/88, train_loss: 0.5004\n",
      "16/88, train_loss: 0.5003\n",
      "17/88, train_loss: 0.4790\n",
      "18/88, train_loss: 0.5004\n",
      "19/88, train_loss: 0.5004\n",
      "20/88, train_loss: 0.5003\n",
      "21/88, train_loss: 0.3988\n",
      "22/88, train_loss: 0.4443\n",
      "23/88, train_loss: 0.4534\n",
      "24/88, train_loss: 0.4863\n",
      "25/88, train_loss: 0.5004\n",
      "26/88, train_loss: 0.5007\n",
      "27/88, train_loss: 0.5006\n",
      "28/88, train_loss: 0.5007\n",
      "29/88, train_loss: 0.5007\n",
      "30/88, train_loss: 0.5008\n",
      "31/88, train_loss: 0.5011\n",
      "32/88, train_loss: 0.4666\n",
      "33/88, train_loss: 0.5006\n",
      "34/88, train_loss: 0.5006\n",
      "35/88, train_loss: 0.5009\n",
      "36/88, train_loss: 0.4369\n",
      "37/88, train_loss: 0.4301\n",
      "38/88, train_loss: 0.5007\n",
      "39/88, train_loss: 0.3994\n",
      "40/88, train_loss: 0.3875\n",
      "41/88, train_loss: 0.5007\n",
      "42/88, train_loss: 0.5005\n",
      "43/88, train_loss: 0.5003\n",
      "44/88, train_loss: 0.4068\n",
      "45/88, train_loss: 0.4457\n",
      "46/88, train_loss: 0.5004\n",
      "47/88, train_loss: 0.5005\n",
      "48/88, train_loss: 0.4767\n",
      "49/88, train_loss: 0.4081\n",
      "50/88, train_loss: 0.4425\n",
      "51/88, train_loss: 0.5008\n",
      "52/88, train_loss: 0.5008\n",
      "53/88, train_loss: 0.4912\n",
      "54/88, train_loss: 0.4892\n",
      "55/88, train_loss: 0.4932\n",
      "56/88, train_loss: 0.4854\n",
      "57/88, train_loss: 0.4341\n",
      "58/88, train_loss: 0.3700\n",
      "59/88, train_loss: 0.3619\n",
      "60/88, train_loss: 0.5003\n",
      "61/88, train_loss: 0.3018\n",
      "62/88, train_loss: 0.5003\n",
      "63/88, train_loss: 0.4261\n",
      "64/88, train_loss: 0.5003\n",
      "65/88, train_loss: 0.4124\n",
      "66/88, train_loss: 0.5006\n",
      "67/88, train_loss: 0.5005\n",
      "68/88, train_loss: 0.5003\n",
      "69/88, train_loss: 0.3648\n",
      "70/88, train_loss: 0.5004\n",
      "71/88, train_loss: 0.4371\n",
      "72/88, train_loss: 0.4871\n",
      "73/88, train_loss: 0.3813\n",
      "74/88, train_loss: 0.5005\n",
      "75/88, train_loss: 0.4426\n",
      "76/88, train_loss: 0.5004\n",
      "77/88, train_loss: 0.4146\n",
      "78/88, train_loss: 0.5010\n",
      "79/88, train_loss: 0.5008\n",
      "80/88, train_loss: 0.5006\n",
      "81/88, train_loss: 0.5010\n",
      "82/88, train_loss: 0.5014\n",
      "83/88, train_loss: 0.5012\n",
      "84/88, train_loss: 0.5021\n",
      "85/88, train_loss: 0.5005\n",
      "86/88, train_loss: 0.3706\n",
      "87/88, train_loss: 0.5028\n",
      "88/88, train_loss: 0.5010\n",
      "Epoch 20 average loss: 0.4715\n",
      "Validation loss: 0.4963\n",
      "Current epoch: 20, current mean dice: 0.1047, best mean dice: 0.1861 at epoch 10\n",
      "\n",
      "Epoch 21/300\n",
      "1/88, train_loss: 0.5018\n",
      "2/88, train_loss: 0.5023\n",
      "3/88, train_loss: 0.5022\n",
      "4/88, train_loss: 0.5004\n",
      "5/88, train_loss: 0.5017\n",
      "6/88, train_loss: 0.5006\n",
      "7/88, train_loss: 0.5004\n",
      "8/88, train_loss: 0.3974\n",
      "9/88, train_loss: 0.4846\n",
      "10/88, train_loss: 0.5015\n",
      "11/88, train_loss: 0.3927\n",
      "12/88, train_loss: 0.4181\n",
      "13/88, train_loss: 0.5009\n",
      "14/88, train_loss: 0.4745\n",
      "15/88, train_loss: 0.4784\n",
      "16/88, train_loss: 0.5004\n",
      "17/88, train_loss: 0.3721\n",
      "18/88, train_loss: 0.4456\n",
      "19/88, train_loss: 0.5004\n",
      "20/88, train_loss: 0.4789\n",
      "21/88, train_loss: 0.4337\n",
      "22/88, train_loss: 0.5004\n",
      "23/88, train_loss: 0.5005\n",
      "24/88, train_loss: 0.5003\n",
      "25/88, train_loss: 0.5004\n",
      "26/88, train_loss: 0.5003\n",
      "27/88, train_loss: 0.4820\n",
      "28/88, train_loss: 0.5003\n",
      "29/88, train_loss: 0.4854\n",
      "30/88, train_loss: 0.3853\n",
      "31/88, train_loss: 0.5005\n",
      "32/88, train_loss: 0.5003\n",
      "33/88, train_loss: 0.5002\n",
      "34/88, train_loss: 0.4694\n",
      "35/88, train_loss: 0.5003\n",
      "36/88, train_loss: 0.5002\n",
      "37/88, train_loss: 0.4986\n",
      "38/88, train_loss: 0.5002\n",
      "39/88, train_loss: 0.4508\n",
      "40/88, train_loss: 0.4346\n",
      "41/88, train_loss: 0.5004\n",
      "42/88, train_loss: 0.4448\n",
      "43/88, train_loss: 0.3631\n",
      "44/88, train_loss: 0.5005\n",
      "45/88, train_loss: 0.2959\n",
      "46/88, train_loss: 0.5007\n",
      "47/88, train_loss: 0.2568\n",
      "48/88, train_loss: 0.4975\n",
      "49/88, train_loss: 0.4069\n",
      "50/88, train_loss: 0.5006\n",
      "51/88, train_loss: 0.4712\n",
      "52/88, train_loss: 0.5004\n",
      "53/88, train_loss: 0.5005\n",
      "54/88, train_loss: 0.5003\n",
      "55/88, train_loss: 0.5004\n",
      "56/88, train_loss: 0.5004\n",
      "57/88, train_loss: 0.5003\n",
      "58/88, train_loss: 0.5004\n",
      "59/88, train_loss: 0.5004\n",
      "60/88, train_loss: 0.4997\n",
      "61/88, train_loss: 0.4568\n",
      "62/88, train_loss: 0.5003\n",
      "63/88, train_loss: 0.5007\n",
      "64/88, train_loss: 0.4066\n",
      "65/88, train_loss: 0.4874\n",
      "66/88, train_loss: 0.4973\n",
      "67/88, train_loss: 0.5004\n",
      "68/88, train_loss: 0.5010\n",
      "69/88, train_loss: 0.3892\n",
      "70/88, train_loss: 0.4582\n",
      "71/88, train_loss: 0.4126\n",
      "72/88, train_loss: 0.5004\n",
      "73/88, train_loss: 0.4493\n",
      "74/88, train_loss: 0.4691\n",
      "75/88, train_loss: 0.5005\n",
      "76/88, train_loss: 0.5009\n",
      "77/88, train_loss: 0.4018\n",
      "78/88, train_loss: 0.5007\n",
      "79/88, train_loss: 0.5004\n",
      "80/88, train_loss: 0.4825\n",
      "81/88, train_loss: 0.5005\n",
      "82/88, train_loss: 0.4644\n",
      "83/88, train_loss: 0.4050\n",
      "84/88, train_loss: 0.4185\n",
      "85/88, train_loss: 0.5005\n",
      "86/88, train_loss: 0.5003\n",
      "87/88, train_loss: 0.3689\n",
      "88/88, train_loss: 0.3935\n",
      "Epoch 21 average loss: 0.4694\n",
      "\n",
      "Epoch 22/300\n",
      "1/88, train_loss: 0.4645\n",
      "2/88, train_loss: 0.3608\n",
      "3/88, train_loss: 0.5002\n",
      "4/88, train_loss: 0.3735\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.4395\n",
      "7/88, train_loss: 0.3907\n",
      "8/88, train_loss: 0.4128\n",
      "9/88, train_loss: 0.5008\n",
      "10/88, train_loss: 0.3859\n",
      "11/88, train_loss: 0.5012\n",
      "12/88, train_loss: 0.3769\n",
      "13/88, train_loss: 0.5009\n",
      "14/88, train_loss: 0.5015\n",
      "15/88, train_loss: 0.5010\n",
      "16/88, train_loss: 0.3900\n",
      "17/88, train_loss: 0.4473\n",
      "18/88, train_loss: 0.5015\n",
      "19/88, train_loss: 0.4919\n",
      "20/88, train_loss: 0.5012\n",
      "21/88, train_loss: 0.4778\n",
      "22/88, train_loss: 0.4892\n",
      "23/88, train_loss: 0.5013\n",
      "24/88, train_loss: 0.5010\n",
      "25/88, train_loss: 0.4735\n",
      "26/88, train_loss: 0.5004\n",
      "27/88, train_loss: 0.2800\n",
      "28/88, train_loss: 0.5003\n",
      "29/88, train_loss: 0.5003\n",
      "30/88, train_loss: 0.5004\n",
      "31/88, train_loss: 0.5003\n",
      "32/88, train_loss: 0.5003\n",
      "33/88, train_loss: 0.4066\n",
      "34/88, train_loss: 0.5003\n",
      "35/88, train_loss: 0.4397\n",
      "36/88, train_loss: 0.5003\n",
      "37/88, train_loss: 0.5003\n",
      "38/88, train_loss: 0.4332\n",
      "39/88, train_loss: 0.4049\n",
      "40/88, train_loss: 0.5007\n",
      "41/88, train_loss: 0.5005\n",
      "42/88, train_loss: 0.4006\n",
      "43/88, train_loss: 0.5003\n",
      "44/88, train_loss: 0.3815\n",
      "45/88, train_loss: 0.4194\n",
      "46/88, train_loss: 0.5004\n",
      "47/88, train_loss: 0.5004\n",
      "48/88, train_loss: 0.4503\n",
      "49/88, train_loss: 0.5002\n",
      "50/88, train_loss: 0.5003\n",
      "51/88, train_loss: 0.4313\n",
      "52/88, train_loss: 0.5003\n",
      "53/88, train_loss: 0.5002\n",
      "54/88, train_loss: 0.5004\n",
      "55/88, train_loss: 0.3751\n",
      "56/88, train_loss: 0.4572\n",
      "57/88, train_loss: 0.5004\n",
      "58/88, train_loss: 0.4959\n",
      "59/88, train_loss: 0.4713\n",
      "60/88, train_loss: 0.4830\n",
      "61/88, train_loss: 0.5003\n",
      "62/88, train_loss: 0.4932\n",
      "63/88, train_loss: 0.4991\n",
      "64/88, train_loss: 0.4597\n",
      "65/88, train_loss: 0.4119\n",
      "66/88, train_loss: 0.4116\n",
      "67/88, train_loss: 0.5002\n",
      "68/88, train_loss: 0.5004\n",
      "69/88, train_loss: 0.4579\n",
      "70/88, train_loss: 0.5003\n",
      "71/88, train_loss: 0.5003\n",
      "72/88, train_loss: 0.5003\n",
      "73/88, train_loss: 0.4225\n",
      "74/88, train_loss: 0.5005\n",
      "75/88, train_loss: 0.5004\n",
      "76/88, train_loss: 0.4643\n",
      "77/88, train_loss: 0.5007\n",
      "78/88, train_loss: 0.4964\n",
      "79/88, train_loss: 0.4039\n",
      "80/88, train_loss: 0.5004\n",
      "81/88, train_loss: 0.4954\n",
      "82/88, train_loss: 0.4928\n",
      "83/88, train_loss: 0.4180\n",
      "84/88, train_loss: 0.4441\n",
      "85/88, train_loss: 0.5003\n",
      "86/88, train_loss: 0.3532\n",
      "87/88, train_loss: 0.5002\n",
      "88/88, train_loss: 0.5002\n",
      "Epoch 22 average loss: 0.4665\n",
      "\n",
      "Epoch 23/300\n",
      "1/88, train_loss: 0.5002\n",
      "2/88, train_loss: 0.5002\n",
      "3/88, train_loss: 0.3840\n",
      "4/88, train_loss: 0.3675\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.4121\n",
      "7/88, train_loss: 0.5002\n",
      "8/88, train_loss: 0.5003\n",
      "9/88, train_loss: 0.4516\n",
      "10/88, train_loss: 0.5006\n",
      "11/88, train_loss: 0.5006\n",
      "12/88, train_loss: 0.3597\n",
      "13/88, train_loss: 0.5004\n",
      "14/88, train_loss: 0.5007\n",
      "15/88, train_loss: 0.5009\n",
      "16/88, train_loss: 0.4046\n",
      "17/88, train_loss: 0.4372\n",
      "18/88, train_loss: 0.4657\n",
      "19/88, train_loss: 0.5003\n",
      "20/88, train_loss: 0.5003\n",
      "21/88, train_loss: 0.4922\n",
      "22/88, train_loss: 0.3987\n",
      "23/88, train_loss: 0.5002\n",
      "24/88, train_loss: 0.5006\n",
      "25/88, train_loss: 0.5002\n",
      "26/88, train_loss: 0.4038\n",
      "27/88, train_loss: 0.4266\n",
      "28/88, train_loss: 0.4748\n",
      "29/88, train_loss: 0.5002\n",
      "30/88, train_loss: 0.5002\n",
      "31/88, train_loss: 0.3707\n",
      "32/88, train_loss: 0.4458\n",
      "33/88, train_loss: 0.4735\n",
      "34/88, train_loss: 0.5002\n",
      "35/88, train_loss: 0.4891\n",
      "36/88, train_loss: 0.4070\n",
      "37/88, train_loss: 0.4859\n",
      "38/88, train_loss: 0.5003\n",
      "39/88, train_loss: 0.4634\n",
      "40/88, train_loss: 0.5007\n",
      "41/88, train_loss: 0.4973\n",
      "42/88, train_loss: 0.5006\n",
      "43/88, train_loss: 0.5007\n",
      "44/88, train_loss: 0.5005\n",
      "45/88, train_loss: 0.5009\n",
      "46/88, train_loss: 0.5005\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.5005\n",
      "49/88, train_loss: 0.5006\n",
      "50/88, train_loss: 0.3951\n",
      "51/88, train_loss: 0.3053\n",
      "52/88, train_loss: 0.3959\n",
      "53/88, train_loss: 0.4682\n",
      "54/88, train_loss: 0.4687\n",
      "55/88, train_loss: 0.3739\n",
      "56/88, train_loss: 0.5003\n",
      "57/88, train_loss: 0.5003\n",
      "58/88, train_loss: 0.5002\n",
      "59/88, train_loss: 0.4549\n",
      "60/88, train_loss: 0.4764\n",
      "61/88, train_loss: 0.3341\n",
      "62/88, train_loss: 0.5002\n",
      "63/88, train_loss: 0.5003\n",
      "64/88, train_loss: 0.5002\n",
      "65/88, train_loss: 0.4072\n",
      "66/88, train_loss: 0.5004\n",
      "67/88, train_loss: 0.5003\n",
      "68/88, train_loss: 0.5002\n",
      "69/88, train_loss: 0.4390\n",
      "70/88, train_loss: 0.5002\n",
      "71/88, train_loss: 0.5002\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.4270\n",
      "74/88, train_loss: 0.4465\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.5002\n",
      "77/88, train_loss: 0.5002\n",
      "78/88, train_loss: 0.4344\n",
      "79/88, train_loss: 0.3575\n",
      "80/88, train_loss: 0.5003\n",
      "81/88, train_loss: 0.4132\n",
      "82/88, train_loss: 0.5004\n",
      "83/88, train_loss: 0.5004\n",
      "84/88, train_loss: 0.4556\n",
      "85/88, train_loss: 0.5002\n",
      "86/88, train_loss: 0.4793\n",
      "87/88, train_loss: 0.3607\n",
      "88/88, train_loss: 0.3884\n",
      "Epoch 23 average loss: 0.4649\n",
      "\n",
      "Epoch 24/300\n",
      "1/88, train_loss: 0.5002\n",
      "2/88, train_loss: 0.5002\n",
      "3/88, train_loss: 0.4007\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.5003\n",
      "6/88, train_loss: 0.5002\n",
      "7/88, train_loss: 0.5004\n",
      "8/88, train_loss: 0.5002\n",
      "9/88, train_loss: 0.5002\n",
      "10/88, train_loss: 0.5002\n",
      "11/88, train_loss: 0.5002\n",
      "12/88, train_loss: 0.5003\n",
      "13/88, train_loss: 0.4959\n",
      "14/88, train_loss: 0.5002\n",
      "15/88, train_loss: 0.5001\n",
      "16/88, train_loss: 0.5002\n",
      "17/88, train_loss: 0.5002\n",
      "18/88, train_loss: 0.5002\n",
      "19/88, train_loss: 0.2618\n",
      "20/88, train_loss: 0.5004\n",
      "21/88, train_loss: 0.4973\n",
      "22/88, train_loss: 0.5002\n",
      "23/88, train_loss: 0.4560\n",
      "24/88, train_loss: 0.3137\n",
      "25/88, train_loss: 0.5003\n",
      "26/88, train_loss: 0.5003\n",
      "27/88, train_loss: 0.5006\n",
      "28/88, train_loss: 0.3589\n",
      "29/88, train_loss: 0.3429\n",
      "30/88, train_loss: 0.5004\n",
      "31/88, train_loss: 0.4983\n",
      "32/88, train_loss: 0.4701\n",
      "33/88, train_loss: 0.5004\n",
      "34/88, train_loss: 0.5006\n",
      "35/88, train_loss: 0.5005\n",
      "36/88, train_loss: 0.5004\n",
      "37/88, train_loss: 0.2829\n",
      "38/88, train_loss: 0.4384\n",
      "39/88, train_loss: 0.3608\n",
      "40/88, train_loss: 0.4972\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.5001\n",
      "43/88, train_loss: 0.3845\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.4452\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.4502\n",
      "48/88, train_loss: 0.3321\n",
      "49/88, train_loss: 0.5003\n",
      "50/88, train_loss: 0.2728\n",
      "51/88, train_loss: 0.4921\n",
      "52/88, train_loss: 0.3439\n",
      "53/88, train_loss: 0.5004\n",
      "54/88, train_loss: 0.5003\n",
      "55/88, train_loss: 0.3766\n",
      "56/88, train_loss: 0.4010\n",
      "57/88, train_loss: 0.5005\n",
      "58/88, train_loss: 0.5003\n",
      "59/88, train_loss: 0.4154\n",
      "60/88, train_loss: 0.4757\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.4977\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.4637\n",
      "69/88, train_loss: 0.4501\n",
      "70/88, train_loss: 0.4341\n",
      "71/88, train_loss: 0.4920\n",
      "72/88, train_loss: 0.5002\n",
      "73/88, train_loss: 0.3235\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.5002\n",
      "76/88, train_loss: 0.5005\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.3019\n",
      "79/88, train_loss: 0.5003\n",
      "80/88, train_loss: 0.4291\n",
      "81/88, train_loss: 0.5003\n",
      "82/88, train_loss: 0.4974\n",
      "83/88, train_loss: 0.5002\n",
      "84/88, train_loss: 0.4316\n",
      "85/88, train_loss: 0.4706\n",
      "86/88, train_loss: 0.4788\n",
      "87/88, train_loss: 0.4831\n",
      "88/88, train_loss: 0.4410\n",
      "Epoch 24 average loss: 0.4633\n",
      "\n",
      "Epoch 25/300\n",
      "1/88, train_loss: 0.5005\n",
      "2/88, train_loss: 0.5008\n",
      "3/88, train_loss: 0.4819\n",
      "4/88, train_loss: 0.5010\n",
      "5/88, train_loss: 0.4404\n",
      "6/88, train_loss: 0.5017\n",
      "7/88, train_loss: 0.5009\n",
      "8/88, train_loss: 0.3879\n",
      "9/88, train_loss: 0.5017\n",
      "10/88, train_loss: 0.5008\n",
      "11/88, train_loss: 0.5010\n",
      "12/88, train_loss: 0.5007\n",
      "13/88, train_loss: 0.5004\n",
      "14/88, train_loss: 0.5010\n",
      "15/88, train_loss: 0.5004\n",
      "16/88, train_loss: 0.4819\n",
      "17/88, train_loss: 0.5003\n",
      "18/88, train_loss: 0.5003\n",
      "19/88, train_loss: 0.4784\n",
      "20/88, train_loss: 0.4934\n",
      "21/88, train_loss: 0.5002\n",
      "22/88, train_loss: 0.5002\n",
      "23/88, train_loss: 0.4489\n",
      "24/88, train_loss: 0.5002\n",
      "25/88, train_loss: 0.3400\n",
      "26/88, train_loss: 0.3421\n",
      "27/88, train_loss: 0.5002\n",
      "28/88, train_loss: 0.5003\n",
      "29/88, train_loss: 0.3877\n",
      "30/88, train_loss: 0.5008\n",
      "31/88, train_loss: 0.4223\n",
      "32/88, train_loss: 0.3639\n",
      "33/88, train_loss: 0.5006\n",
      "34/88, train_loss: 0.5012\n",
      "35/88, train_loss: 0.4974\n",
      "36/88, train_loss: 0.5001\n",
      "37/88, train_loss: 0.5006\n",
      "38/88, train_loss: 0.5004\n",
      "39/88, train_loss: 0.3801\n",
      "40/88, train_loss: 0.5008\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.5002\n",
      "43/88, train_loss: 0.5001\n",
      "44/88, train_loss: 0.4570\n",
      "45/88, train_loss: 0.5003\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.5004\n",
      "48/88, train_loss: 0.3674\n",
      "49/88, train_loss: 0.5011\n",
      "50/88, train_loss: 0.5007\n",
      "51/88, train_loss: 0.5004\n",
      "52/88, train_loss: 0.5005\n",
      "53/88, train_loss: 0.4107\n",
      "54/88, train_loss: 0.4303\n",
      "55/88, train_loss: 0.3805\n",
      "56/88, train_loss: 0.4481\n",
      "57/88, train_loss: 0.5003\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.4176\n",
      "60/88, train_loss: 0.4470\n",
      "61/88, train_loss: 0.4858\n",
      "62/88, train_loss: 0.3490\n",
      "63/88, train_loss: 0.3505\n",
      "64/88, train_loss: 0.5002\n",
      "65/88, train_loss: 0.4612\n",
      "66/88, train_loss: 0.4484\n",
      "67/88, train_loss: 0.4904\n",
      "68/88, train_loss: 0.3912\n",
      "69/88, train_loss: 0.5006\n",
      "70/88, train_loss: 0.5003\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.3788\n",
      "73/88, train_loss: 0.4692\n",
      "74/88, train_loss: 0.5002\n",
      "75/88, train_loss: 0.3302\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.3872\n",
      "80/88, train_loss: 0.3655\n",
      "81/88, train_loss: 0.3939\n",
      "82/88, train_loss: 0.3374\n",
      "83/88, train_loss: 0.4236\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.4006\n",
      "86/88, train_loss: 0.3660\n",
      "87/88, train_loss: 0.5002\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 25 average loss: 0.4620\n",
      "Validation loss: 0.4856\n",
      "Saved new best model\n",
      "Current epoch: 25, current mean dice: 0.2572, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 26/300\n",
      "1/88, train_loss: 0.4143\n",
      "2/88, train_loss: 0.5002\n",
      "3/88, train_loss: 0.3412\n",
      "4/88, train_loss: 0.4762\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.5003\n",
      "7/88, train_loss: 0.5005\n",
      "8/88, train_loss: 0.5002\n",
      "9/88, train_loss: 0.4811\n",
      "10/88, train_loss: 0.3784\n",
      "11/88, train_loss: 0.5008\n",
      "12/88, train_loss: 0.4681\n",
      "13/88, train_loss: 0.5004\n",
      "14/88, train_loss: 0.4322\n",
      "15/88, train_loss: 0.5002\n",
      "16/88, train_loss: 0.5007\n",
      "17/88, train_loss: 0.5003\n",
      "18/88, train_loss: 0.4915\n",
      "19/88, train_loss: 0.3728\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.2758\n",
      "26/88, train_loss: 0.4004\n",
      "27/88, train_loss: 0.3796\n",
      "28/88, train_loss: 0.3538\n",
      "29/88, train_loss: 0.4403\n",
      "30/88, train_loss: 0.3713\n",
      "31/88, train_loss: 0.3831\n",
      "32/88, train_loss: 0.3429\n",
      "33/88, train_loss: 0.5008\n",
      "34/88, train_loss: 0.5004\n",
      "35/88, train_loss: 0.4132\n",
      "36/88, train_loss: 0.5001\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.4671\n",
      "39/88, train_loss: 0.5001\n",
      "40/88, train_loss: 0.4646\n",
      "41/88, train_loss: 0.4846\n",
      "42/88, train_loss: 0.4937\n",
      "43/88, train_loss: 0.3817\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.4772\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.5002\n",
      "48/88, train_loss: 0.5002\n",
      "49/88, train_loss: 0.2899\n",
      "50/88, train_loss: 0.4619\n",
      "51/88, train_loss: 0.5002\n",
      "52/88, train_loss: 0.5004\n",
      "53/88, train_loss: 0.4406\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.5006\n",
      "56/88, train_loss: 0.5004\n",
      "57/88, train_loss: 0.5004\n",
      "58/88, train_loss: 0.4416\n",
      "59/88, train_loss: 0.3911\n",
      "60/88, train_loss: 0.3928\n",
      "61/88, train_loss: 0.5004\n",
      "62/88, train_loss: 0.3654\n",
      "63/88, train_loss: 0.3935\n",
      "64/88, train_loss: 0.4772\n",
      "65/88, train_loss: 0.5004\n",
      "66/88, train_loss: 0.3403\n",
      "67/88, train_loss: 0.4292\n",
      "68/88, train_loss: 0.5003\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.5002\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.3687\n",
      "78/88, train_loss: 0.4969\n",
      "79/88, train_loss: 0.4927\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.4259\n",
      "82/88, train_loss: 0.4405\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.4815\n",
      "86/88, train_loss: 0.5001\n",
      "87/88, train_loss: 0.4791\n",
      "88/88, train_loss: 0.5003\n",
      "Epoch 26 average loss: 0.4614\n",
      "\n",
      "Epoch 27/300\n",
      "1/88, train_loss: 0.5004\n",
      "2/88, train_loss: 0.5003\n",
      "3/88, train_loss: 0.4220\n",
      "4/88, train_loss: 0.5004\n",
      "5/88, train_loss: 0.5004\n",
      "6/88, train_loss: 0.5003\n",
      "7/88, train_loss: 0.4995\n",
      "8/88, train_loss: 0.5004\n",
      "9/88, train_loss: 0.5004\n",
      "10/88, train_loss: 0.5007\n",
      "11/88, train_loss: 0.5004\n",
      "12/88, train_loss: 0.2748\n",
      "13/88, train_loss: 0.4763\n",
      "14/88, train_loss: 0.4405\n",
      "15/88, train_loss: 0.5004\n",
      "16/88, train_loss: 0.3060\n",
      "17/88, train_loss: 0.5002\n",
      "18/88, train_loss: 0.4500\n",
      "19/88, train_loss: 0.2498\n",
      "20/88, train_loss: 0.3578\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.4440\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5003\n",
      "25/88, train_loss: 0.3773\n",
      "26/88, train_loss: 0.4755\n",
      "27/88, train_loss: 0.3902\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.5002\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.5004\n",
      "34/88, train_loss: 0.3739\n",
      "35/88, train_loss: 0.4026\n",
      "36/88, train_loss: 0.3720\n",
      "37/88, train_loss: 0.5006\n",
      "38/88, train_loss: 0.5004\n",
      "39/88, train_loss: 0.4536\n",
      "40/88, train_loss: 0.5002\n",
      "41/88, train_loss: 0.5003\n",
      "42/88, train_loss: 0.5007\n",
      "43/88, train_loss: 0.4111\n",
      "44/88, train_loss: 0.5005\n",
      "45/88, train_loss: 0.4017\n",
      "46/88, train_loss: 0.4942\n",
      "47/88, train_loss: 0.2099\n",
      "48/88, train_loss: 0.5001\n",
      "49/88, train_loss: 0.3625\n",
      "50/88, train_loss: 0.4861\n",
      "51/88, train_loss: 0.3636\n",
      "52/88, train_loss: 0.1692\n",
      "53/88, train_loss: 0.5002\n",
      "54/88, train_loss: 0.2645\n",
      "55/88, train_loss: 0.5002\n",
      "56/88, train_loss: 0.4352\n",
      "57/88, train_loss: 0.4200\n",
      "58/88, train_loss: 0.4632\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.4671\n",
      "61/88, train_loss: 0.5004\n",
      "62/88, train_loss: 0.5002\n",
      "63/88, train_loss: 0.5003\n",
      "64/88, train_loss: 0.4358\n",
      "65/88, train_loss: 0.5003\n",
      "66/88, train_loss: 0.5002\n",
      "67/88, train_loss: 0.5002\n",
      "68/88, train_loss: 0.5001\n",
      "69/88, train_loss: 0.3779\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.4091\n",
      "72/88, train_loss: 0.4863\n",
      "73/88, train_loss: 0.4699\n",
      "74/88, train_loss: 0.5002\n",
      "75/88, train_loss: 0.5002\n",
      "76/88, train_loss: 0.5002\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.5004\n",
      "79/88, train_loss: 0.4809\n",
      "80/88, train_loss: 0.5003\n",
      "81/88, train_loss: 0.4517\n",
      "82/88, train_loss: 0.5001\n",
      "83/88, train_loss: 0.2518\n",
      "84/88, train_loss: 0.5004\n",
      "85/88, train_loss: 0.5005\n",
      "86/88, train_loss: 0.5003\n",
      "87/88, train_loss: 0.5004\n",
      "88/88, train_loss: 0.5002\n",
      "Epoch 27 average loss: 0.4556\n",
      "\n",
      "Epoch 28/300\n",
      "1/88, train_loss: 0.3484\n",
      "2/88, train_loss: 0.3314\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.3041\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.4446\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.4239\n",
      "10/88, train_loss: 0.3321\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.5001\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.3437\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.5001\n",
      "18/88, train_loss: 0.4251\n",
      "19/88, train_loss: 0.4924\n",
      "20/88, train_loss: 0.4573\n",
      "21/88, train_loss: 0.3984\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5002\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.3376\n",
      "27/88, train_loss: 0.5002\n",
      "28/88, train_loss: 0.3824\n",
      "29/88, train_loss: 0.4687\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.5002\n",
      "32/88, train_loss: 0.3911\n",
      "33/88, train_loss: 0.5005\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.3260\n",
      "36/88, train_loss: 0.5001\n",
      "37/88, train_loss: 0.4378\n",
      "38/88, train_loss: 0.5003\n",
      "39/88, train_loss: 0.4805\n",
      "40/88, train_loss: 0.4173\n",
      "41/88, train_loss: 0.5004\n",
      "42/88, train_loss: 0.5006\n",
      "43/88, train_loss: 0.5003\n",
      "44/88, train_loss: 0.4633\n",
      "45/88, train_loss: 0.5005\n",
      "46/88, train_loss: 0.4939\n",
      "47/88, train_loss: 0.5003\n",
      "48/88, train_loss: 0.3985\n",
      "49/88, train_loss: 0.5004\n",
      "50/88, train_loss: 0.4353\n",
      "51/88, train_loss: 0.4830\n",
      "52/88, train_loss: 0.3996\n",
      "53/88, train_loss: 0.5004\n",
      "54/88, train_loss: 0.5003\n",
      "55/88, train_loss: 0.3360\n",
      "56/88, train_loss: 0.4189\n",
      "57/88, train_loss: 0.5002\n",
      "58/88, train_loss: 0.3778\n",
      "59/88, train_loss: 0.4894\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.3687\n",
      "62/88, train_loss: 0.3227\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.4631\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.4357\n",
      "67/88, train_loss: 0.4297\n",
      "68/88, train_loss: 0.5002\n",
      "69/88, train_loss: 0.4004\n",
      "70/88, train_loss: 0.3443\n",
      "71/88, train_loss: 0.5002\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.5004\n",
      "74/88, train_loss: 0.5004\n",
      "75/88, train_loss: 0.2914\n",
      "76/88, train_loss: 0.3347\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.3779\n",
      "79/88, train_loss: 0.4000\n",
      "80/88, train_loss: 0.5004\n",
      "81/88, train_loss: 0.5001\n",
      "82/88, train_loss: 0.5002\n",
      "83/88, train_loss: 0.5002\n",
      "84/88, train_loss: 0.3634\n",
      "85/88, train_loss: 0.4581\n",
      "86/88, train_loss: 0.5003\n",
      "87/88, train_loss: 0.5004\n",
      "88/88, train_loss: 0.3947\n",
      "Epoch 28 average loss: 0.4515\n",
      "\n",
      "Epoch 29/300\n",
      "1/88, train_loss: 0.5002\n",
      "2/88, train_loss: 0.5006\n",
      "3/88, train_loss: 0.5005\n",
      "4/88, train_loss: 0.5004\n",
      "5/88, train_loss: 0.5004\n",
      "6/88, train_loss: 0.3374\n",
      "7/88, train_loss: 0.3977\n",
      "8/88, train_loss: 0.4333\n",
      "9/88, train_loss: 0.4906\n",
      "10/88, train_loss: 0.5003\n",
      "11/88, train_loss: 0.3576\n",
      "12/88, train_loss: 0.5002\n",
      "13/88, train_loss: 0.4638\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.3174\n",
      "16/88, train_loss: 0.2643\n",
      "17/88, train_loss: 0.5003\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.3567\n",
      "20/88, train_loss: 0.3369\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.3344\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.2643\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.5002\n",
      "28/88, train_loss: 0.4270\n",
      "29/88, train_loss: 0.4064\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.3259\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.3651\n",
      "34/88, train_loss: 0.3916\n",
      "35/88, train_loss: 0.3885\n",
      "36/88, train_loss: 0.5003\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.3867\n",
      "39/88, train_loss: 0.5003\n",
      "40/88, train_loss: 0.3702\n",
      "41/88, train_loss: 0.4216\n",
      "42/88, train_loss: 0.5009\n",
      "43/88, train_loss: 0.4833\n",
      "44/88, train_loss: 0.5003\n",
      "45/88, train_loss: 0.3827\n",
      "46/88, train_loss: 0.5003\n",
      "47/88, train_loss: 0.4767\n",
      "48/88, train_loss: 0.3979\n",
      "49/88, train_loss: 0.5002\n",
      "50/88, train_loss: 0.5005\n",
      "51/88, train_loss: 0.5002\n",
      "52/88, train_loss: 0.3707\n",
      "53/88, train_loss: 0.4150\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.3865\n",
      "56/88, train_loss: 0.3592\n",
      "57/88, train_loss: 0.5003\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.4820\n",
      "62/88, train_loss: 0.5002\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.5001\n",
      "67/88, train_loss: 0.5002\n",
      "68/88, train_loss: 0.4286\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.4237\n",
      "71/88, train_loss: 0.4444\n",
      "72/88, train_loss: 0.5002\n",
      "73/88, train_loss: 0.5005\n",
      "74/88, train_loss: 0.5002\n",
      "75/88, train_loss: 0.3286\n",
      "76/88, train_loss: 0.5003\n",
      "77/88, train_loss: 0.3702\n",
      "78/88, train_loss: 0.5003\n",
      "79/88, train_loss: 0.4875\n",
      "80/88, train_loss: 0.5007\n",
      "81/88, train_loss: 0.5006\n",
      "82/88, train_loss: 0.4646\n",
      "83/88, train_loss: 0.5008\n",
      "84/88, train_loss: 0.4582\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.4203\n",
      "87/88, train_loss: 0.4451\n",
      "88/88, train_loss: 0.3345\n",
      "Epoch 29 average loss: 0.4512\n",
      "\n",
      "Epoch 30/300\n",
      "1/88, train_loss: 0.5002\n",
      "2/88, train_loss: 0.3160\n",
      "3/88, train_loss: 0.3200\n",
      "4/88, train_loss: 0.5001\n",
      "5/88, train_loss: 0.3855\n",
      "6/88, train_loss: 0.4150\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.4978\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.3876\n",
      "12/88, train_loss: 0.3872\n",
      "13/88, train_loss: 0.4070\n",
      "14/88, train_loss: 0.4427\n",
      "15/88, train_loss: 0.4541\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.5004\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5003\n",
      "20/88, train_loss: 0.4344\n",
      "21/88, train_loss: 0.3679\n",
      "22/88, train_loss: 0.4562\n",
      "23/88, train_loss: 0.3387\n",
      "24/88, train_loss: 0.3939\n",
      "25/88, train_loss: 0.5006\n",
      "26/88, train_loss: 0.5009\n",
      "27/88, train_loss: 0.5005\n",
      "28/88, train_loss: 0.5009\n",
      "29/88, train_loss: 0.4068\n",
      "30/88, train_loss: 0.5003\n",
      "31/88, train_loss: 0.3620\n",
      "32/88, train_loss: 0.5005\n",
      "33/88, train_loss: 0.5004\n",
      "34/88, train_loss: 0.5005\n",
      "35/88, train_loss: 0.3576\n",
      "36/88, train_loss: 0.5003\n",
      "37/88, train_loss: 0.4693\n",
      "38/88, train_loss: 0.4397\n",
      "39/88, train_loss: 0.5003\n",
      "40/88, train_loss: 0.5005\n",
      "41/88, train_loss: 0.5011\n",
      "42/88, train_loss: 0.3983\n",
      "43/88, train_loss: 0.5001\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.3940\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.2667\n",
      "48/88, train_loss: 0.3288\n",
      "49/88, train_loss: 0.5001\n",
      "50/88, train_loss: 0.5006\n",
      "51/88, train_loss: 0.3870\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.3663\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.3412\n",
      "57/88, train_loss: 0.4309\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.3741\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.5003\n",
      "63/88, train_loss: 0.3521\n",
      "64/88, train_loss: 0.3316\n",
      "65/88, train_loss: 0.3640\n",
      "66/88, train_loss: 0.5001\n",
      "67/88, train_loss: 0.4251\n",
      "68/88, train_loss: 0.5001\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.4367\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.4556\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.5002\n",
      "78/88, train_loss: 0.5002\n",
      "79/88, train_loss: 0.4194\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.4018\n",
      "82/88, train_loss: 0.3981\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.3620\n",
      "85/88, train_loss: 0.5002\n",
      "86/88, train_loss: 0.3813\n",
      "87/88, train_loss: 0.5002\n",
      "88/88, train_loss: 0.3153\n",
      "Epoch 30 average loss: 0.4487\n",
      "Validation loss: 0.4869\n",
      "Current epoch: 30, current mean dice: 0.1960, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 31/300\n",
      "1/88, train_loss: 0.3625\n",
      "2/88, train_loss: 0.3698\n",
      "3/88, train_loss: 0.3596\n",
      "4/88, train_loss: 0.3771\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.4078\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.4939\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.5003\n",
      "12/88, train_loss: 0.5004\n",
      "13/88, train_loss: 0.4466\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.5001\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.5001\n",
      "18/88, train_loss: 0.5003\n",
      "19/88, train_loss: 0.5006\n",
      "20/88, train_loss: 0.4702\n",
      "21/88, train_loss: 0.3573\n",
      "22/88, train_loss: 0.4367\n",
      "23/88, train_loss: 0.3846\n",
      "24/88, train_loss: 0.4286\n",
      "25/88, train_loss: 0.5003\n",
      "26/88, train_loss: 0.3278\n",
      "27/88, train_loss: 0.5001\n",
      "28/88, train_loss: 0.3918\n",
      "29/88, train_loss: 0.4117\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.3818\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.3644\n",
      "35/88, train_loss: 0.4371\n",
      "36/88, train_loss: 0.4714\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.3158\n",
      "39/88, train_loss: 0.5004\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.3952\n",
      "42/88, train_loss: 0.4338\n",
      "43/88, train_loss: 0.4240\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5006\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.5003\n",
      "49/88, train_loss: 0.3351\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.5003\n",
      "52/88, train_loss: 0.5002\n",
      "53/88, train_loss: 0.4737\n",
      "54/88, train_loss: 0.3311\n",
      "55/88, train_loss: 0.5002\n",
      "56/88, train_loss: 0.5002\n",
      "57/88, train_loss: 0.5002\n",
      "58/88, train_loss: 0.2742\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.5004\n",
      "61/88, train_loss: 0.2188\n",
      "62/88, train_loss: 0.3573\n",
      "63/88, train_loss: 0.3845\n",
      "64/88, train_loss: 0.5004\n",
      "65/88, train_loss: 0.3915\n",
      "66/88, train_loss: 0.5003\n",
      "67/88, train_loss: 0.4324\n",
      "68/88, train_loss: 0.3882\n",
      "69/88, train_loss: 0.4824\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.4296\n",
      "72/88, train_loss: 0.5002\n",
      "73/88, train_loss: 0.5002\n",
      "74/88, train_loss: 0.5002\n",
      "75/88, train_loss: 0.4511\n",
      "76/88, train_loss: 0.5002\n",
      "77/88, train_loss: 0.3341\n",
      "78/88, train_loss: 0.4154\n",
      "79/88, train_loss: 0.5004\n",
      "80/88, train_loss: 0.1758\n",
      "81/88, train_loss: 0.4797\n",
      "82/88, train_loss: 0.4432\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.3682\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.5003\n",
      "87/88, train_loss: 0.2628\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 31 average loss: 0.4453\n",
      "\n",
      "Epoch 32/300\n",
      "1/88, train_loss: 0.5004\n",
      "2/88, train_loss: 0.5002\n",
      "3/88, train_loss: 0.4241\n",
      "4/88, train_loss: 0.3594\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.4229\n",
      "7/88, train_loss: 0.4820\n",
      "8/88, train_loss: 0.3381\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.3642\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.3936\n",
      "18/88, train_loss: 0.4859\n",
      "19/88, train_loss: 0.4474\n",
      "20/88, train_loss: 0.2600\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.5002\n",
      "23/88, train_loss: 0.5002\n",
      "24/88, train_loss: 0.4355\n",
      "25/88, train_loss: 0.5003\n",
      "26/88, train_loss: 0.5003\n",
      "27/88, train_loss: 0.3291\n",
      "28/88, train_loss: 0.3211\n",
      "29/88, train_loss: 0.5003\n",
      "30/88, train_loss: 0.5004\n",
      "31/88, train_loss: 0.3889\n",
      "32/88, train_loss: 0.5002\n",
      "33/88, train_loss: 0.5004\n",
      "34/88, train_loss: 0.4743\n",
      "35/88, train_loss: 0.4117\n",
      "36/88, train_loss: 0.4227\n",
      "37/88, train_loss: 0.2841\n",
      "38/88, train_loss: 0.3615\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.4427\n",
      "42/88, train_loss: 0.3513\n",
      "43/88, train_loss: 0.5002\n",
      "44/88, train_loss: 0.1575\n",
      "45/88, train_loss: 0.5003\n",
      "46/88, train_loss: 0.5004\n",
      "47/88, train_loss: 0.4881\n",
      "48/88, train_loss: 0.5009\n",
      "49/88, train_loss: 0.3587\n",
      "50/88, train_loss: 0.5004\n",
      "51/88, train_loss: 0.5005\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.3673\n",
      "54/88, train_loss: 0.5002\n",
      "55/88, train_loss: 0.5003\n",
      "56/88, train_loss: 0.3469\n",
      "57/88, train_loss: 0.3805\n",
      "58/88, train_loss: 0.3347\n",
      "59/88, train_loss: 0.3541\n",
      "60/88, train_loss: 0.3773\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.3308\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5003\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.5003\n",
      "67/88, train_loss: 0.4846\n",
      "68/88, train_loss: 0.5002\n",
      "69/88, train_loss: 0.5007\n",
      "70/88, train_loss: 0.4295\n",
      "71/88, train_loss: 0.4476\n",
      "72/88, train_loss: 0.5004\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.4683\n",
      "76/88, train_loss: 0.3178\n",
      "77/88, train_loss: 0.3568\n",
      "78/88, train_loss: 0.4285\n",
      "79/88, train_loss: 0.5002\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.4223\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.4423\n",
      "85/88, train_loss: 0.3013\n",
      "86/88, train_loss: 0.5003\n",
      "87/88, train_loss: 0.5001\n",
      "88/88, train_loss: 0.4024\n",
      "Epoch 32 average loss: 0.4455\n",
      "\n",
      "Epoch 33/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.5003\n",
      "4/88, train_loss: 0.3878\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.5003\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5003\n",
      "9/88, train_loss: 0.3563\n",
      "10/88, train_loss: 0.3572\n",
      "11/88, train_loss: 0.3154\n",
      "12/88, train_loss: 0.2158\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.4351\n",
      "15/88, train_loss: 0.4229\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.2233\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.3445\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.5002\n",
      "22/88, train_loss: 0.5002\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5002\n",
      "25/88, train_loss: 0.2779\n",
      "26/88, train_loss: 0.2874\n",
      "27/88, train_loss: 0.5001\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.3237\n",
      "31/88, train_loss: 0.3643\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.5001\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5002\n",
      "37/88, train_loss: 0.3773\n",
      "38/88, train_loss: 0.4914\n",
      "39/88, train_loss: 0.4142\n",
      "40/88, train_loss: 0.5003\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.3987\n",
      "43/88, train_loss: 0.3555\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.4279\n",
      "48/88, train_loss: 0.5002\n",
      "49/88, train_loss: 0.5002\n",
      "50/88, train_loss: 0.4548\n",
      "51/88, train_loss: 0.4772\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.1922\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.4276\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.3504\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.3416\n",
      "63/88, train_loss: 0.5002\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.3176\n",
      "66/88, train_loss: 0.3262\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5007\n",
      "69/88, train_loss: 0.3737\n",
      "70/88, train_loss: 0.2764\n",
      "71/88, train_loss: 0.4544\n",
      "72/88, train_loss: 0.3805\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.3931\n",
      "75/88, train_loss: 0.5003\n",
      "76/88, train_loss: 0.5003\n",
      "77/88, train_loss: 0.3479\n",
      "78/88, train_loss: 0.3216\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.4490\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.3892\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.4092\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.4296\n",
      "87/88, train_loss: 0.5002\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 33 average loss: 0.4420\n",
      "\n",
      "Epoch 34/300\n",
      "1/88, train_loss: 0.3630\n",
      "2/88, train_loss: 0.5005\n",
      "3/88, train_loss: 0.3268\n",
      "4/88, train_loss: 0.3204\n",
      "5/88, train_loss: 0.3179\n",
      "6/88, train_loss: 0.3247\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.4354\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.3723\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.3881\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.2468\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.3777\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.4253\n",
      "25/88, train_loss: 0.5003\n",
      "26/88, train_loss: 0.5002\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.3543\n",
      "29/88, train_loss: 0.5010\n",
      "30/88, train_loss: 0.5002\n",
      "31/88, train_loss: 0.4084\n",
      "32/88, train_loss: 0.3891\n",
      "33/88, train_loss: 0.3119\n",
      "34/88, train_loss: 0.3759\n",
      "35/88, train_loss: 0.3544\n",
      "36/88, train_loss: 0.2149\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.4413\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.4415\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.4574\n",
      "47/88, train_loss: 0.3881\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.4469\n",
      "50/88, train_loss: 0.5003\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.4476\n",
      "54/88, train_loss: 0.5002\n",
      "55/88, train_loss: 0.3167\n",
      "56/88, train_loss: 0.3689\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.3433\n",
      "59/88, train_loss: 0.3521\n",
      "60/88, train_loss: 0.5003\n",
      "61/88, train_loss: 0.5004\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.5002\n",
      "64/88, train_loss: 0.4675\n",
      "65/88, train_loss: 0.5009\n",
      "66/88, train_loss: 0.3788\n",
      "67/88, train_loss: 0.4177\n",
      "68/88, train_loss: 0.3100\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.4595\n",
      "71/88, train_loss: 0.4455\n",
      "72/88, train_loss: 0.4391\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.3430\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.4094\n",
      "79/88, train_loss: 0.3689\n",
      "80/88, train_loss: 0.4631\n",
      "81/88, train_loss: 0.5001\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5002\n",
      "84/88, train_loss: 0.5002\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.2459\n",
      "87/88, train_loss: 0.3701\n",
      "88/88, train_loss: 0.3032\n",
      "Epoch 34 average loss: 0.4402\n",
      "\n",
      "Epoch 35/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.4041\n",
      "3/88, train_loss: 0.5003\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.5001\n",
      "7/88, train_loss: 0.5005\n",
      "8/88, train_loss: 0.3077\n",
      "9/88, train_loss: 0.3426\n",
      "10/88, train_loss: 0.1894\n",
      "11/88, train_loss: 0.3452\n",
      "12/88, train_loss: 0.5003\n",
      "13/88, train_loss: 0.3407\n",
      "14/88, train_loss: 0.4321\n",
      "15/88, train_loss: 0.5001\n",
      "16/88, train_loss: 0.3249\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.3374\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.3531\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.3124\n",
      "27/88, train_loss: 0.5001\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.3873\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.4102\n",
      "34/88, train_loss: 0.3031\n",
      "35/88, train_loss: 0.4603\n",
      "36/88, train_loss: 0.4371\n",
      "37/88, train_loss: 0.3339\n",
      "38/88, train_loss: 0.2120\n",
      "39/88, train_loss: 0.5001\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.5006\n",
      "42/88, train_loss: 0.5002\n",
      "43/88, train_loss: 0.5004\n",
      "44/88, train_loss: 0.5002\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.3746\n",
      "47/88, train_loss: 0.5002\n",
      "48/88, train_loss: 0.5003\n",
      "49/88, train_loss: 0.3506\n",
      "50/88, train_loss: 0.5005\n",
      "51/88, train_loss: 0.4581\n",
      "52/88, train_loss: 0.5004\n",
      "53/88, train_loss: 0.3659\n",
      "54/88, train_loss: 0.4850\n",
      "55/88, train_loss: 0.3412\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.3274\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.3933\n",
      "60/88, train_loss: 0.4114\n",
      "61/88, train_loss: 0.3957\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.5002\n",
      "64/88, train_loss: 0.4718\n",
      "65/88, train_loss: 0.2041\n",
      "66/88, train_loss: 0.4312\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.3476\n",
      "69/88, train_loss: 0.3891\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.3432\n",
      "72/88, train_loss: 0.5002\n",
      "73/88, train_loss: 0.5003\n",
      "74/88, train_loss: 0.4850\n",
      "75/88, train_loss: 0.4088\n",
      "76/88, train_loss: 0.5003\n",
      "77/88, train_loss: 0.5002\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.3423\n",
      "80/88, train_loss: 0.5003\n",
      "81/88, train_loss: 0.4360\n",
      "82/88, train_loss: 0.4042\n",
      "83/88, train_loss: 0.5002\n",
      "84/88, train_loss: 0.4070\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.3502\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.4399\n",
      "Epoch 35 average loss: 0.4387\n",
      "Validation loss: 0.4892\n",
      "Current epoch: 35, current mean dice: 0.1595, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 36/300\n",
      "1/88, train_loss: 0.3107\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5002\n",
      "4/88, train_loss: 0.4114\n",
      "5/88, train_loss: 0.3832\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.3281\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5005\n",
      "10/88, train_loss: 0.5004\n",
      "11/88, train_loss: 0.5002\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.3513\n",
      "16/88, train_loss: 0.5006\n",
      "17/88, train_loss: 0.3485\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5004\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.4292\n",
      "23/88, train_loss: 0.3490\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.3790\n",
      "26/88, train_loss: 0.3095\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.5002\n",
      "29/88, train_loss: 0.1811\n",
      "30/88, train_loss: 0.2219\n",
      "31/88, train_loss: 0.3507\n",
      "32/88, train_loss: 0.4071\n",
      "33/88, train_loss: 0.3289\n",
      "34/88, train_loss: 0.3146\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.5002\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.3029\n",
      "39/88, train_loss: 0.5002\n",
      "40/88, train_loss: 0.4072\n",
      "41/88, train_loss: 0.4438\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5001\n",
      "44/88, train_loss: 0.3278\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.2442\n",
      "47/88, train_loss: 0.4650\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.5001\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.3157\n",
      "53/88, train_loss: 0.5001\n",
      "54/88, train_loss: 0.5002\n",
      "55/88, train_loss: 0.5002\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.3198\n",
      "61/88, train_loss: 0.4476\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.3584\n",
      "64/88, train_loss: 0.4448\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.4110\n",
      "68/88, train_loss: 0.3651\n",
      "69/88, train_loss: 0.5002\n",
      "70/88, train_loss: 0.4057\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.3076\n",
      "75/88, train_loss: 0.3952\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.3421\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.3719\n",
      "81/88, train_loss: 0.5002\n",
      "82/88, train_loss: 0.2815\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.4124\n",
      "85/88, train_loss: 0.1926\n",
      "86/88, train_loss: 0.3832\n",
      "87/88, train_loss: 0.3440\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 36 average loss: 0.4341\n",
      "\n",
      "Epoch 37/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.3623\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.3364\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.1730\n",
      "7/88, train_loss: 0.3466\n",
      "8/88, train_loss: 0.5003\n",
      "9/88, train_loss: 0.2629\n",
      "10/88, train_loss: 0.5003\n",
      "11/88, train_loss: 0.5002\n",
      "12/88, train_loss: 0.2497\n",
      "13/88, train_loss: 0.3285\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.3671\n",
      "18/88, train_loss: 0.3560\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.4416\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.4536\n",
      "25/88, train_loss: 0.5002\n",
      "26/88, train_loss: 0.4768\n",
      "27/88, train_loss: 0.3565\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.4019\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.3035\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.3044\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.3199\n",
      "40/88, train_loss: 0.3927\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.3189\n",
      "43/88, train_loss: 0.4344\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.3925\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.4085\n",
      "49/88, train_loss: 0.4088\n",
      "50/88, train_loss: 0.5002\n",
      "51/88, train_loss: 0.5001\n",
      "52/88, train_loss: 0.5002\n",
      "53/88, train_loss: 0.3506\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.3482\n",
      "56/88, train_loss: 0.3207\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.4653\n",
      "59/88, train_loss: 0.4171\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.5004\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.3870\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.3323\n",
      "66/88, train_loss: 0.3794\n",
      "67/88, train_loss: 0.3887\n",
      "68/88, train_loss: 0.5002\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.3053\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.4095\n",
      "75/88, train_loss: 0.3487\n",
      "76/88, train_loss: 0.3310\n",
      "77/88, train_loss: 0.4790\n",
      "78/88, train_loss: 0.3432\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.3265\n",
      "81/88, train_loss: 0.3249\n",
      "82/88, train_loss: 0.1706\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.4237\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.3707\n",
      "Epoch 37 average loss: 0.4309\n",
      "\n",
      "Epoch 38/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.5002\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.3918\n",
      "6/88, train_loss: 0.4413\n",
      "7/88, train_loss: 0.4429\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.3724\n",
      "10/88, train_loss: 0.4512\n",
      "11/88, train_loss: 0.4198\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5003\n",
      "14/88, train_loss: 0.4172\n",
      "15/88, train_loss: 0.5001\n",
      "16/88, train_loss: 0.3411\n",
      "17/88, train_loss: 0.3037\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.1549\n",
      "20/88, train_loss: 0.2084\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.3715\n",
      "25/88, train_loss: 0.2350\n",
      "26/88, train_loss: 0.4119\n",
      "27/88, train_loss: 0.3771\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.3231\n",
      "30/88, train_loss: 0.4368\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.2028\n",
      "33/88, train_loss: 0.5001\n",
      "34/88, train_loss: 0.5002\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.5002\n",
      "37/88, train_loss: 0.3440\n",
      "38/88, train_loss: 0.5001\n",
      "39/88, train_loss: 0.3961\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.3019\n",
      "42/88, train_loss: 0.5003\n",
      "43/88, train_loss: 0.3239\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5004\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.3582\n",
      "48/88, train_loss: 0.5006\n",
      "49/88, train_loss: 0.3062\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.4610\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.5002\n",
      "54/88, train_loss: 0.3415\n",
      "55/88, train_loss: 0.3555\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5002\n",
      "60/88, train_loss: 0.5002\n",
      "61/88, train_loss: 0.5002\n",
      "62/88, train_loss: 0.5002\n",
      "63/88, train_loss: 0.3338\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.3310\n",
      "66/88, train_loss: 0.5001\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.3365\n",
      "69/88, train_loss: 0.3152\n",
      "70/88, train_loss: 0.3426\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.3710\n",
      "74/88, train_loss: 0.3001\n",
      "75/88, train_loss: 0.3151\n",
      "76/88, train_loss: 0.3739\n",
      "77/88, train_loss: 0.3799\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.4270\n",
      "80/88, train_loss: 0.1848\n",
      "81/88, train_loss: 0.5004\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.4240\n",
      "85/88, train_loss: 0.5003\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.4128\n",
      "88/88, train_loss: 0.3135\n",
      "Epoch 38 average loss: 0.4279\n",
      "\n",
      "Epoch 39/300\n",
      "1/88, train_loss: 0.5002\n",
      "2/88, train_loss: 0.3811\n",
      "3/88, train_loss: 0.5002\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.2602\n",
      "6/88, train_loss: 0.5003\n",
      "7/88, train_loss: 0.2229\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.5001\n",
      "10/88, train_loss: 0.5002\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.1496\n",
      "14/88, train_loss: 0.1618\n",
      "15/88, train_loss: 0.5001\n",
      "16/88, train_loss: 0.3240\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.3245\n",
      "20/88, train_loss: 0.4008\n",
      "21/88, train_loss: 0.5002\n",
      "22/88, train_loss: 0.4127\n",
      "23/88, train_loss: 0.2956\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.3896\n",
      "29/88, train_loss: 0.4568\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.3273\n",
      "33/88, train_loss: 0.5003\n",
      "34/88, train_loss: 0.3031\n",
      "35/88, train_loss: 0.4001\n",
      "36/88, train_loss: 0.4281\n",
      "37/88, train_loss: 0.1988\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.3126\n",
      "41/88, train_loss: 0.3021\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.3871\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.5002\n",
      "49/88, train_loss: 0.5002\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.4673\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.3004\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5005\n",
      "58/88, train_loss: 0.2669\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.3621\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.4535\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.3290\n",
      "67/88, train_loss: 0.3594\n",
      "68/88, train_loss: 0.3301\n",
      "69/88, train_loss: 0.3247\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.1801\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.3444\n",
      "74/88, train_loss: 0.3094\n",
      "75/88, train_loss: 0.5002\n",
      "76/88, train_loss: 0.3993\n",
      "77/88, train_loss: 0.5004\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.3594\n",
      "81/88, train_loss: 0.3562\n",
      "82/88, train_loss: 0.3083\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.3338\n",
      "88/88, train_loss: 0.2330\n",
      "Epoch 39 average loss: 0.4257\n",
      "\n",
      "Epoch 40/300\n",
      "1/88, train_loss: 0.3484\n",
      "2/88, train_loss: 0.3304\n",
      "3/88, train_loss: 0.3180\n",
      "4/88, train_loss: 0.4444\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.3633\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5005\n",
      "16/88, train_loss: 0.3383\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.3185\n",
      "22/88, train_loss: 0.4052\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.3385\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.3858\n",
      "29/88, train_loss: 0.3275\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.3056\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.5003\n",
      "36/88, train_loss: 0.3625\n",
      "37/88, train_loss: 0.5002\n",
      "38/88, train_loss: 0.5001\n",
      "39/88, train_loss: 0.4911\n",
      "40/88, train_loss: 0.3562\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.4354\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5002\n",
      "45/88, train_loss: 0.5004\n",
      "46/88, train_loss: 0.3760\n",
      "47/88, train_loss: 0.5002\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.3710\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5002\n",
      "52/88, train_loss: 0.1939\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.3622\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.2991\n",
      "58/88, train_loss: 0.4405\n",
      "59/88, train_loss: 0.3095\n",
      "60/88, train_loss: 0.2975\n",
      "61/88, train_loss: 0.3925\n",
      "62/88, train_loss: 0.4500\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5004\n",
      "65/88, train_loss: 0.1640\n",
      "66/88, train_loss: 0.4356\n",
      "67/88, train_loss: 0.2076\n",
      "68/88, train_loss: 0.2655\n",
      "69/88, train_loss: 0.3341\n",
      "70/88, train_loss: 0.3208\n",
      "71/88, train_loss: 0.3937\n",
      "72/88, train_loss: 0.5003\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.2964\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.3958\n",
      "77/88, train_loss: 0.5002\n",
      "78/88, train_loss: 0.3856\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.2563\n",
      "81/88, train_loss: 0.3776\n",
      "82/88, train_loss: 0.5002\n",
      "83/88, train_loss: 0.3267\n",
      "84/88, train_loss: 0.5005\n",
      "85/88, train_loss: 0.5002\n",
      "86/88, train_loss: 0.3736\n",
      "87/88, train_loss: 0.4297\n",
      "88/88, train_loss: 0.5002\n",
      "Epoch 40 average loss: 0.4299\n",
      "Validation loss: 0.4885\n",
      "Current epoch: 40, current mean dice: 0.1996, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 41/300\n",
      "1/88, train_loss: 0.5003\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.5006\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.3711\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.2000\n",
      "9/88, train_loss: 0.2785\n",
      "10/88, train_loss: 0.1934\n",
      "11/88, train_loss: 0.3858\n",
      "12/88, train_loss: 0.4391\n",
      "13/88, train_loss: 0.5002\n",
      "14/88, train_loss: 0.3450\n",
      "15/88, train_loss: 0.3960\n",
      "16/88, train_loss: 0.3005\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.3493\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.4099\n",
      "21/88, train_loss: 0.3792\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.2935\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.4203\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.3146\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.3826\n",
      "30/88, train_loss: 0.3013\n",
      "31/88, train_loss: 0.3727\n",
      "32/88, train_loss: 0.2998\n",
      "33/88, train_loss: 0.5003\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.3675\n",
      "36/88, train_loss: 0.3585\n",
      "37/88, train_loss: 0.3072\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.5002\n",
      "42/88, train_loss: 0.5001\n",
      "43/88, train_loss: 0.5001\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.5002\n",
      "48/88, train_loss: 0.4573\n",
      "49/88, train_loss: 0.5001\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.3451\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.3584\n",
      "56/88, train_loss: 0.5002\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.3245\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.3478\n",
      "62/88, train_loss: 0.3322\n",
      "63/88, train_loss: 0.2137\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.4591\n",
      "66/88, train_loss: 0.1124\n",
      "67/88, train_loss: 0.3214\n",
      "68/88, train_loss: 0.3329\n",
      "69/88, train_loss: 0.3928\n",
      "70/88, train_loss: 0.3172\n",
      "71/88, train_loss: 0.5007\n",
      "72/88, train_loss: 0.4051\n",
      "73/88, train_loss: 0.3350\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.3007\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5002\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.4302\n",
      "82/88, train_loss: 0.3111\n",
      "83/88, train_loss: 0.4033\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5001\n",
      "88/88, train_loss: 0.5002\n",
      "Epoch 41 average loss: 0.4258\n",
      "\n",
      "Epoch 42/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.3086\n",
      "3/88, train_loss: 0.3192\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.4054\n",
      "7/88, train_loss: 0.3058\n",
      "8/88, train_loss: 0.3918\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.3523\n",
      "11/88, train_loss: 0.3279\n",
      "12/88, train_loss: 0.3377\n",
      "13/88, train_loss: 0.3427\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.3593\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.5001\n",
      "18/88, train_loss: 0.2640\n",
      "19/88, train_loss: 0.4137\n",
      "20/88, train_loss: 0.3154\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.5004\n",
      "23/88, train_loss: 0.3405\n",
      "24/88, train_loss: 0.3403\n",
      "25/88, train_loss: 0.5002\n",
      "26/88, train_loss: 0.5002\n",
      "27/88, train_loss: 0.2332\n",
      "28/88, train_loss: 0.2924\n",
      "29/88, train_loss: 0.3374\n",
      "30/88, train_loss: 0.5011\n",
      "31/88, train_loss: 0.2024\n",
      "32/88, train_loss: 0.3155\n",
      "33/88, train_loss: 0.5001\n",
      "34/88, train_loss: 0.4055\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.3149\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.3180\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.4562\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.4434\n",
      "49/88, train_loss: 0.4371\n",
      "50/88, train_loss: 0.3262\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.2946\n",
      "53/88, train_loss: 0.3362\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.5002\n",
      "56/88, train_loss: 0.3128\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.3764\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.3176\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5002\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.5004\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.3579\n",
      "69/88, train_loss: 0.3131\n",
      "70/88, train_loss: 0.5002\n",
      "71/88, train_loss: 0.5003\n",
      "72/88, train_loss: 0.3822\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.3848\n",
      "76/88, train_loss: 0.3369\n",
      "77/88, train_loss: 0.3193\n",
      "78/88, train_loss: 0.3266\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.3642\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5004\n",
      "84/88, train_loss: 0.4128\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.3571\n",
      "87/88, train_loss: 0.5001\n",
      "88/88, train_loss: 0.3036\n",
      "Epoch 42 average loss: 0.4228\n",
      "\n",
      "Epoch 43/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.3309\n",
      "4/88, train_loss: 0.5001\n",
      "5/88, train_loss: 0.3975\n",
      "6/88, train_loss: 0.3186\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5001\n",
      "10/88, train_loss: 0.5003\n",
      "11/88, train_loss: 0.4023\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5006\n",
      "14/88, train_loss: 0.3423\n",
      "15/88, train_loss: 0.4101\n",
      "16/88, train_loss: 0.3638\n",
      "17/88, train_loss: 0.5003\n",
      "18/88, train_loss: 0.3287\n",
      "19/88, train_loss: 0.5008\n",
      "20/88, train_loss: 0.1863\n",
      "21/88, train_loss: 0.5003\n",
      "22/88, train_loss: 0.3953\n",
      "23/88, train_loss: 0.1414\n",
      "24/88, train_loss: 0.5002\n",
      "25/88, train_loss: 0.2990\n",
      "26/88, train_loss: 0.5002\n",
      "27/88, train_loss: 0.3267\n",
      "28/88, train_loss: 0.2940\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.3225\n",
      "31/88, train_loss: 0.3005\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.3879\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.4642\n",
      "42/88, train_loss: 0.5001\n",
      "43/88, train_loss: 0.3491\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.3585\n",
      "48/88, train_loss: 0.2727\n",
      "49/88, train_loss: 0.3708\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5001\n",
      "54/88, train_loss: 0.2979\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.4341\n",
      "61/88, train_loss: 0.3134\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.3159\n",
      "64/88, train_loss: 0.3267\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.1781\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.3632\n",
      "72/88, train_loss: 0.1504\n",
      "73/88, train_loss: 0.4164\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.3211\n",
      "76/88, train_loss: 0.1268\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.3465\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.2959\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.3506\n",
      "84/88, train_loss: 0.1802\n",
      "85/88, train_loss: 0.4092\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.3319\n",
      "Epoch 43 average loss: 0.4207\n",
      "\n",
      "Epoch 44/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.1162\n",
      "3/88, train_loss: 0.3228\n",
      "4/88, train_loss: 0.3314\n",
      "5/88, train_loss: 0.2293\n",
      "6/88, train_loss: 0.4592\n",
      "7/88, train_loss: 0.3722\n",
      "8/88, train_loss: 0.2988\n",
      "9/88, train_loss: 0.3121\n",
      "10/88, train_loss: 0.3973\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.3513\n",
      "15/88, train_loss: 0.3795\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.2974\n",
      "20/88, train_loss: 0.3362\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.3947\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.5002\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.3524\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.5002\n",
      "34/88, train_loss: 0.5003\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.3203\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.5001\n",
      "39/88, train_loss: 0.3111\n",
      "40/88, train_loss: 0.5002\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.4435\n",
      "43/88, train_loss: 0.3128\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.2024\n",
      "46/88, train_loss: 0.3618\n",
      "47/88, train_loss: 0.3440\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.1964\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.2969\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.3088\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.3916\n",
      "57/88, train_loss: 0.3891\n",
      "58/88, train_loss: 0.4065\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.3882\n",
      "61/88, train_loss: 0.3434\n",
      "62/88, train_loss: 0.3572\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.1166\n",
      "67/88, train_loss: 0.5002\n",
      "68/88, train_loss: 0.4331\n",
      "69/88, train_loss: 0.5002\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.3528\n",
      "75/88, train_loss: 0.5004\n",
      "76/88, train_loss: 0.3293\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.3070\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.3399\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.3272\n",
      "84/88, train_loss: 0.5006\n",
      "85/88, train_loss: 0.4184\n",
      "86/88, train_loss: 0.5006\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.4174\n",
      "Epoch 44 average loss: 0.4247\n",
      "\n",
      "Epoch 45/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5006\n",
      "4/88, train_loss: 0.3220\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.3372\n",
      "7/88, train_loss: 0.3863\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.3605\n",
      "10/88, train_loss: 0.2886\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.3073\n",
      "13/88, train_loss: 0.2324\n",
      "14/88, train_loss: 0.5003\n",
      "15/88, train_loss: 0.3848\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.2971\n",
      "18/88, train_loss: 0.3239\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.4068\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.4056\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.1142\n",
      "27/88, train_loss: 0.3958\n",
      "28/88, train_loss: 0.4738\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.3265\n",
      "32/88, train_loss: 0.2247\n",
      "33/88, train_loss: 0.5006\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.5005\n",
      "36/88, train_loss: 0.5003\n",
      "37/88, train_loss: 0.5005\n",
      "38/88, train_loss: 0.3578\n",
      "39/88, train_loss: 0.5003\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.4013\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.3483\n",
      "46/88, train_loss: 0.3000\n",
      "47/88, train_loss: 0.4263\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.3769\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.4199\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.1978\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.5003\n",
      "61/88, train_loss: 0.5002\n",
      "62/88, train_loss: 0.3097\n",
      "63/88, train_loss: 0.3095\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.3661\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.3401\n",
      "69/88, train_loss: 0.2114\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.1466\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5002\n",
      "76/88, train_loss: 0.3512\n",
      "77/88, train_loss: 0.3506\n",
      "78/88, train_loss: 0.3449\n",
      "79/88, train_loss: 0.3234\n",
      "80/88, train_loss: 0.3114\n",
      "81/88, train_loss: 0.3570\n",
      "82/88, train_loss: 0.5001\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.2282\n",
      "85/88, train_loss: 0.3205\n",
      "86/88, train_loss: 0.5002\n",
      "87/88, train_loss: 0.3757\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 45 average loss: 0.4212\n",
      "Validation loss: 0.4874\n",
      "Current epoch: 45, current mean dice: 0.1962, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 46/300\n",
      "1/88, train_loss: 0.3670\n",
      "2/88, train_loss: 0.3100\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.3398\n",
      "6/88, train_loss: 0.5003\n",
      "7/88, train_loss: 0.3531\n",
      "8/88, train_loss: 0.4201\n",
      "9/88, train_loss: 0.2937\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.1945\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.4106\n",
      "16/88, train_loss: 0.5004\n",
      "17/88, train_loss: 0.3067\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.3258\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.3214\n",
      "24/88, train_loss: 0.3215\n",
      "25/88, train_loss: 0.3010\n",
      "26/88, train_loss: 0.3845\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.3320\n",
      "30/88, train_loss: 0.3554\n",
      "31/88, train_loss: 0.2997\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.3215\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.5001\n",
      "37/88, train_loss: 0.3025\n",
      "38/88, train_loss: 0.4443\n",
      "39/88, train_loss: 0.5001\n",
      "40/88, train_loss: 0.1536\n",
      "41/88, train_loss: 0.5002\n",
      "42/88, train_loss: 0.2906\n",
      "43/88, train_loss: 0.1745\n",
      "44/88, train_loss: 0.3408\n",
      "45/88, train_loss: 0.5004\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.5002\n",
      "48/88, train_loss: 0.5006\n",
      "49/88, train_loss: 0.3238\n",
      "50/88, train_loss: 0.5003\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.3177\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.3492\n",
      "56/88, train_loss: 0.4270\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.3445\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.3337\n",
      "64/88, train_loss: 0.3710\n",
      "65/88, train_loss: 0.3880\n",
      "66/88, train_loss: 0.3054\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.2863\n",
      "69/88, train_loss: 0.1709\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.3223\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.4158\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.3462\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.1830\n",
      "83/88, train_loss: 0.3893\n",
      "84/88, train_loss: 0.4180\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5002\n",
      "88/88, train_loss: 0.3090\n",
      "Epoch 46 average loss: 0.4167\n",
      "\n",
      "Epoch 47/300\n",
      "1/88, train_loss: 0.3119\n",
      "2/88, train_loss: 0.3391\n",
      "3/88, train_loss: 0.3860\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.1788\n",
      "7/88, train_loss: 0.3407\n",
      "8/88, train_loss: 0.3032\n",
      "9/88, train_loss: 0.5004\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.3279\n",
      "12/88, train_loss: 0.5003\n",
      "13/88, train_loss: 0.5004\n",
      "14/88, train_loss: 0.5007\n",
      "15/88, train_loss: 0.1430\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.4346\n",
      "18/88, train_loss: 0.3344\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.5002\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.5004\n",
      "27/88, train_loss: 0.3839\n",
      "28/88, train_loss: 0.4213\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.3097\n",
      "33/88, train_loss: 0.5002\n",
      "34/88, train_loss: 0.3918\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.3158\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.1556\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.3030\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.3108\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.3748\n",
      "48/88, train_loss: 0.5002\n",
      "49/88, train_loss: 0.2942\n",
      "50/88, train_loss: 0.3250\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5005\n",
      "53/88, train_loss: 0.3370\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.3329\n",
      "56/88, train_loss: 0.3200\n",
      "57/88, train_loss: 0.1889\n",
      "58/88, train_loss: 0.3458\n",
      "59/88, train_loss: 0.5002\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.3108\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.5002\n",
      "64/88, train_loss: 0.2904\n",
      "65/88, train_loss: 0.2424\n",
      "66/88, train_loss: 0.4193\n",
      "67/88, train_loss: 0.3199\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.3310\n",
      "73/88, train_loss: 0.1325\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.1948\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.3512\n",
      "83/88, train_loss: 0.2851\n",
      "84/88, train_loss: 0.1449\n",
      "85/88, train_loss: 0.3097\n",
      "86/88, train_loss: 0.5001\n",
      "87/88, train_loss: 0.4463\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 47 average loss: 0.4147\n",
      "\n",
      "Epoch 48/300\n",
      "1/88, train_loss: 0.3032\n",
      "2/88, train_loss: 0.2943\n",
      "3/88, train_loss: 0.5002\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.3759\n",
      "6/88, train_loss: 0.5001\n",
      "7/88, train_loss: 0.3122\n",
      "8/88, train_loss: 0.1953\n",
      "9/88, train_loss: 0.5005\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.2844\n",
      "13/88, train_loss: 0.5003\n",
      "14/88, train_loss: 0.3341\n",
      "15/88, train_loss: 0.3219\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.5003\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.3439\n",
      "20/88, train_loss: 0.3185\n",
      "21/88, train_loss: 0.5002\n",
      "22/88, train_loss: 0.5002\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.3046\n",
      "26/88, train_loss: 0.3440\n",
      "27/88, train_loss: 0.2954\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5004\n",
      "31/88, train_loss: 0.5010\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.3629\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.3872\n",
      "36/88, train_loss: 0.2947\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5003\n",
      "40/88, train_loss: 0.3144\n",
      "41/88, train_loss: 0.3024\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5002\n",
      "44/88, train_loss: 0.2980\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.3206\n",
      "49/88, train_loss: 0.3580\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.3799\n",
      "53/88, train_loss: 0.2935\n",
      "54/88, train_loss: 0.2887\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.3764\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.3304\n",
      "60/88, train_loss: 0.3739\n",
      "61/88, train_loss: 0.3401\n",
      "62/88, train_loss: 0.3831\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.3440\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.1584\n",
      "69/88, train_loss: 0.5004\n",
      "70/88, train_loss: 0.3110\n",
      "71/88, train_loss: 0.1819\n",
      "72/88, train_loss: 0.3688\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.2964\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.1850\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5001\n",
      "83/88, train_loss: 0.5002\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.3186\n",
      "87/88, train_loss: 0.3278\n",
      "88/88, train_loss: 0.1063\n",
      "Epoch 48 average loss: 0.4152\n",
      "\n",
      "Epoch 49/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.3155\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.1878\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.3135\n",
      "9/88, train_loss: 0.5003\n",
      "10/88, train_loss: 0.3225\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.3014\n",
      "14/88, train_loss: 0.3027\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.1383\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5002\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.2204\n",
      "22/88, train_loss: 0.3584\n",
      "23/88, train_loss: 0.5002\n",
      "24/88, train_loss: 0.5005\n",
      "25/88, train_loss: 0.1964\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.4401\n",
      "28/88, train_loss: 0.3266\n",
      "29/88, train_loss: 0.5003\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.3304\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.3522\n",
      "36/88, train_loss: 0.4417\n",
      "37/88, train_loss: 0.2474\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5006\n",
      "42/88, train_loss: 0.3671\n",
      "43/88, train_loss: 0.5002\n",
      "44/88, train_loss: 0.3555\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.3192\n",
      "48/88, train_loss: 0.4082\n",
      "49/88, train_loss: 0.5001\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.2923\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.2967\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.3228\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.4149\n",
      "58/88, train_loss: 0.3146\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2111\n",
      "62/88, train_loss: 0.3185\n",
      "63/88, train_loss: 0.1298\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.5001\n",
      "67/88, train_loss: 0.3988\n",
      "68/88, train_loss: 0.4098\n",
      "69/88, train_loss: 0.5002\n",
      "70/88, train_loss: 0.3264\n",
      "71/88, train_loss: 0.5003\n",
      "72/88, train_loss: 0.3750\n",
      "73/88, train_loss: 0.3137\n",
      "74/88, train_loss: 0.3974\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.3048\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.3118\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.4269\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.3226\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.3031\n",
      "88/88, train_loss: 0.3383\n",
      "Epoch 49 average loss: 0.4179\n",
      "\n",
      "Epoch 50/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.3680\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.1306\n",
      "9/88, train_loss: 0.2996\n",
      "10/88, train_loss: 0.3108\n",
      "11/88, train_loss: 0.3417\n",
      "12/88, train_loss: 0.3613\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.3301\n",
      "15/88, train_loss: 0.3210\n",
      "16/88, train_loss: 0.3916\n",
      "17/88, train_loss: 0.5002\n",
      "18/88, train_loss: 0.4103\n",
      "19/88, train_loss: 0.2917\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.3014\n",
      "22/88, train_loss: 0.3681\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5002\n",
      "26/88, train_loss: 0.5003\n",
      "27/88, train_loss: 0.5003\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.3262\n",
      "32/88, train_loss: 0.3202\n",
      "33/88, train_loss: 0.3426\n",
      "34/88, train_loss: 0.3056\n",
      "35/88, train_loss: 0.3415\n",
      "36/88, train_loss: 0.3418\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.5005\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.3087\n",
      "43/88, train_loss: 0.2823\n",
      "44/88, train_loss: 0.2962\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.0915\n",
      "48/88, train_loss: 0.5001\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.3701\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.3345\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5002\n",
      "58/88, train_loss: 0.1567\n",
      "59/88, train_loss: 0.3773\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.3063\n",
      "62/88, train_loss: 0.5002\n",
      "63/88, train_loss: 0.1342\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.2994\n",
      "67/88, train_loss: 0.3104\n",
      "68/88, train_loss: 0.5001\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.3314\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.3293\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.2969\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.3007\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.4393\n",
      "82/88, train_loss: 0.1814\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.3120\n",
      "85/88, train_loss: 0.3490\n",
      "86/88, train_loss: 0.3431\n",
      "87/88, train_loss: 0.1123\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 50 average loss: 0.4088\n",
      "Validation loss: 0.4918\n",
      "Current epoch: 50, current mean dice: 0.1282, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 51/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.2994\n",
      "3/88, train_loss: 0.1233\n",
      "4/88, train_loss: 0.5001\n",
      "5/88, train_loss: 0.3172\n",
      "6/88, train_loss: 0.3453\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.3333\n",
      "9/88, train_loss: 0.3423\n",
      "10/88, train_loss: 0.2910\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.1491\n",
      "13/88, train_loss: 0.5002\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.3680\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.3633\n",
      "18/88, train_loss: 0.3344\n",
      "19/88, train_loss: 0.3034\n",
      "20/88, train_loss: 0.3053\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5002\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.3763\n",
      "27/88, train_loss: 0.1490\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.3328\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.3557\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5003\n",
      "37/88, train_loss: 0.5002\n",
      "38/88, train_loss: 0.3718\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.3982\n",
      "43/88, train_loss: 0.2897\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5002\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.2859\n",
      "48/88, train_loss: 0.3011\n",
      "49/88, train_loss: 0.1183\n",
      "50/88, train_loss: 0.5003\n",
      "51/88, train_loss: 0.3404\n",
      "52/88, train_loss: 0.2592\n",
      "53/88, train_loss: 0.3092\n",
      "54/88, train_loss: 0.2921\n",
      "55/88, train_loss: 0.2982\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.1427\n",
      "60/88, train_loss: 0.3381\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.3398\n",
      "63/88, train_loss: 0.5004\n",
      "64/88, train_loss: 0.2991\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.5006\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.1321\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.2942\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.3916\n",
      "74/88, train_loss: 0.5002\n",
      "75/88, train_loss: 0.2928\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.3078\n",
      "78/88, train_loss: 0.3126\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.3942\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.3434\n",
      "84/88, train_loss: 0.5003\n",
      "85/88, train_loss: 0.5002\n",
      "86/88, train_loss: 0.5001\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 51 average loss: 0.4085\n",
      "\n",
      "Epoch 52/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.2801\n",
      "4/88, train_loss: 0.1102\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.3041\n",
      "7/88, train_loss: 0.0831\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.3121\n",
      "12/88, train_loss: 0.5001\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.5001\n",
      "16/88, train_loss: 0.2977\n",
      "17/88, train_loss: 0.4191\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.4336\n",
      "20/88, train_loss: 0.3533\n",
      "21/88, train_loss: 0.5002\n",
      "22/88, train_loss: 0.3047\n",
      "23/88, train_loss: 0.2984\n",
      "24/88, train_loss: 0.5002\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.3122\n",
      "27/88, train_loss: 0.5002\n",
      "28/88, train_loss: 0.3792\n",
      "29/88, train_loss: 0.3355\n",
      "30/88, train_loss: 0.4330\n",
      "31/88, train_loss: 0.3070\n",
      "32/88, train_loss: 0.5002\n",
      "33/88, train_loss: 0.3194\n",
      "34/88, train_loss: 0.3586\n",
      "35/88, train_loss: 0.3385\n",
      "36/88, train_loss: 0.5001\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.3038\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.3237\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.2949\n",
      "43/88, train_loss: 0.3458\n",
      "44/88, train_loss: 0.2990\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.5003\n",
      "48/88, train_loss: 0.5001\n",
      "49/88, train_loss: 0.2913\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5001\n",
      "52/88, train_loss: 0.3642\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.3106\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.3667\n",
      "57/88, train_loss: 0.3531\n",
      "58/88, train_loss: 0.2974\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2843\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.1700\n",
      "64/88, train_loss: 0.2945\n",
      "65/88, train_loss: 0.5005\n",
      "66/88, train_loss: 0.3025\n",
      "67/88, train_loss: 0.5004\n",
      "68/88, train_loss: 0.2928\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.3078\n",
      "71/88, train_loss: 0.3419\n",
      "72/88, train_loss: 0.5005\n",
      "73/88, train_loss: 0.1944\n",
      "74/88, train_loss: 0.3784\n",
      "75/88, train_loss: 0.3310\n",
      "76/88, train_loss: 0.3342\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.2781\n",
      "84/88, train_loss: 0.5000\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.3477\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 52 average loss: 0.4079\n",
      "\n",
      "Epoch 53/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.4095\n",
      "5/88, train_loss: 0.3848\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.3399\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.3308\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.3339\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5003\n",
      "15/88, train_loss: 0.5002\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.4229\n",
      "18/88, train_loss: 0.2014\n",
      "19/88, train_loss: 0.5002\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.3626\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.3103\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.2866\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.3028\n",
      "29/88, train_loss: 0.3251\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.2801\n",
      "33/88, train_loss: 0.3698\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.3131\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.3656\n",
      "41/88, train_loss: 0.3413\n",
      "42/88, train_loss: 0.2892\n",
      "43/88, train_loss: 0.3056\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.3132\n",
      "47/88, train_loss: 0.1734\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.2959\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.3560\n",
      "53/88, train_loss: 0.2951\n",
      "54/88, train_loss: 0.5003\n",
      "55/88, train_loss: 0.1862\n",
      "56/88, train_loss: 0.3066\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.3091\n",
      "59/88, train_loss: 0.3198\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2893\n",
      "63/88, train_loss: 0.2919\n",
      "64/88, train_loss: 0.3303\n",
      "65/88, train_loss: 0.3057\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.3020\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5003\n",
      "73/88, train_loss: 0.3070\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.1858\n",
      "77/88, train_loss: 0.4311\n",
      "78/88, train_loss: 0.5004\n",
      "79/88, train_loss: 0.2890\n",
      "80/88, train_loss: 0.3041\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.3174\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.3578\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5002\n",
      "87/88, train_loss: 0.3561\n",
      "88/88, train_loss: 0.3627\n",
      "Epoch 53 average loss: 0.4121\n",
      "\n",
      "Epoch 54/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.3117\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.3300\n",
      "5/88, train_loss: 0.3618\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.0949\n",
      "11/88, train_loss: 0.5004\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.1253\n",
      "14/88, train_loss: 0.2905\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5002\n",
      "17/88, train_loss: 0.3876\n",
      "18/88, train_loss: 0.3305\n",
      "19/88, train_loss: 0.3108\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.2954\n",
      "25/88, train_loss: 0.1365\n",
      "26/88, train_loss: 0.2884\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.3383\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.3422\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.3368\n",
      "33/88, train_loss: 0.5002\n",
      "34/88, train_loss: 0.2834\n",
      "35/88, train_loss: 0.3002\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.3134\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.4273\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5002\n",
      "42/88, train_loss: 0.2924\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.2996\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.2942\n",
      "47/88, train_loss: 0.0997\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.3206\n",
      "50/88, train_loss: 0.4825\n",
      "51/88, train_loss: 0.5002\n",
      "52/88, train_loss: 0.3255\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.3236\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.2955\n",
      "57/88, train_loss: 0.5003\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.1308\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5003\n",
      "62/88, train_loss: 0.3530\n",
      "63/88, train_loss: 0.3710\n",
      "64/88, train_loss: 0.3563\n",
      "65/88, train_loss: 0.3151\n",
      "66/88, train_loss: 0.2863\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.3010\n",
      "70/88, train_loss: 0.2941\n",
      "71/88, train_loss: 0.3363\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.3269\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.3157\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.5004\n",
      "80/88, train_loss: 0.5002\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.3647\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.3345\n",
      "86/88, train_loss: 0.5002\n",
      "87/88, train_loss: 0.5005\n",
      "88/88, train_loss: 0.3702\n",
      "Epoch 54 average loss: 0.4068\n",
      "\n",
      "Epoch 55/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.5003\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.1471\n",
      "6/88, train_loss: 0.2940\n",
      "7/88, train_loss: 0.4282\n",
      "8/88, train_loss: 0.2881\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.3751\n",
      "11/88, train_loss: 0.2920\n",
      "12/88, train_loss: 0.3308\n",
      "13/88, train_loss: 0.3291\n",
      "14/88, train_loss: 0.2896\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.2895\n",
      "17/88, train_loss: 0.3178\n",
      "18/88, train_loss: 0.0888\n",
      "19/88, train_loss: 0.3008\n",
      "20/88, train_loss: 0.3377\n",
      "21/88, train_loss: 0.5002\n",
      "22/88, train_loss: 0.5005\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.3625\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.2944\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.3266\n",
      "36/88, train_loss: 0.3447\n",
      "37/88, train_loss: 0.5003\n",
      "38/88, train_loss: 0.2904\n",
      "39/88, train_loss: 0.3502\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2920\n",
      "44/88, train_loss: 0.2073\n",
      "45/88, train_loss: 0.3073\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.3345\n",
      "49/88, train_loss: 0.5001\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5001\n",
      "52/88, train_loss: 0.1053\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.3816\n",
      "55/88, train_loss: 0.3014\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.4008\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2988\n",
      "63/88, train_loss: 0.2903\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.3043\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.4006\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.3435\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.3011\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.3199\n",
      "79/88, train_loss: 0.3452\n",
      "80/88, train_loss: 0.2996\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5001\n",
      "83/88, train_loss: 0.2905\n",
      "84/88, train_loss: 0.3252\n",
      "85/88, train_loss: 0.3446\n",
      "86/88, train_loss: 0.3164\n",
      "87/88, train_loss: 0.2734\n",
      "88/88, train_loss: 0.3015\n",
      "Epoch 55 average loss: 0.4053\n",
      "Validation loss: 0.4916\n",
      "Current epoch: 55, current mean dice: 0.1325, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 56/300\n",
      "1/88, train_loss: 0.3203\n",
      "2/88, train_loss: 0.1260\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5001\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.2879\n",
      "7/88, train_loss: 0.2868\n",
      "8/88, train_loss: 0.2982\n",
      "9/88, train_loss: 0.5001\n",
      "10/88, train_loss: 0.3473\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.5001\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.3646\n",
      "15/88, train_loss: 0.3251\n",
      "16/88, train_loss: 0.5002\n",
      "17/88, train_loss: 0.3264\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.2871\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.3312\n",
      "22/88, train_loss: 0.3039\n",
      "23/88, train_loss: 0.2949\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.3161\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.3034\n",
      "29/88, train_loss: 0.2995\n",
      "30/88, train_loss: 0.2821\n",
      "31/88, train_loss: 0.3228\n",
      "32/88, train_loss: 0.3546\n",
      "33/88, train_loss: 0.5001\n",
      "34/88, train_loss: 0.3338\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.3351\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.1194\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2548\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.0927\n",
      "47/88, train_loss: 0.3005\n",
      "48/88, train_loss: 0.5008\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.1215\n",
      "51/88, train_loss: 0.2806\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.1244\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5002\n",
      "59/88, train_loss: 0.5003\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.3066\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.3042\n",
      "64/88, train_loss: 0.2800\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.2928\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.3088\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.2968\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5003\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.3212\n",
      "82/88, train_loss: 0.5001\n",
      "83/88, train_loss: 0.2825\n",
      "84/88, train_loss: 0.3409\n",
      "85/88, train_loss: 0.5002\n",
      "86/88, train_loss: 0.3202\n",
      "87/88, train_loss: 0.2916\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 56 average loss: 0.4019\n",
      "\n",
      "Epoch 57/300\n",
      "1/88, train_loss: 0.0898\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.3514\n",
      "4/88, train_loss: 0.0821\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.0991\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.3195\n",
      "12/88, train_loss: 0.3335\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.3165\n",
      "16/88, train_loss: 0.4162\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.2908\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.3040\n",
      "22/88, train_loss: 0.2816\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.2779\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.2919\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.5002\n",
      "34/88, train_loss: 0.3141\n",
      "35/88, train_loss: 0.3004\n",
      "36/88, train_loss: 0.3059\n",
      "37/88, train_loss: 0.2987\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.2926\n",
      "41/88, train_loss: 0.2927\n",
      "42/88, train_loss: 0.5001\n",
      "43/88, train_loss: 0.3051\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.3382\n",
      "49/88, train_loss: 0.3240\n",
      "50/88, train_loss: 0.2841\n",
      "51/88, train_loss: 0.5002\n",
      "52/88, train_loss: 0.5001\n",
      "53/88, train_loss: 0.5002\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.2921\n",
      "57/88, train_loss: 0.1765\n",
      "58/88, train_loss: 0.5002\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5004\n",
      "61/88, train_loss: 0.3091\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.1190\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.3079\n",
      "66/88, train_loss: 0.3160\n",
      "67/88, train_loss: 0.2809\n",
      "68/88, train_loss: 0.3250\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.3179\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.1051\n",
      "73/88, train_loss: 0.3776\n",
      "74/88, train_loss: 0.1947\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.2947\n",
      "82/88, train_loss: 0.5002\n",
      "83/88, train_loss: 0.1223\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5002\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.3293\n",
      "88/88, train_loss: 0.2939\n",
      "Epoch 57 average loss: 0.3997\n",
      "\n",
      "Epoch 58/300\n",
      "1/88, train_loss: 0.2961\n",
      "2/88, train_loss: 0.3028\n",
      "3/88, train_loss: 0.2909\n",
      "4/88, train_loss: 0.2902\n",
      "5/88, train_loss: 0.0599\n",
      "6/88, train_loss: 0.5001\n",
      "7/88, train_loss: 0.2984\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.2569\n",
      "12/88, train_loss: 0.3397\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.2709\n",
      "16/88, train_loss: 0.5004\n",
      "17/88, train_loss: 0.5001\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.2904\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.5003\n",
      "22/88, train_loss: 0.5003\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5009\n",
      "25/88, train_loss: 0.3173\n",
      "26/88, train_loss: 0.3028\n",
      "27/88, train_loss: 0.3019\n",
      "28/88, train_loss: 0.3063\n",
      "29/88, train_loss: 0.5006\n",
      "30/88, train_loss: 0.3180\n",
      "31/88, train_loss: 0.5002\n",
      "32/88, train_loss: 0.5003\n",
      "33/88, train_loss: 0.2986\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.3307\n",
      "36/88, train_loss: 0.3117\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.3103\n",
      "41/88, train_loss: 0.0915\n",
      "42/88, train_loss: 0.3099\n",
      "43/88, train_loss: 0.4351\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.2934\n",
      "48/88, train_loss: 0.1889\n",
      "49/88, train_loss: 0.3673\n",
      "50/88, train_loss: 0.5002\n",
      "51/88, train_loss: 0.3603\n",
      "52/88, train_loss: 0.3285\n",
      "53/88, train_loss: 0.3144\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.3128\n",
      "56/88, train_loss: 0.2946\n",
      "57/88, train_loss: 0.3232\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.3124\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2900\n",
      "62/88, train_loss: 0.5002\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.3155\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.3397\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.2001\n",
      "74/88, train_loss: 0.3694\n",
      "75/88, train_loss: 0.5002\n",
      "76/88, train_loss: 0.3130\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5004\n",
      "79/88, train_loss: 0.3246\n",
      "80/88, train_loss: 0.3207\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.3314\n",
      "85/88, train_loss: 0.3358\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5002\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 58 average loss: 0.4042\n",
      "\n",
      "Epoch 59/300\n",
      "1/88, train_loss: 0.0851\n",
      "2/88, train_loss: 0.2821\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.3042\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.1263\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.3249\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.3557\n",
      "15/88, train_loss: 0.3512\n",
      "16/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.1055\n",
      "48/88, train_loss: 0.2931\n",
      "49/88, train_loss: 0.1533\n",
      "50/88, train_loss: 0.2991\n",
      "51/88, train_loss: 0.5003\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.2995\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.3411\n",
      "59/88, train_loss: 0.1641\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.1159\n",
      "63/88, train_loss: 0.3380\n",
      "64/88, train_loss: 0.4081\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.3871\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.3670\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.5004\n",
      "72/88, train_loss: 0.3577\n",
      "73/88, train_loss: 0.4844\n",
      "74/88, train_loss: 0.3722\n",
      "75/88, train_loss: 0.3224\n",
      "76/88, train_loss: 0.5002\n",
      "77/88, train_loss: 0.3395\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.3027\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.3487\n",
      "83/88, train_loss: 0.2869\n",
      "84/88, train_loss: 0.3053\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 59 average loss: 0.4084\n",
      "\n",
      "Epoch 60/300\n",
      "1/88, train_loss: 0.1819\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5005\n",
      "4/88, train_loss: 0.3095\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.3694\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.5001\n",
      "10/88, train_loss: 0.5002\n",
      "11/88, train_loss: 0.3194\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.2918\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.3309\n",
      "17/88, train_loss: 0.3147\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.1219\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.3293\n",
      "24/88, train_loss: 0.3214\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.3267\n",
      "27/88, train_loss: 0.5002\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.3393\n",
      "33/88, train_loss: 0.2901\n",
      "34/88, train_loss: 0.3755\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.3077\n",
      "37/88, train_loss: 0.3500\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.3816\n",
      "41/88, train_loss: 0.5002\n",
      "42/88, train_loss: 0.2917\n",
      "43/88, train_loss: 0.2884\n",
      "44/88, train_loss: 0.3053\n",
      "45/88, train_loss: 0.5003\n",
      "46/88, train_loss: 0.1758\n",
      "47/88, train_loss: 0.3423\n",
      "48/88, train_loss: 0.5001\n",
      "49/88, train_loss: 0.3395\n",
      "50/88, train_loss: 0.3192\n",
      "51/88, train_loss: 0.3524\n",
      "52/88, train_loss: 0.2912\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.0696\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5004\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2961\n",
      "63/88, train_loss: 0.4331\n",
      "64/88, train_loss: 0.2728\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.3065\n",
      "67/88, train_loss: 0.3016\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.2827\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.5002\n",
      "74/88, train_loss: 0.3245\n",
      "75/88, train_loss: 0.3146\n",
      "76/88, train_loss: 0.5005\n",
      "77/88, train_loss: 0.4339\n",
      "78/88, train_loss: 0.3152\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5003\n",
      "81/88, train_loss: 0.5001\n",
      "82/88, train_loss: 0.2989\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.4151\n",
      "85/88, train_loss: 0.3037\n",
      "86/88, train_loss: 0.3253\n",
      "87/88, train_loss: 0.5001\n",
      "88/88, train_loss: 0.3260\n",
      "Epoch 60 average loss: 0.4090\n",
      "Validation loss: 0.4918\n",
      "Current epoch: 60, current mean dice: 0.1313, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 61/300\n",
      "1/88, train_loss: 0.0883\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.3318\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.2287\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.3272\n",
      "15/88, train_loss: 0.3272\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.2781\n",
      "18/88, train_loss: 0.3222\n",
      "19/88, train_loss: 0.2964\n",
      "20/88, train_loss: 0.3055\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.3187\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.2880\n",
      "26/88, train_loss: 0.3209\n",
      "27/88, train_loss: 0.2998\n",
      "28/88, train_loss: 0.2970\n",
      "29/88, train_loss: 0.2975\n",
      "30/88, train_loss: 0.3201\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.3744\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.3358\n",
      "40/88, train_loss: 0.1011\n",
      "41/88, train_loss: 0.2968\n",
      "42/88, train_loss: 0.2886\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.3139\n",
      "45/88, train_loss: 0.3239\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.5004\n",
      "49/88, train_loss: 0.2867\n",
      "50/88, train_loss: 0.5002\n",
      "51/88, train_loss: 0.3801\n",
      "52/88, train_loss: 0.3551\n",
      "53/88, train_loss: 0.5002\n",
      "54/88, train_loss: 0.1629\n",
      "55/88, train_loss: 0.3408\n",
      "56/88, train_loss: 0.5004\n",
      "57/88, train_loss: 0.3013\n",
      "58/88, train_loss: 0.2854\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2323\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5005\n",
      "66/88, train_loss: 0.2858\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.3798\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.3204\n",
      "74/88, train_loss: 0.5004\n",
      "75/88, train_loss: 0.3199\n",
      "76/88, train_loss: 0.5004\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.3085\n",
      "79/88, train_loss: 0.2905\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.3376\n",
      "83/88, train_loss: 0.2993\n",
      "84/88, train_loss: 0.3071\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.5001\n",
      "87/88, train_loss: 0.2955\n",
      "88/88, train_loss: 0.3371\n",
      "Epoch 61 average loss: 0.4035\n",
      "\n",
      "Epoch 62/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.3368\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.3268\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5002\n",
      "10/88, train_loss: 0.2838\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5002\n",
      "14/88, train_loss: 0.3325\n",
      "15/88, train_loss: 0.3474\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.2939\n",
      "18/88, train_loss: 0.0887\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.3028\n",
      "21/88, train_loss: 0.0839\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5002\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.2882\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.2835\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.2961\n",
      "33/88, train_loss: 0.2894\n",
      "34/88, train_loss: 0.3008\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.2944\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.3721\n",
      "39/88, train_loss: 0.5004\n",
      "40/88, train_loss: 0.1704\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5001\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.3270\n",
      "46/88, train_loss: 0.2891\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.3086\n",
      "49/88, train_loss: 0.3413\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.5001\n",
      "52/88, train_loss: 0.2916\n",
      "53/88, train_loss: 0.2906\n",
      "54/88, train_loss: 0.1253\n",
      "55/88, train_loss: 0.3334\n",
      "56/88, train_loss: 0.2923\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.5002\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.5005\n",
      "63/88, train_loss: 0.5002\n",
      "64/88, train_loss: 0.3176\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.2991\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.3120\n",
      "69/88, train_loss: 0.2798\n",
      "70/88, train_loss: 0.3016\n",
      "71/88, train_loss: 0.3170\n",
      "72/88, train_loss: 0.2891\n",
      "73/88, train_loss: 0.5002\n",
      "74/88, train_loss: 0.3079\n",
      "75/88, train_loss: 0.1577\n",
      "76/88, train_loss: 0.3319\n",
      "77/88, train_loss: 0.3301\n",
      "78/88, train_loss: 0.3246\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5002\n",
      "82/88, train_loss: 0.2976\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.2806\n",
      "85/88, train_loss: 0.2826\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.3515\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 62 average loss: 0.3986\n",
      "\n",
      "Epoch 63/300\n",
      "1/88, train_loss: 0.2844\n",
      "2/88, train_loss: 0.3060\n",
      "3/88, train_loss: 0.3005\n",
      "4/88, train_loss: 0.2975\n",
      "5/88, train_loss: 0.3073\n",
      "6/88, train_loss: 0.2847\n",
      "7/88, train_loss: 0.3169\n",
      "8/88, train_loss: 0.2783\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.3318\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.3125\n",
      "16/88, train_loss: 0.2929\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.4867\n",
      "19/88, train_loss: 0.4164\n",
      "20/88, train_loss: 0.3298\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5008\n",
      "23/88, train_loss: 0.2916\n",
      "24/88, train_loss: 0.2932\n",
      "25/88, train_loss: 0.5002\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.5003\n",
      "28/88, train_loss: 0.3196\n",
      "29/88, train_loss: 0.3382\n",
      "30/88, train_loss: 0.3201\n",
      "31/88, train_loss: 0.2875\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.5005\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.3012\n",
      "38/88, train_loss: 0.3141\n",
      "39/88, train_loss: 0.5002\n",
      "40/88, train_loss: 0.2795\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.2919\n",
      "46/88, train_loss: 0.3148\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.3088\n",
      "50/88, train_loss: 0.2871\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5002\n",
      "53/88, train_loss: 0.3067\n",
      "54/88, train_loss: 0.5003\n",
      "55/88, train_loss: 0.2973\n",
      "56/88, train_loss: 0.2874\n",
      "57/88, train_loss: 0.1918\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.3239\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.3464\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.3703\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.4060\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.2773\n",
      "70/88, train_loss: 0.3030\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.3021\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.5007\n",
      "75/88, train_loss: 0.3126\n",
      "76/88, train_loss: 0.5003\n",
      "77/88, train_loss: 0.2960\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.3238\n",
      "81/88, train_loss: 0.2842\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.3276\n",
      "84/88, train_loss: 0.2963\n",
      "85/88, train_loss: 0.3522\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.2798\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 63 average loss: 0.4021\n",
      "\n",
      "Epoch 64/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.2950\n",
      "4/88, train_loss: 0.2914\n",
      "5/88, train_loss: 0.3038\n",
      "6/88, train_loss: 0.3384\n",
      "7/88, train_loss: 0.4024\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.0991\n",
      "10/88, train_loss: 0.5002\n",
      "11/88, train_loss: 0.5003\n",
      "12/88, train_loss: 0.2851\n",
      "13/88, train_loss: 0.5002\n",
      "14/88, train_loss: 0.5004\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5002\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.3042\n",
      "22/88, train_loss: 0.3606\n",
      "23/88, train_loss: 0.5001\n",
      "24/88, train_loss: 0.2870\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.2828\n",
      "28/88, train_loss: 0.3157\n",
      "29/88, train_loss: 0.2825\n",
      "30/88, train_loss: 0.2973\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.3383\n",
      "36/88, train_loss: 0.2821\n",
      "37/88, train_loss: 0.2823\n",
      "38/88, train_loss: 0.3133\n",
      "39/88, train_loss: 0.2975\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.3200\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2900\n",
      "44/88, train_loss: 0.2825\n",
      "45/88, train_loss: 0.2889\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5002\n",
      "48/88, train_loss: 0.3198\n",
      "49/88, train_loss: 0.5001\n",
      "50/88, train_loss: 0.3208\n",
      "51/88, train_loss: 0.2808\n",
      "52/88, train_loss: 0.0847\n",
      "53/88, train_loss: 0.3218\n",
      "54/88, train_loss: 0.2897\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.3575\n",
      "58/88, train_loss: 0.3249\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.2986\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.3207\n",
      "63/88, train_loss: 0.5004\n",
      "64/88, train_loss: 0.5004\n",
      "65/88, train_loss: 0.2882\n",
      "66/88, train_loss: 0.3205\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5002\n",
      "69/88, train_loss: 0.5008\n",
      "70/88, train_loss: 0.3781\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5003\n",
      "73/88, train_loss: 0.1594\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.3016\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.4044\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.2805\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.1183\n",
      "84/88, train_loss: 0.5000\n",
      "85/88, train_loss: 0.3373\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.2861\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 64 average loss: 0.3993\n",
      "\n",
      "Epoch 65/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5006\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.3476\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.3308\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.2861\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.2962\n",
      "14/88, train_loss: 0.2966\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.2926\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.2935\n",
      "19/88, train_loss: 0.3522\n",
      "20/88, train_loss: 0.3042\n",
      "21/88, train_loss: 0.2966\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.2741\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.3247\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.3288\n",
      "28/88, train_loss: 0.2846\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.2991\n",
      "31/88, train_loss: 0.5002\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.3124\n",
      "34/88, train_loss: 0.2854\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.2954\n",
      "37/88, train_loss: 0.2946\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.4215\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2887\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.2935\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5006\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.3347\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.1043\n",
      "52/88, train_loss: 0.3519\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.3199\n",
      "56/88, train_loss: 0.1432\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.3698\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.3317\n",
      "61/88, train_loss: 0.3392\n",
      "62/88, train_loss: 0.3740\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5002\n",
      "65/88, train_loss: 0.2966\n",
      "66/88, train_loss: 0.1378\n",
      "67/88, train_loss: 0.2855\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.5002\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.2865\n",
      "73/88, train_loss: 0.2787\n",
      "74/88, train_loss: 0.2909\n",
      "75/88, train_loss: 0.2929\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.3046\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.2838\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.0716\n",
      "87/88, train_loss: 0.1117\n",
      "88/88, train_loss: 0.5002\n",
      "Epoch 65 average loss: 0.3990\n",
      "Validation loss: 0.4926\n",
      "Current epoch: 65, current mean dice: 0.1240, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 66/300\n",
      "1/88, train_loss: 0.5002\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.2969\n",
      "5/88, train_loss: 0.2961\n",
      "6/88, train_loss: 0.2938\n",
      "7/88, train_loss: 0.3099\n",
      "8/88, train_loss: 0.2988\n",
      "9/88, train_loss: 0.5001\n",
      "10/88, train_loss: 0.3106\n",
      "11/88, train_loss: 0.5002\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5001\n",
      "16/88, train_loss: 0.1150\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.0595\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.3133\n",
      "22/88, train_loss: 0.2969\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.3454\n",
      "26/88, train_loss: 0.1916\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5002\n",
      "30/88, train_loss: 0.3410\n",
      "31/88, train_loss: 0.2825\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.2869\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.3211\n",
      "36/88, train_loss: 0.2848\n",
      "37/88, train_loss: 0.5003\n",
      "38/88, train_loss: 0.2968\n",
      "39/88, train_loss: 0.3004\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.2842\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.2937\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.3115\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.1458\n",
      "53/88, train_loss: 0.5001\n",
      "54/88, train_loss: 0.2869\n",
      "55/88, train_loss: 0.2971\n",
      "56/88, train_loss: 0.3116\n",
      "57/88, train_loss: 0.0848\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5005\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.2838\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.3146\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.1153\n",
      "67/88, train_loss: 0.5002\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.2980\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.1092\n",
      "75/88, train_loss: 0.0909\n",
      "76/88, train_loss: 0.3075\n",
      "77/88, train_loss: 0.2913\n",
      "78/88, train_loss: 0.2834\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.2955\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.2957\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.3094\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 66 average loss: 0.3949\n",
      "\n",
      "Epoch 67/300\n",
      "1/88, train_loss: 0.3218\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.3240\n",
      "4/88, train_loss: 0.2935\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.3503\n",
      "8/88, train_loss: 0.5004\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.3160\n",
      "12/88, train_loss: 0.2885\n",
      "13/88, train_loss: 0.3413\n",
      "14/88, train_loss: 0.2769\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.0692\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.2999\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.5004\n",
      "21/88, train_loss: 0.2987\n",
      "22/88, train_loss: 0.0816\n",
      "23/88, train_loss: 0.5006\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.2843\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.2779\n",
      "28/88, train_loss: 0.2983\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.0711\n",
      "31/88, train_loss: 0.2981\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.2680\n",
      "35/88, train_loss: 0.3210\n",
      "36/88, train_loss: 0.3163\n",
      "37/88, train_loss: 0.5004\n",
      "38/88, train_loss: 0.5002\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.0727\n",
      "41/88, train_loss: 0.2929\n",
      "42/88, train_loss: 0.5004\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.2818\n",
      "45/88, train_loss: 0.3032\n",
      "46/88, train_loss: 0.3299\n",
      "47/88, train_loss: 0.5003\n",
      "48/88, train_loss: 0.2834\n",
      "49/88, train_loss: 0.3705\n",
      "50/88, train_loss: 0.5002\n",
      "51/88, train_loss: 0.5001\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.2973\n",
      "54/88, train_loss: 0.2843\n",
      "55/88, train_loss: 0.5002\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5002\n",
      "59/88, train_loss: 0.2919\n",
      "60/88, train_loss: 0.2763\n",
      "61/88, train_loss: 0.3154\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.2782\n",
      "64/88, train_loss: 0.2753\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.3044\n",
      "67/88, train_loss: 0.0697\n",
      "68/88, train_loss: 0.5001\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.3182\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.0849\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.2793\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.2915\n",
      "84/88, train_loss: 0.5000\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.5001\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.2824\n",
      "Epoch 67 average loss: 0.3919\n",
      "\n",
      "Epoch 68/300\n",
      "1/88, train_loss: 0.2774\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5005\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.3097\n",
      "7/88, train_loss: 0.2785\n",
      "8/88, train_loss: 0.2750\n",
      "9/88, train_loss: 0.5003\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.2826\n",
      "14/88, train_loss: 0.3169\n",
      "15/88, train_loss: 0.2776\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.5001\n",
      "18/88, train_loss: 0.2798\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.3213\n",
      "21/88, train_loss: 0.2868\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.3000\n",
      "26/88, train_loss: 0.2730\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.2996\n",
      "31/88, train_loss: 0.2952\n",
      "32/88, train_loss: 0.2900\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.5002\n",
      "35/88, train_loss: 0.2829\n",
      "36/88, train_loss: 0.5002\n",
      "37/88, train_loss: 0.5002\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.2793\n",
      "41/88, train_loss: 0.5003\n",
      "42/88, train_loss: 0.5002\n",
      "43/88, train_loss: 0.2997\n",
      "44/88, train_loss: 0.3023\n",
      "45/88, train_loss: 0.5002\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.1717\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.0692\n",
      "50/88, train_loss: 0.2640\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.0580\n",
      "53/88, train_loss: 0.5005\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.3077\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.2782\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2856\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.2059\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.2813\n",
      "66/88, train_loss: 0.5003\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.3215\n",
      "69/88, train_loss: 0.2830\n",
      "70/88, train_loss: 0.2840\n",
      "71/88, train_loss: 0.2853\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.2836\n",
      "74/88, train_loss: 0.5006\n",
      "75/88, train_loss: 0.3024\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.1011\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.3249\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.2998\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.1887\n",
      "84/88, train_loss: 0.3056\n",
      "85/88, train_loss: 0.2964\n",
      "86/88, train_loss: 0.3136\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 68 average loss: 0.3925\n",
      "\n",
      "Epoch 69/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.1112\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5003\n",
      "6/88, train_loss: 0.5003\n",
      "7/88, train_loss: 0.2816\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5001\n",
      "10/88, train_loss: 0.2938\n",
      "11/88, train_loss: 0.5003\n",
      "12/88, train_loss: 0.2920\n",
      "13/88, train_loss: 0.2993\n",
      "14/88, train_loss: 0.2882\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.0896\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.3294\n",
      "20/88, train_loss: 0.2970\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5002\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5002\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.5002\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.2984\n",
      "29/88, train_loss: 0.2853\n",
      "30/88, train_loss: 0.2870\n",
      "31/88, train_loss: 0.5004\n",
      "32/88, train_loss: 0.2849\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.3109\n",
      "35/88, train_loss: 0.2802\n",
      "36/88, train_loss: 0.0534\n",
      "37/88, train_loss: 0.3334\n",
      "38/88, train_loss: 0.3203\n",
      "39/88, train_loss: 0.2869\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2843\n",
      "44/88, train_loss: 0.3349\n",
      "45/88, train_loss: 0.5001\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.5002\n",
      "49/88, train_loss: 0.2775\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.3062\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.2869\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.3161\n",
      "56/88, train_loss: 0.2874\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.2981\n",
      "59/88, train_loss: 0.5003\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2819\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.2946\n",
      "65/88, train_loss: 0.3000\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.3738\n",
      "70/88, train_loss: 0.2771\n",
      "71/88, train_loss: 0.2748\n",
      "72/88, train_loss: 0.2948\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.5001\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.2855\n",
      "77/88, train_loss: 0.1049\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.2722\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.2966\n",
      "83/88, train_loss: 0.2605\n",
      "84/88, train_loss: 0.0856\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.0788\n",
      "Epoch 69 average loss: 0.3909\n",
      "\n",
      "Epoch 70/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.2750\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.2823\n",
      "7/88, train_loss: 0.2833\n",
      "8/88, train_loss: 0.3437\n",
      "9/88, train_loss: 0.3191\n",
      "10/88, train_loss: 0.2999\n",
      "11/88, train_loss: 0.5006\n",
      "12/88, train_loss: 0.2764\n",
      "13/88, train_loss: 0.3418\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.2908\n",
      "16/88, train_loss: 0.3185\n",
      "17/88, train_loss: 0.2878\n",
      "18/88, train_loss: 0.3220\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.2991\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.1367\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5003\n",
      "26/88, train_loss: 0.2879\n",
      "27/88, train_loss: 0.2921\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.0646\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.0925\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.2932\n",
      "35/88, train_loss: 0.2905\n",
      "36/88, train_loss: 0.5001\n",
      "37/88, train_loss: 0.2903\n",
      "38/88, train_loss: 0.3408\n",
      "39/88, train_loss: 0.2815\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.1245\n",
      "43/88, train_loss: 0.3056\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5002\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.2928\n",
      "48/88, train_loss: 0.0615\n",
      "49/88, train_loss: 0.5006\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.2879\n",
      "54/88, train_loss: 0.2883\n",
      "55/88, train_loss: 0.2935\n",
      "56/88, train_loss: 0.5002\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.3008\n",
      "59/88, train_loss: 0.2798\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.2887\n",
      "64/88, train_loss: 0.2812\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.2924\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.3046\n",
      "75/88, train_loss: 0.2767\n",
      "76/88, train_loss: 0.2986\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.0658\n",
      "80/88, train_loss: 0.2755\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.2772\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 70 average loss: 0.3910\n",
      "Validation loss: 0.4942\n",
      "Current epoch: 70, current mean dice: 0.1040, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 71/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.3143\n",
      "3/88, train_loss: 0.2753\n",
      "4/88, train_loss: 0.2919\n",
      "5/88, train_loss: 0.2872\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.2977\n",
      "8/88, train_loss: 0.0507\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.3048\n",
      "11/88, train_loss: 0.2847\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.2865\n",
      "14/88, train_loss: 0.2842\n",
      "15/88, train_loss: 0.2901\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.2980\n",
      "18/88, train_loss: 0.2716\n",
      "19/88, train_loss: 0.3084\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.2889\n",
      "23/88, train_loss: 0.2758\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.2820\n",
      "26/88, train_loss: 0.2803\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.0729\n",
      "29/88, train_loss: 0.5002\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.5002\n",
      "32/88, train_loss: 0.0625\n",
      "33/88, train_loss: 0.3108\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.2853\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.2813\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.3115\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.2869\n",
      "47/88, train_loss: 0.3258\n",
      "48/88, train_loss: 0.2745\n",
      "49/88, train_loss: 0.2856\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.3209\n",
      "52/88, train_loss: 0.5004\n",
      "53/88, train_loss: 0.2675\n",
      "54/88, train_loss: 0.3426\n",
      "55/88, train_loss: 0.5002\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5001\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5003\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.5002\n",
      "62/88, train_loss: 0.1025\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5002\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.2915\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.2781\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.3347\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.2941\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.2823\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.3465\n",
      "82/88, train_loss: 0.5002\n",
      "83/88, train_loss: 0.2849\n",
      "84/88, train_loss: 0.0876\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.3159\n",
      "87/88, train_loss: 0.2904\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 71 average loss: 0.3899\n",
      "\n",
      "Epoch 72/300\n",
      "1/88, train_loss: 0.3011\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.5002\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5002\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.2820\n",
      "11/88, train_loss: 0.2692\n",
      "12/88, train_loss: 0.2946\n",
      "13/88, train_loss: 0.2875\n",
      "14/88, train_loss: 0.5003\n",
      "15/88, train_loss: 0.1393\n",
      "16/88, train_loss: 0.0704\n",
      "17/88, train_loss: 0.2942\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.2715\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.2763\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.2775\n",
      "29/88, train_loss: 0.0930\n",
      "30/88, train_loss: 0.3042\n",
      "31/88, train_loss: 0.5004\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.0641\n",
      "36/88, train_loss: 0.5001\n",
      "37/88, train_loss: 0.3052\n",
      "38/88, train_loss: 0.2706\n",
      "39/88, train_loss: 0.5001\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.5001\n",
      "43/88, train_loss: 0.2719\n",
      "44/88, train_loss: 0.2803\n",
      "45/88, train_loss: 0.2882\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.2914\n",
      "48/88, train_loss: 0.3459\n",
      "49/88, train_loss: 0.2759\n",
      "50/88, train_loss: 0.3070\n",
      "51/88, train_loss: 0.5001\n",
      "52/88, train_loss: 0.1293\n",
      "53/88, train_loss: 0.0775\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.2661\n",
      "56/88, train_loss: 0.2868\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5006\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2922\n",
      "62/88, train_loss: 0.5001\n",
      "63/88, train_loss: 0.2803\n",
      "64/88, train_loss: 0.2760\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.2890\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.1128\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.2920\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5002\n",
      "74/88, train_loss: 0.2894\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.2903\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.1269\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5000\n",
      "85/88, train_loss: 0.0587\n",
      "86/88, train_loss: 0.5002\n",
      "87/88, train_loss: 0.2799\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 72 average loss: 0.3888\n",
      "\n",
      "Epoch 73/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.2883\n",
      "3/88, train_loss: 0.1042\n",
      "4/88, train_loss: 0.2803\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5002\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5002\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.5006\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.2870\n",
      "17/88, train_loss: 0.0987\n",
      "18/88, train_loss: 0.5002\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.2764\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5004\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.3225\n",
      "25/88, train_loss: 0.2771\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.3121\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.0738\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.2963\n",
      "34/88, train_loss: 0.0745\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.2730\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5002\n",
      "40/88, train_loss: 0.2792\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.2766\n",
      "43/88, train_loss: 0.5002\n",
      "44/88, train_loss: 0.2808\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5003\n",
      "48/88, train_loss: 0.2961\n",
      "49/88, train_loss: 0.3135\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.2682\n",
      "52/88, train_loss: 0.2858\n",
      "53/88, train_loss: 0.5001\n",
      "54/88, train_loss: 0.3338\n",
      "55/88, train_loss: 0.2818\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.2895\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2794\n",
      "62/88, train_loss: 0.0949\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5003\n",
      "65/88, train_loss: 0.0553\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.2721\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.5003\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.1176\n",
      "75/88, train_loss: 0.3007\n",
      "76/88, train_loss: 0.2767\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.2842\n",
      "79/88, train_loss: 0.2866\n",
      "80/88, train_loss: 0.2775\n",
      "81/88, train_loss: 0.1267\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.3058\n",
      "84/88, train_loss: 0.2784\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.0507\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.2975\n",
      "Epoch 73 average loss: 0.3884\n",
      "\n",
      "Epoch 74/300\n",
      "1/88, train_loss: 0.2819\n",
      "2/88, train_loss: 0.2793\n",
      "3/88, train_loss: 0.2895\n",
      "4/88, train_loss: 0.2879\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.2862\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.2781\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.2756\n",
      "12/88, train_loss: 0.5001\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5001\n",
      "17/88, train_loss: 0.5002\n",
      "18/88, train_loss: 0.3184\n",
      "19/88, train_loss: 0.2739\n",
      "20/88, train_loss: 0.5001\n",
      "21/88, train_loss: 0.3287\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.5003\n",
      "24/88, train_loss: 0.5002\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.0662\n",
      "27/88, train_loss: 0.5001\n",
      "28/88, train_loss: 0.3077\n",
      "29/88, train_loss: 0.5005\n",
      "30/88, train_loss: 0.5003\n",
      "31/88, train_loss: 0.0666\n",
      "32/88, train_loss: 0.3169\n",
      "33/88, train_loss: 0.0907\n",
      "34/88, train_loss: 0.1021\n",
      "35/88, train_loss: 0.2726\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.2771\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5003\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2925\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.3131\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.2722\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5001\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.0919\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.2901\n",
      "60/88, train_loss: 0.2796\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.0758\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.2911\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.2758\n",
      "69/88, train_loss: 0.0926\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5002\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.0838\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.2779\n",
      "76/88, train_loss: 0.2897\n",
      "77/88, train_loss: 0.2730\n",
      "78/88, train_loss: 0.5006\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.2806\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.2960\n",
      "83/88, train_loss: 0.2760\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.3719\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.0743\n",
      "88/88, train_loss: 0.2792\n",
      "Epoch 74 average loss: 0.3884\n",
      "\n",
      "Epoch 75/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.2715\n",
      "3/88, train_loss: 0.2793\n",
      "4/88, train_loss: 0.5001\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.2775\n",
      "8/88, train_loss: 0.2856\n",
      "9/88, train_loss: 0.2825\n",
      "10/88, train_loss: 0.3072\n",
      "11/88, train_loss: 0.2842\n",
      "12/88, train_loss: 0.5001\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.5001\n",
      "15/88, train_loss: 0.2823\n",
      "16/88, train_loss: 0.2858\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.2930\n",
      "19/88, train_loss: 0.5001\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.1025\n",
      "23/88, train_loss: 0.3139\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.2844\n",
      "26/88, train_loss: 0.3160\n",
      "27/88, train_loss: 0.2924\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.0880\n",
      "30/88, train_loss: 0.2806\n",
      "31/88, train_loss: 0.0525\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.2927\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.2922\n",
      "36/88, train_loss: 0.2829\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.2852\n",
      "39/88, train_loss: 0.0681\n",
      "40/88, train_loss: 0.0818\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5003\n",
      "46/88, train_loss: 0.3070\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.2845\n",
      "56/88, train_loss: 0.3124\n",
      "57/88, train_loss: 0.2833\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.2828\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2843\n",
      "62/88, train_loss: 0.3466\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.2948\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5001\n",
      "68/88, train_loss: 0.5001\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.2989\n",
      "71/88, train_loss: 0.0628\n",
      "72/88, train_loss: 0.5001\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.0707\n",
      "75/88, train_loss: 0.2921\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.5006\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.2978\n",
      "81/88, train_loss: 0.5005\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.2938\n",
      "84/88, train_loss: 0.2943\n",
      "85/88, train_loss: 0.3531\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.5004\n",
      "Epoch 75 average loss: 0.3891\n",
      "Validation loss: 0.4973\n",
      "Current epoch: 75, current mean dice: 0.0413, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 76/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.0532\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.3027\n",
      "10/88, train_loss: 0.2859\n",
      "11/88, train_loss: 0.2866\n",
      "12/88, train_loss: 0.0710\n",
      "13/88, train_loss: 0.3093\n",
      "14/88, train_loss: 0.2802\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.2900\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.2899\n",
      "20/88, train_loss: 0.2954\n",
      "21/88, train_loss: 0.5003\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.2876\n",
      "24/88, train_loss: 0.2816\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.5003\n",
      "27/88, train_loss: 0.2999\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5005\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.2909\n",
      "33/88, train_loss: 0.2743\n",
      "34/88, train_loss: 0.2830\n",
      "35/88, train_loss: 0.2726\n",
      "36/88, train_loss: 0.2899\n",
      "37/88, train_loss: 0.2705\n",
      "38/88, train_loss: 0.5002\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.2688\n",
      "41/88, train_loss: 0.3194\n",
      "42/88, train_loss: 0.2918\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.2979\n",
      "46/88, train_loss: 0.3254\n",
      "47/88, train_loss: 0.2800\n",
      "48/88, train_loss: 0.5001\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5003\n",
      "51/88, train_loss: 0.5002\n",
      "52/88, train_loss: 0.2813\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.2831\n",
      "56/88, train_loss: 0.5004\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.0600\n",
      "59/88, train_loss: 0.2748\n",
      "60/88, train_loss: 0.2983\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.5002\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.2841\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.2898\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.2654\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.3042\n",
      "75/88, train_loss: 0.3265\n",
      "76/88, train_loss: 0.2728\n",
      "77/88, train_loss: 0.2720\n",
      "78/88, train_loss: 0.2750\n",
      "79/88, train_loss: 0.2717\n",
      "80/88, train_loss: 0.2779\n",
      "81/88, train_loss: 0.2735\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.2982\n",
      "84/88, train_loss: 0.5004\n",
      "85/88, train_loss: 0.2702\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.2937\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 76 average loss: 0.3861\n",
      "\n",
      "Epoch 77/300\n",
      "1/88, train_loss: 0.1318\n",
      "2/88, train_loss: 0.3225\n",
      "3/88, train_loss: 0.2743\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5001\n",
      "10/88, train_loss: 0.1060\n",
      "11/88, train_loss: 0.2790\n",
      "12/88, train_loss: 0.2812\n",
      "13/88, train_loss: 0.5004\n",
      "14/88, train_loss: 0.2775\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.2716\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.2964\n",
      "20/88, train_loss: 0.2837\n",
      "21/88, train_loss: 0.2986\n",
      "22/88, train_loss: 0.3040\n",
      "23/88, train_loss: 0.0650\n",
      "24/88, train_loss: 0.2913\n",
      "25/88, train_loss: 0.5004\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.2868\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.2776\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.2865\n",
      "33/88, train_loss: 0.5001\n",
      "34/88, train_loss: 0.3064\n",
      "35/88, train_loss: 0.5002\n",
      "36/88, train_loss: 0.2814\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.5001\n",
      "39/88, train_loss: 0.0593\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.2864\n",
      "42/88, train_loss: 0.3362\n",
      "43/88, train_loss: 0.2851\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.2946\n",
      "47/88, train_loss: 0.2817\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5003\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.2755\n",
      "55/88, train_loss: 0.5000\n",
      "56/88, train_loss: 0.3319\n",
      "57/88, train_loss: 0.1943\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.3273\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.5005\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.5002\n",
      "66/88, train_loss: 0.5001\n",
      "67/88, train_loss: 0.2891\n",
      "68/88, train_loss: 0.1678\n",
      "69/88, train_loss: 0.2967\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.5003\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.2763\n",
      "74/88, train_loss: 0.2916\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5001\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.3034\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.3096\n",
      "81/88, train_loss: 0.3091\n",
      "82/88, train_loss: 0.2872\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5002\n",
      "85/88, train_loss: 0.2839\n",
      "86/88, train_loss: 0.2809\n",
      "87/88, train_loss: 0.3070\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 77 average loss: 0.3921\n",
      "\n",
      "Epoch 78/300\n",
      "1/88, train_loss: 0.2759\n",
      "2/88, train_loss: 0.2778\n",
      "3/88, train_loss: 0.5002\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.2829\n",
      "8/88, train_loss: 0.3034\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.2764\n",
      "11/88, train_loss: 0.0644\n",
      "12/88, train_loss: 0.5001\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.0728\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.2943\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.2913\n",
      "23/88, train_loss: 0.3116\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.3134\n",
      "26/88, train_loss: 0.2735\n",
      "27/88, train_loss: 0.2802\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5004\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.3146\n",
      "32/88, train_loss: 0.3162\n",
      "33/88, train_loss: 0.2775\n",
      "34/88, train_loss: 0.2694\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.3066\n",
      "37/88, train_loss: 0.2996\n",
      "38/88, train_loss: 0.3074\n",
      "39/88, train_loss: 0.5001\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.3475\n",
      "42/88, train_loss: 0.0769\n",
      "43/88, train_loss: 0.0612\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5001\n",
      "47/88, train_loss: 0.5002\n",
      "48/88, train_loss: 0.1204\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.3296\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.2759\n",
      "53/88, train_loss: 0.0922\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.5005\n",
      "56/88, train_loss: 0.5004\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5001\n",
      "59/88, train_loss: 0.2926\n",
      "60/88, train_loss: 0.2835\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.5003\n",
      "64/88, train_loss: 0.2738\n",
      "65/88, train_loss: 0.2825\n",
      "66/88, train_loss: 0.3193\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.2858\n",
      "69/88, train_loss: 0.5001\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.2789\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.2859\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.2849\n",
      "79/88, train_loss: 0.2844\n",
      "80/88, train_loss: 0.1042\n",
      "81/88, train_loss: 0.5002\n",
      "82/88, train_loss: 0.5001\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5000\n",
      "85/88, train_loss: 0.3379\n",
      "86/88, train_loss: 0.5001\n",
      "87/88, train_loss: 0.2935\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 78 average loss: 0.3900\n",
      "\n",
      "Epoch 79/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.2917\n",
      "3/88, train_loss: 0.2802\n",
      "4/88, train_loss: 0.3197\n",
      "5/88, train_loss: 0.2829\n",
      "6/88, train_loss: 0.5001\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.0633\n",
      "9/88, train_loss: 0.2981\n",
      "10/88, train_loss: 0.3526\n",
      "11/88, train_loss: 0.0677\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.1485\n",
      "14/88, train_loss: 0.2909\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.2835\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.2875\n",
      "22/88, train_loss: 0.2776\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.3078\n",
      "25/88, train_loss: 0.3555\n",
      "26/88, train_loss: 0.2772\n",
      "27/88, train_loss: 0.3020\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.2936\n",
      "30/88, train_loss: 0.5001\n",
      "31/88, train_loss: 0.0565\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.2900\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.5001\n",
      "36/88, train_loss: 0.2743\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.3012\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.3436\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5001\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5001\n",
      "45/88, train_loss: 0.2713\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.0624\n",
      "48/88, train_loss: 0.3149\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5005\n",
      "51/88, train_loss: 0.3084\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5004\n",
      "54/88, train_loss: 0.2805\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.3065\n",
      "57/88, train_loss: 0.0476\n",
      "58/88, train_loss: 0.2849\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.2867\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.5005\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.5004\n",
      "66/88, train_loss: 0.2681\n",
      "67/88, train_loss: 0.2754\n",
      "68/88, train_loss: 0.2732\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.2944\n",
      "73/88, train_loss: 0.3177\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.2806\n",
      "83/88, train_loss: 0.5002\n",
      "84/88, train_loss: 0.2780\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.0854\n",
      "Epoch 79 average loss: 0.3896\n",
      "\n",
      "Epoch 80/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.2968\n",
      "3/88, train_loss: 0.2695\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.2765\n",
      "7/88, train_loss: 0.2969\n",
      "8/88, train_loss: 0.5002\n",
      "9/88, train_loss: 0.2782\n",
      "10/88, train_loss: 0.2808\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.2816\n",
      "13/88, train_loss: 0.2781\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.2779\n",
      "16/88, train_loss: 0.2847\n",
      "17/88, train_loss: 0.2773\n",
      "18/88, train_loss: 0.2670\n",
      "19/88, train_loss: 0.2739\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.3171\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.2867\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.5002\n",
      "26/88, train_loss: 0.2964\n",
      "27/88, train_loss: 0.2785\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.2764\n",
      "30/88, train_loss: 0.2968\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.3195\n",
      "33/88, train_loss: 0.2756\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.2848\n",
      "36/88, train_loss: 0.2973\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.2840\n",
      "39/88, train_loss: 0.5001\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5002\n",
      "42/88, train_loss: 0.2785\n",
      "43/88, train_loss: 0.5001\n",
      "44/88, train_loss: 0.2871\n",
      "45/88, train_loss: 0.2716\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.2976\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.3085\n",
      "51/88, train_loss: 0.0639\n",
      "52/88, train_loss: 0.0662\n",
      "53/88, train_loss: 0.2928\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.2775\n",
      "59/88, train_loss: 0.2703\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5001\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.3053\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5003\n",
      "66/88, train_loss: 0.3008\n",
      "67/88, train_loss: 0.2758\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5004\n",
      "70/88, train_loss: 0.5001\n",
      "71/88, train_loss: 0.3390\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.2885\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.0953\n",
      "78/88, train_loss: 0.2726\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5002\n",
      "81/88, train_loss: 0.5004\n",
      "82/88, train_loss: 0.2787\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5000\n",
      "85/88, train_loss: 0.2846\n",
      "86/88, train_loss: 0.5005\n",
      "87/88, train_loss: 0.3130\n",
      "88/88, train_loss: 0.5004\n",
      "Epoch 80 average loss: 0.3866\n",
      "Validation loss: 0.4962\n",
      "Current epoch: 80, current mean dice: 0.0628, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 81/300\n",
      "1/88, train_loss: 0.0515\n",
      "2/88, train_loss: 0.2803\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.2775\n",
      "8/88, train_loss: 0.2698\n",
      "9/88, train_loss: 0.3036\n",
      "10/88, train_loss: 0.2712\n",
      "11/88, train_loss: 0.2990\n",
      "12/88, train_loss: 0.5002\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.2748\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.3336\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.2742\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.0902\n",
      "25/88, train_loss: 0.5005\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.0647\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.2920\n",
      "31/88, train_loss: 0.3070\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.2884\n",
      "34/88, train_loss: 0.5001\n",
      "35/88, train_loss: 0.2779\n",
      "36/88, train_loss: 0.2699\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.2772\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5001\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5004\n",
      "45/88, train_loss: 0.2920\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.2864\n",
      "48/88, train_loss: 0.3170\n",
      "49/88, train_loss: 0.2738\n",
      "50/88, train_loss: 0.0497\n",
      "51/88, train_loss: 0.2717\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.0488\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.2938\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.2948\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.3225\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.3176\n",
      "62/88, train_loss: 0.2691\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.2970\n",
      "65/88, train_loss: 0.2793\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.0493\n",
      "70/88, train_loss: 0.3056\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.2715\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.0350\n",
      "75/88, train_loss: 0.5004\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.3107\n",
      "80/88, train_loss: 0.2770\n",
      "81/88, train_loss: 0.5001\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5000\n",
      "85/88, train_loss: 0.5002\n",
      "86/88, train_loss: 0.2806\n",
      "87/88, train_loss: 0.2754\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 81 average loss: 0.3855\n",
      "\n",
      "Epoch 82/300\n",
      "1/88, train_loss: 0.0452\n",
      "2/88, train_loss: 0.2952\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.5000\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.0851\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.2945\n",
      "18/88, train_loss: 0.3139\n",
      "19/88, train_loss: 0.2925\n",
      "20/88, train_loss: 0.2699\n",
      "21/88, train_loss: 0.2948\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5004\n",
      "25/88, train_loss: 0.2761\n",
      "26/88, train_loss: 0.5001\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.3081\n",
      "29/88, train_loss: 0.3198\n",
      "30/88, train_loss: 0.2671\n",
      "31/88, train_loss: 0.5003\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.3019\n",
      "35/88, train_loss: 0.2651\n",
      "36/88, train_loss: 0.0623\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.2664\n",
      "39/88, train_loss: 0.2723\n",
      "40/88, train_loss: 0.3249\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2825\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.2806\n",
      "46/88, train_loss: 0.2957\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.2785\n",
      "49/88, train_loss: 0.5001\n",
      "50/88, train_loss: 0.3114\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5003\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.2786\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.2814\n",
      "57/88, train_loss: 0.5002\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.2814\n",
      "60/88, train_loss: 0.2793\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.5004\n",
      "63/88, train_loss: 0.2790\n",
      "64/88, train_loss: 0.2746\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.2854\n",
      "67/88, train_loss: 0.5007\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5007\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.2868\n",
      "74/88, train_loss: 0.2731\n",
      "75/88, train_loss: 0.2883\n",
      "76/88, train_loss: 0.2913\n",
      "77/88, train_loss: 0.2795\n",
      "78/88, train_loss: 0.5001\n",
      "79/88, train_loss: 0.2826\n",
      "80/88, train_loss: 0.2707\n",
      "81/88, train_loss: 0.2683\n",
      "82/88, train_loss: 0.2717\n",
      "83/88, train_loss: 0.2817\n",
      "84/88, train_loss: 0.0294\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5006\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.0845\n",
      "Epoch 82 average loss: 0.3850\n",
      "\n",
      "Epoch 83/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.3110\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.2655\n",
      "6/88, train_loss: 0.0731\n",
      "7/88, train_loss: 0.2754\n",
      "8/88, train_loss: 0.2623\n",
      "9/88, train_loss: 0.2628\n",
      "10/88, train_loss: 0.5001\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.2680\n",
      "13/88, train_loss: 0.2850\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.2755\n",
      "16/88, train_loss: 0.2716\n",
      "17/88, train_loss: 0.5002\n",
      "18/88, train_loss: 0.2702\n",
      "19/88, train_loss: 0.2781\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.2856\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.1327\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.3136\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.5000\n",
      "32/88, train_loss: 0.0708\n",
      "33/88, train_loss: 0.5002\n",
      "34/88, train_loss: 0.2899\n",
      "35/88, train_loss: 0.2695\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.2787\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.2774\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.2700\n",
      "44/88, train_loss: 0.3012\n",
      "45/88, train_loss: 0.0481\n",
      "46/88, train_loss: 0.0487\n",
      "47/88, train_loss: 0.0817\n",
      "48/88, train_loss: 0.2896\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.2726\n",
      "51/88, train_loss: 0.2740\n",
      "52/88, train_loss: 0.2983\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5003\n",
      "55/88, train_loss: 0.2862\n",
      "56/88, train_loss: 0.5000\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.2930\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2938\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.2952\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.5001\n",
      "67/88, train_loss: 0.5003\n",
      "68/88, train_loss: 0.3169\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.2741\n",
      "71/88, train_loss: 0.5001\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.5000\n",
      "74/88, train_loss: 0.2765\n",
      "75/88, train_loss: 0.2715\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5003\n",
      "79/88, train_loss: 0.5001\n",
      "80/88, train_loss: 0.5001\n",
      "81/88, train_loss: 0.5001\n",
      "82/88, train_loss: 0.5002\n",
      "83/88, train_loss: 0.2745\n",
      "84/88, train_loss: 0.2708\n",
      "85/88, train_loss: 0.3571\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.2834\n",
      "Epoch 83 average loss: 0.3852\n",
      "\n",
      "Epoch 84/300\n",
      "1/88, train_loss: 0.0742\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.3380\n",
      "4/88, train_loss: 0.3531\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.3375\n",
      "7/88, train_loss: 0.4501\n",
      "8/88, train_loss: 0.4724\n",
      "9/88, train_loss: 0.3428\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.2791\n",
      "12/88, train_loss: 0.4583\n",
      "13/88, train_loss: 0.5001\n",
      "14/88, train_loss: 0.3184\n",
      "15/88, train_loss: 0.3850\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.3713\n",
      "18/88, train_loss: 0.5003\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.3831\n",
      "21/88, train_loss: 0.2938\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.2807\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.3170\n",
      "26/88, train_loss: 0.2767\n",
      "27/88, train_loss: 0.2826\n",
      "28/88, train_loss: 0.5000\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.2287\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.3290\n",
      "33/88, train_loss: 0.5000\n",
      "34/88, train_loss: 0.0945\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.1003\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5002\n",
      "40/88, train_loss: 0.3410\n",
      "41/88, train_loss: 0.5001\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.3142\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5002\n",
      "47/88, train_loss: 0.3164\n",
      "48/88, train_loss: 0.3220\n",
      "49/88, train_loss: 0.5002\n",
      "50/88, train_loss: 0.3126\n",
      "51/88, train_loss: 0.2922\n",
      "52/88, train_loss: 0.0692\n",
      "53/88, train_loss: 0.3006\n",
      "54/88, train_loss: 0.3478\n",
      "55/88, train_loss: 0.0641\n",
      "56/88, train_loss: 0.5001\n",
      "57/88, train_loss: 0.2890\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5001\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2755\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.5000\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.2979\n",
      "73/88, train_loss: 0.3027\n",
      "74/88, train_loss: 0.3039\n",
      "75/88, train_loss: 0.3060\n",
      "76/88, train_loss: 0.3380\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.5000\n",
      "81/88, train_loss: 0.5001\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5001\n",
      "86/88, train_loss: 0.5000\n",
      "87/88, train_loss: 0.0878\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 84 average loss: 0.4051\n",
      "\n",
      "Epoch 85/300\n",
      "1/88, train_loss: 0.2862\n",
      "2/88, train_loss: 0.5001\n",
      "3/88, train_loss: 0.3195\n",
      "4/88, train_loss: 0.2754\n",
      "5/88, train_loss: 0.5000\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5001\n",
      "8/88, train_loss: 0.5002\n",
      "9/88, train_loss: 0.5000\n",
      "10/88, train_loss: 0.3132\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.0768\n",
      "14/88, train_loss: 0.2803\n",
      "15/88, train_loss: 0.3084\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.2924\n",
      "18/88, train_loss: 0.5000\n",
      "19/88, train_loss: 0.5000\n",
      "20/88, train_loss: 0.3092\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.2897\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.5000\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.2980\n",
      "30/88, train_loss: 0.2764\n",
      "31/88, train_loss: 0.2935\n",
      "32/88, train_loss: 0.5003\n",
      "33/88, train_loss: 0.2915\n",
      "34/88, train_loss: 0.5000\n",
      "35/88, train_loss: 0.2860\n",
      "36/88, train_loss: 0.5002\n",
      "37/88, train_loss: 0.5001\n",
      "38/88, train_loss: 0.2883\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.0785\n",
      "41/88, train_loss: 0.3155\n",
      "42/88, train_loss: 0.5002\n",
      "43/88, train_loss: 0.2816\n",
      "44/88, train_loss: 0.2921\n",
      "45/88, train_loss: 0.1542\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.0719\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.0760\n",
      "53/88, train_loss: 0.0565\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.0518\n",
      "56/88, train_loss: 0.3095\n",
      "57/88, train_loss: 0.5005\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.2974\n",
      "62/88, train_loss: 0.3049\n",
      "63/88, train_loss: 0.5000\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5000\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.2780\n",
      "69/88, train_loss: 0.2991\n",
      "70/88, train_loss: 0.2934\n",
      "71/88, train_loss: 0.2643\n",
      "72/88, train_loss: 0.1743\n",
      "73/88, train_loss: 0.5001\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.5000\n",
      "76/88, train_loss: 0.2965\n",
      "77/88, train_loss: 0.5001\n",
      "78/88, train_loss: 0.5002\n",
      "79/88, train_loss: 0.2753\n",
      "80/88, train_loss: 0.5003\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.3196\n",
      "83/88, train_loss: 0.5001\n",
      "84/88, train_loss: 0.3029\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.5001\n",
      "87/88, train_loss: 0.2373\n",
      "88/88, train_loss: 0.5001\n",
      "Epoch 85 average loss: 0.3923\n",
      "Validation loss: 0.4910\n",
      "Current epoch: 85, current mean dice: 0.1622, best mean dice: 0.2572 at epoch 25\n",
      "\n",
      "Epoch 86/300\n",
      "1/88, train_loss: 0.2898\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5002\n",
      "6/88, train_loss: 0.5000\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.3838\n",
      "9/88, train_loss: 0.1016\n",
      "10/88, train_loss: 0.5000\n",
      "11/88, train_loss: 0.2907\n",
      "12/88, train_loss: 0.2769\n",
      "13/88, train_loss: 0.2784\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.3000\n",
      "19/88, train_loss: 0.2765\n",
      "20/88, train_loss: 0.2938\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5001\n",
      "23/88, train_loss: 0.2839\n",
      "24/88, train_loss: 0.5001\n",
      "25/88, train_loss: 0.5001\n",
      "26/88, train_loss: 0.3159\n",
      "27/88, train_loss: 0.2985\n",
      "28/88, train_loss: 0.5001\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.2755\n",
      "32/88, train_loss: 0.3238\n",
      "33/88, train_loss: 0.2819\n",
      "34/88, train_loss: 0.2796\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.2800\n",
      "37/88, train_loss: 0.5000\n",
      "38/88, train_loss: 0.2790\n",
      "39/88, train_loss: 0.0743\n",
      "40/88, train_loss: 0.5001\n",
      "41/88, train_loss: 0.2986\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5000\n",
      "45/88, train_loss: 0.0600\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5001\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.3133\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.2627\n",
      "52/88, train_loss: 0.3074\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.5000\n",
      "55/88, train_loss: 0.2951\n",
      "56/88, train_loss: 0.2887\n",
      "57/88, train_loss: 0.5003\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.3079\n",
      "61/88, train_loss: 0.2754\n",
      "62/88, train_loss: 0.3177\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.0689\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.2730\n",
      "68/88, train_loss: 0.2710\n",
      "69/88, train_loss: 0.2975\n",
      "70/88, train_loss: 0.5000\n",
      "71/88, train_loss: 0.2750\n",
      "72/88, train_loss: 0.5005\n",
      "73/88, train_loss: 0.2775\n",
      "74/88, train_loss: 0.5000\n",
      "75/88, train_loss: 0.2850\n",
      "76/88, train_loss: 0.5000\n",
      "77/88, train_loss: 0.3025\n",
      "78/88, train_loss: 0.2726\n",
      "79/88, train_loss: 0.5002\n",
      "80/88, train_loss: 0.0464\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.2979\n",
      "83/88, train_loss: 0.2831\n",
      "84/88, train_loss: 0.5001\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.2802\n",
      "87/88, train_loss: 0.5000\n",
      "88/88, train_loss: 0.5003\n",
      "Epoch 86 average loss: 0.3880\n",
      "\n",
      "Epoch 87/300\n",
      "1/88, train_loss: 0.5000\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.5001\n",
      "4/88, train_loss: 0.5000\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.2711\n",
      "7/88, train_loss: 0.2711\n",
      "8/88, train_loss: 0.5001\n",
      "9/88, train_loss: 0.2914\n",
      "10/88, train_loss: 0.2915\n",
      "11/88, train_loss: 0.5000\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.2769\n",
      "14/88, train_loss: 0.2907\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.2713\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.5002\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.5000\n",
      "22/88, train_loss: 0.5006\n",
      "23/88, train_loss: 0.1012\n",
      "24/88, train_loss: 0.2769\n",
      "25/88, train_loss: 0.2637\n",
      "26/88, train_loss: 0.5003\n",
      "27/88, train_loss: 0.5002\n",
      "28/88, train_loss: 0.0600\n",
      "29/88, train_loss: 0.5000\n",
      "30/88, train_loss: 0.5000\n",
      "31/88, train_loss: 0.0944\n",
      "32/88, train_loss: 0.5001\n",
      "33/88, train_loss: 0.0545\n",
      "34/88, train_loss: 0.2870\n",
      "35/88, train_loss: 0.5000\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.2963\n",
      "38/88, train_loss: 0.5000\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.0476\n",
      "45/88, train_loss: 0.5000\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.2935\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.2692\n",
      "50/88, train_loss: 0.5001\n",
      "51/88, train_loss: 0.5001\n",
      "52/88, train_loss: 0.3198\n",
      "53/88, train_loss: 0.3045\n",
      "54/88, train_loss: 0.2697\n",
      "55/88, train_loss: 0.5002\n",
      "56/88, train_loss: 0.3026\n",
      "57/88, train_loss: 0.5000\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.0728\n",
      "60/88, train_loss: 0.5000\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.2974\n",
      "63/88, train_loss: 0.2757\n",
      "64/88, train_loss: 0.5001\n",
      "65/88, train_loss: 0.0590\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.2792\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.2724\n",
      "70/88, train_loss: 0.2781\n",
      "71/88, train_loss: 0.2817\n",
      "72/88, train_loss: 0.2801\n",
      "73/88, train_loss: 0.2765\n",
      "74/88, train_loss: 0.2751\n",
      "75/88, train_loss: 0.5001\n",
      "76/88, train_loss: 0.2722\n",
      "77/88, train_loss: 0.5000\n",
      "78/88, train_loss: 0.5000\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.2604\n",
      "81/88, train_loss: 0.2759\n",
      "82/88, train_loss: 0.5000\n",
      "83/88, train_loss: 0.2944\n",
      "84/88, train_loss: 0.3428\n",
      "85/88, train_loss: 0.5000\n",
      "86/88, train_loss: 0.2601\n",
      "87/88, train_loss: 0.2892\n",
      "88/88, train_loss: 0.5000\n",
      "Epoch 87 average loss: 0.3847\n",
      "\n",
      "Epoch 88/300\n",
      "1/88, train_loss: 0.5001\n",
      "2/88, train_loss: 0.5004\n",
      "3/88, train_loss: 0.5000\n",
      "4/88, train_loss: 0.0612\n",
      "5/88, train_loss: 0.0397\n",
      "6/88, train_loss: 0.5001\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.2837\n",
      "10/88, train_loss: 0.2892\n",
      "11/88, train_loss: 0.5001\n",
      "12/88, train_loss: 0.5000\n",
      "13/88, train_loss: 0.2857\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.2840\n",
      "17/88, train_loss: 0.1052\n",
      "18/88, train_loss: 0.2846\n",
      "19/88, train_loss: 0.2585\n",
      "20/88, train_loss: 0.5000\n",
      "21/88, train_loss: 0.2700\n",
      "22/88, train_loss: 0.5000\n",
      "23/88, train_loss: 0.5000\n",
      "24/88, train_loss: 0.5000\n",
      "25/88, train_loss: 0.5000\n",
      "26/88, train_loss: 0.5000\n",
      "27/88, train_loss: 0.5001\n",
      "28/88, train_loss: 0.5002\n",
      "29/88, train_loss: 0.2870\n",
      "30/88, train_loss: 0.2841\n",
      "31/88, train_loss: 0.0646\n",
      "32/88, train_loss: 0.2648\n",
      "33/88, train_loss: 0.0603\n",
      "34/88, train_loss: 0.3000\n",
      "35/88, train_loss: 0.2865\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.2965\n",
      "38/88, train_loss: 0.5001\n",
      "39/88, train_loss: 0.2993\n",
      "40/88, train_loss: 0.2731\n",
      "41/88, train_loss: 0.2772\n",
      "42/88, train_loss: 0.5000\n",
      "43/88, train_loss: 0.5001\n",
      "44/88, train_loss: 0.2673\n",
      "45/88, train_loss: 0.2951\n",
      "46/88, train_loss: 0.5000\n",
      "47/88, train_loss: 0.5000\n",
      "48/88, train_loss: 0.5000\n",
      "49/88, train_loss: 0.5000\n",
      "50/88, train_loss: 0.5000\n",
      "51/88, train_loss: 0.5000\n",
      "52/88, train_loss: 0.5000\n",
      "53/88, train_loss: 0.5000\n",
      "54/88, train_loss: 0.2964\n",
      "55/88, train_loss: 0.5001\n",
      "56/88, train_loss: 0.2780\n",
      "57/88, train_loss: 0.3162\n",
      "58/88, train_loss: 0.5000\n",
      "59/88, train_loss: 0.5000\n",
      "60/88, train_loss: 0.5001\n",
      "61/88, train_loss: 0.5000\n",
      "62/88, train_loss: 0.5000\n",
      "63/88, train_loss: 0.5001\n",
      "64/88, train_loss: 0.5000\n",
      "65/88, train_loss: 0.5001\n",
      "66/88, train_loss: 0.5000\n",
      "67/88, train_loss: 0.5000\n",
      "68/88, train_loss: 0.5000\n",
      "69/88, train_loss: 0.2962\n",
      "70/88, train_loss: 0.5006\n",
      "71/88, train_loss: 0.5000\n",
      "72/88, train_loss: 0.5000\n",
      "73/88, train_loss: 0.0533\n",
      "74/88, train_loss: 0.2821\n",
      "75/88, train_loss: 0.2617\n",
      "76/88, train_loss: 0.2720\n",
      "77/88, train_loss: 0.2865\n",
      "78/88, train_loss: 0.3044\n",
      "79/88, train_loss: 0.5000\n",
      "80/88, train_loss: 0.3116\n",
      "81/88, train_loss: 0.5000\n",
      "82/88, train_loss: 0.2864\n",
      "83/88, train_loss: 0.5000\n",
      "84/88, train_loss: 0.2785\n",
      "85/88, train_loss: 0.0552\n",
      "88/88, train_loss: 0.0399\n",
      "Epoch 88 average loss: 0.3841\n",
      "\n",
      "Epoch 89/300\n",
      "1/88, train_loss: 0.2786\n",
      "2/88, train_loss: 0.5000\n",
      "3/88, train_loss: 0.2784\n",
      "4/88, train_loss: 0.5002\n",
      "5/88, train_loss: 0.5001\n",
      "6/88, train_loss: 0.2803\n",
      "7/88, train_loss: 0.5000\n",
      "8/88, train_loss: 0.5000\n",
      "9/88, train_loss: 0.5007\n",
      "10/88, train_loss: 0.2711\n",
      "11/88, train_loss: 0.2605\n",
      "12/88, train_loss: 0.3065\n",
      "13/88, train_loss: 0.2689\n",
      "14/88, train_loss: 0.5000\n",
      "15/88, train_loss: 0.5000\n",
      "16/88, train_loss: 0.5000\n",
      "17/88, train_loss: 0.5000\n",
      "18/88, train_loss: 0.5001\n",
      "19/88, train_loss: 0.0892\n",
      "20/88, train_loss: 0.2907\n",
      "21/88, train_loss: 0.5001\n",
      "22/88, train_loss: 0.2721\n",
      "23/88, train_loss: 0.2590\n",
      "24/88, train_loss: 0.2579\n",
      "25/88, train_loss: 0.2791\n",
      "26/88, train_loss: 0.2676\n",
      "27/88, train_loss: 0.5001\n",
      "28/88, train_loss: 0.2640\n",
      "29/88, train_loss: 0.5001\n",
      "30/88, train_loss: 0.2697\n",
      "31/88, train_loss: 0.5001\n",
      "32/88, train_loss: 0.5000\n",
      "33/88, train_loss: 0.3009\n",
      "34/88, train_loss: 0.2708\n",
      "35/88, train_loss: 0.2755\n",
      "36/88, train_loss: 0.5000\n",
      "37/88, train_loss: 0.2764\n",
      "38/88, train_loss: 0.2830\n",
      "39/88, train_loss: 0.5000\n",
      "40/88, train_loss: 0.5000\n",
      "41/88, train_loss: 0.5000\n",
      "42/88, train_loss: 0.5002\n",
      "43/88, train_loss: 0.5000\n",
      "44/88, train_loss: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import monai\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.data import (\n",
    "    decollate_batch,\n",
    "    CacheDataset,\n",
    "    list_data_collate,\n",
    ")\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ConcatItemsd,\n",
    "    SaveImaged,\n",
    "    Invertd,\n",
    "    SpatialPadd\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.utils import one_hot\n",
    "\n",
    "class HardnessWeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-5, hardness_lambda=0.6, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "        self.hardness_lambda = hardness_lambda\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        probs = F.softmax(input, dim=1)\n",
    "        target_oh = one_hot(target, num_classes=self.num_classes)\n",
    "\n",
    "        hardness_weight = self.hardness_lambda * torch.abs(probs - target_oh) + (1.0 - self.hardness_lambda)\n",
    "\n",
    "        intersection = (hardness_weight * probs * target_oh).sum(dim=(2, 3, 4))\n",
    "        union = (hardness_weight * (probs + target_oh)).sum(dim=(2, 3, 4))\n",
    "\n",
    "        dice_score = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        dice_loss = 1.0 - dice_score.mean()\n",
    "\n",
    "        return dice_loss\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set paths for your data\n",
    "# Set paths for your data\n",
    "t1_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/t1_masks\"\n",
    "t2_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/t2_masks\"\n",
    "labels_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/label_masks\"\n",
    "output_dir = \"./output\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create data dictionaries for MONAI\n",
    "def create_data_dicts(t1_dir, t2_dir, labels_dir):\n",
    "    t1_files = sorted([os.path.join(t1_dir, f) for f in os.listdir(t1_dir) if f.endswith('.nii.gz')])\n",
    "    t2_files = sorted([os.path.join(t2_dir, f) for f in os.listdir(t2_dir) if f.endswith('.nii.gz')])\n",
    "    label_files = sorted([os.path.join(labels_dir, f) for f in os.listdir(labels_dir) if f.endswith('.nii.gz')])\n",
    "    \n",
    "    # Ensure consistent file counts\n",
    "    assert len(t1_files) == len(t2_files) == len(label_files), \"Mismatch in file counts\"\n",
    "    \n",
    "    data_dicts = [\n",
    "        {\n",
    "            \"t1\": t1_file,\n",
    "            \"t2\": t2_file,\n",
    "            \"label\": label_file\n",
    "        }\n",
    "        for t1_file, t2_file, label_file in zip(t1_files, t2_files, label_files)\n",
    "    ]\n",
    "    return data_dicts\n",
    "\n",
    "def create_data_streams(base_folder,prefix):\n",
    "  test_folders = [prefix+\"_\"+f for f in ['t1_masks','t2_masks','label_masks']]\n",
    "  test_paths = [os.path.join(base_folder, f) for f in test_folders]\n",
    "  test_dict = create_data_dicts(test_paths[0],test_paths[1],test_paths[2])\n",
    "  return test_dict\n",
    "\n",
    "def create_data_split(base_folder):\n",
    "  train_dict = create_data_streams(base_folder,'train')\n",
    "  val_dict = create_data_streams(base_folder,'val')\n",
    "  test_dict = create_data_streams(base_folder,'test')\n",
    "  return train_dict,val_dict,test_dict\n",
    "\n",
    "# Define transforms for NIfTI files\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t1\", \"t2\", \"label\"]),  # Load NIfTI files\n",
    "    EnsureChannelFirstd(keys=[\"t1\", \"t2\", \"label\"]),\n",
    "    Orientationd(keys=[\"t1\", \"t2\", \"label\"], axcodes=\"RAS\"),  # Standardize orientation\n",
    "    Spacingd(keys=[\"t1\", \"t2\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\", \"nearest\")),\n",
    "    SpatialPadd(keys=[\"t1\", \"t2\", \"label\"], spatial_size=(96, 96, 96), mode=(\"constant\", \"constant\", \"constant\")),\n",
    "    ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "    ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),  # Concatenate T1 and T2 along channel dimension\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t1\", \"t2\", \"label\"]),  # Load NIfTI files\n",
    "    EnsureChannelFirstd(keys=[\"t1\", \"t2\", \"label\"]),\n",
    "    Orientationd(keys=[\"t1\", \"t2\", \"label\"], axcodes=\"RAS\"),  # Standardize orientation\n",
    "    Spacingd(keys=[\"t1\", \"t2\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\", \"nearest\")),\n",
    "    SpatialPadd(keys=[\"t1\", \"t2\", \"label\"], spatial_size=(96, 96, 96), mode=(\"constant\", \"constant\", \"constant\")),\n",
    "    ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "    ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),  # Concatenate T1 and T2 along channel dimension\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Split dataset into training and validation\n",
    "def train_val_split(data_dicts, val_ratio=0.2):\n",
    "    n_val = int(len(data_dicts) * val_ratio)\n",
    "    n_train = len(data_dicts) - n_val\n",
    "    \n",
    "    train_files = data_dicts[:n_train]\n",
    "    val_files = data_dicts[n_train:]\n",
    "    \n",
    "    return train_files, val_files\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "base_folder = \"/projectnb/cs585bp/students/econlin/VS_Seg-master\"\n",
    "train_files,val_files,test_files = create_data_split(base_folder)\n",
    "\n",
    "# Using CacheDataset for better performance\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=list_data_collate)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4, collate_fn=list_data_collate)\n",
    "\n",
    "# Define model\n",
    "# Assuming you have n classes (including background)\n",
    "num_classes = 2  # Change to the number of segmentation classes + background\n",
    "\n",
    "# Create Swin-UNETR model (with 2 input channels for T1 and T2 concatenated)\n",
    "model = SwinUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=2,  # 2 channels for T1 and T2\n",
    "    out_channels=num_classes,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function, optimizer, and metrics\n",
    "loss_function = HardnessWeightedDiceLoss(num_classes=num_classes)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "# Post-processing transforms\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=num_classes)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=num_classes)])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "val_interval = 5\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "writer = SummaryWriter(os.path.join(output_dir, 'logs'))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    val_epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_loader)}, train_loss: {loss.item():.4f}\")\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    print(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    writer.add_scalar(\"train_loss\", epoch_loss, epoch + 1)\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            metric_sum = 0.0\n",
    "            metric_count = 0\n",
    "            \n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "\n",
    "                # Calculate validation loss - ADD THIS\n",
    "                val_loss = loss_function(val_outputs, val_labels).item()\n",
    "                val_epoch_loss += val_loss  # Accumulate validation loss\n",
    "                \n",
    "                # Compute metric\n",
    "                val_outputs_list = decollate_batch(val_outputs)\n",
    "                val_labels_list = decollate_batch(val_labels)\n",
    "                \n",
    "                val_outputs_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "                val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "                \n",
    "                dice_metric(y_pred=val_outputs_convert, y=val_labels_convert)\n",
    "\n",
    "            # Calculate average validation loss - ADD THIS\n",
    "            val_epoch_loss /= len(val_loader)\n",
    "            writer.add_scalar(\"val_loss\", val_epoch_loss, epoch + 1)\n",
    "        \n",
    "            # Log validation loss - ADD THIS\n",
    "            print(f\"Validation loss: {val_epoch_loss:.4f}\")\n",
    "        \n",
    "            metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "\n",
    "\n",
    "            \n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n",
    "                print(\"Saved new best model\")\n",
    "                \n",
    "            print(f\"Current epoch: {epoch + 1}, current mean dice: {metric:.4f}, best mean dice: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "            writer.add_scalar(\"val_mean_dice\", metric, epoch + 1)\n",
    "            \n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()\n",
    "\n",
    "# Function to infer and save predictions as NIfTI files\n",
    "def infer_and_save(model, data_dicts, output_dir):\n",
    "    # Define inference transforms\n",
    "    infer_transforms = Compose([\n",
    "        LoadImaged(keys=[\"t1\", \"t2\"]),\n",
    "        EnsureChannelFirstd(keys=[\"t1\", \"t2\"]),\n",
    "        Orientationd(keys=[\"t1\", \"t2\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"t1\", \"t2\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\")),\n",
    "        ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "        ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),\n",
    "        EnsureTyped(keys=[\"image\"]),\n",
    "    ])\n",
    "    \n",
    "    # Create a new dataset for inference (without labels)\n",
    "    infer_ds = CacheDataset(data=data_dicts, transform=infer_transforms, cache_rate=1.0, num_workers=4)\n",
    "    infer_loader = DataLoader(infer_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate)\n",
    "    \n",
    "    # Create output directory for predictions\n",
    "    pred_dir = os.path.join(output_dir, \"predictions\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    \n",
    "    # Post-transform to convert predictions back to NIfTI\n",
    "    post_transforms = Compose([\n",
    "        EnsureTyped(keys=\"pred\"),\n",
    "        AsDiscrete(argmax=True, to_onehot=num_classes),\n",
    "        # Add transforms to map predictions back to original space if needed\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"t1_meta_dict\", output_dir=pred_dir, output_postfix=\"seg\", resample=False, output_dtype=None),\n",
    "    ])\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(infer_loader):\n",
    "            print(f\"Processing case {i+1}/{len(infer_loader)}\")\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            \n",
    "            # Get the filename for saving\n",
    "            t1_path = batch_data[\"t1_meta_dict\"][\"filename_or_obj\"][0]\n",
    "            base_name = os.path.basename(t1_path)\n",
    "            \n",
    "            # Run inference\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Process outputs and save as NIfTI\n",
    "            outputs = outputs.argmax(dim=1, keepdim=True)\n",
    "            batch_data[\"pred\"] = outputs\n",
    "            \n",
    "            # Save the prediction as NIfTI\n",
    "            post_transforms(batch_data)\n",
    "            \n",
    "            print(f\"Saved prediction for {base_name}\")\n",
    "\n",
    "# Load the best model and run inference\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n",
    "infer_and_save(model, val_files, output_dir)\n",
    "print(f\"Inference completed. Predictions saved to {os.path.join(output_dir, 'predictions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4bfd26-7ae1-4b22-b746-d97d8bcebcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|| 37/37 [00:02<00:00, 16.47it/s]\n",
      "Loading dataset: 100%|| 9/9 [00:00<00:00, 15.42it/s]\n",
      "/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/300\n",
      "1/19, train_loss: 1.4254\n",
      "2/19, train_loss: 1.2029\n",
      "3/19, train_loss: 1.0576\n",
      "4/19, train_loss: 0.9663\n",
      "5/19, train_loss: 0.9142\n",
      "6/19, train_loss: 0.8803\n",
      "7/19, train_loss: 0.8563\n",
      "8/19, train_loss: 0.8355\n",
      "9/19, train_loss: 0.8183\n",
      "10/19, train_loss: 0.8035\n",
      "11/19, train_loss: 0.7959\n",
      "12/19, train_loss: 0.7838\n",
      "13/19, train_loss: 0.7773\n",
      "14/19, train_loss: 0.7704\n",
      "15/19, train_loss: 0.7613\n",
      "16/19, train_loss: 0.7576\n",
      "17/19, train_loss: 0.7521\n",
      "18/19, train_loss: 0.7455\n",
      "19/19, train_loss: 0.7407\n",
      "Epoch 1 average loss: 0.8760\n",
      "\n",
      "Epoch 2/300\n",
      "1/19, train_loss: 0.7394\n",
      "2/19, train_loss: 0.7349\n",
      "3/19, train_loss: 0.7296\n",
      "4/19, train_loss: 0.7267\n",
      "5/19, train_loss: 0.7250\n",
      "6/19, train_loss: 0.7221\n",
      "7/19, train_loss: 0.7191\n",
      "8/19, train_loss: 0.7169\n",
      "9/19, train_loss: 0.7132\n",
      "10/19, train_loss: 0.7111\n",
      "11/19, train_loss: 0.7099\n",
      "12/19, train_loss: 0.7079\n",
      "13/19, train_loss: 0.7055\n",
      "14/19, train_loss: 0.7039\n",
      "15/19, train_loss: 0.7010\n",
      "16/19, train_loss: 0.7005\n",
      "17/19, train_loss: 0.6993\n",
      "18/19, train_loss: 0.6976\n",
      "19/19, train_loss: 0.6957\n",
      "Epoch 2 average loss: 0.7136\n",
      "\n",
      "Epoch 3/300\n",
      "1/19, train_loss: 0.6953\n",
      "2/19, train_loss: 0.6944\n",
      "3/19, train_loss: 0.6917\n",
      "4/19, train_loss: 0.6905\n",
      "5/19, train_loss: 0.6896\n",
      "6/19, train_loss: 0.6881\n",
      "7/19, train_loss: 0.6862\n",
      "8/19, train_loss: 0.6860\n",
      "9/19, train_loss: 0.6849\n",
      "10/19, train_loss: 0.6835\n",
      "11/19, train_loss: 0.6819\n",
      "12/19, train_loss: 0.6812\n",
      "13/19, train_loss: 0.6801\n",
      "14/19, train_loss: 0.6797\n",
      "15/19, train_loss: 0.6790\n",
      "16/19, train_loss: 0.6775\n",
      "17/19, train_loss: 0.6757\n",
      "18/19, train_loss: 0.6761\n",
      "19/19, train_loss: 0.6746\n",
      "Epoch 3 average loss: 0.6840\n",
      "\n",
      "Epoch 4/300\n",
      "1/19, train_loss: 0.6743\n",
      "2/19, train_loss: 0.6732\n",
      "3/19, train_loss: 0.6733\n",
      "4/19, train_loss: 0.6711\n",
      "5/19, train_loss: 0.6705\n",
      "6/19, train_loss: 0.6701\n",
      "7/19, train_loss: 0.6687\n",
      "8/19, train_loss: 0.6681\n",
      "9/19, train_loss: 0.6673\n",
      "10/19, train_loss: 0.6665\n",
      "11/19, train_loss: 0.6654\n",
      "12/19, train_loss: 0.6648\n",
      "13/19, train_loss: 0.6637\n",
      "14/19, train_loss: 0.6634\n",
      "15/19, train_loss: 0.6624\n",
      "16/19, train_loss: 0.6618\n",
      "17/19, train_loss: 0.6609\n",
      "18/19, train_loss: 0.6595\n",
      "19/19, train_loss: 0.6603\n",
      "Epoch 4 average loss: 0.6666\n",
      "\n",
      "Epoch 5/300\n",
      "1/19, train_loss: 0.6590\n",
      "2/19, train_loss: 0.6588\n",
      "3/19, train_loss: 0.6576\n",
      "4/19, train_loss: 0.6574\n",
      "5/19, train_loss: 0.6565\n",
      "6/19, train_loss: 0.6553\n",
      "7/19, train_loss: 0.6549\n",
      "8/19, train_loss: 0.6541\n",
      "9/19, train_loss: 0.6543\n",
      "10/19, train_loss: 0.6530\n",
      "11/19, train_loss: 0.6520\n",
      "12/19, train_loss: 0.6514\n",
      "13/19, train_loss: 0.6508\n",
      "14/19, train_loss: 0.6503\n",
      "15/19, train_loss: 0.6497\n",
      "16/19, train_loss: 0.6489\n",
      "17/19, train_loss: 0.6489\n",
      "18/19, train_loss: 0.6473\n",
      "19/19, train_loss: 0.6471\n",
      "Epoch 5 average loss: 0.6530\n",
      "Saved new best model\n",
      "Current epoch: 5, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 6/300\n",
      "1/19, train_loss: 0.6464\n",
      "2/19, train_loss: 0.6457\n",
      "3/19, train_loss: 0.6446\n",
      "4/19, train_loss: 0.6444\n",
      "5/19, train_loss: 0.6439\n",
      "6/19, train_loss: 0.6433\n",
      "7/19, train_loss: 0.6429\n",
      "8/19, train_loss: 0.6422\n",
      "9/19, train_loss: 0.6417\n",
      "10/19, train_loss: 0.6413\n",
      "11/19, train_loss: 0.6402\n",
      "12/19, train_loss: 0.6400\n",
      "13/19, train_loss: 0.6390\n",
      "14/19, train_loss: 0.6389\n",
      "15/19, train_loss: 0.6378\n",
      "16/19, train_loss: 0.6375\n",
      "17/19, train_loss: 0.6371\n",
      "18/19, train_loss: 0.6359\n",
      "19/19, train_loss: 0.6367\n",
      "Epoch 6 average loss: 0.6410\n",
      "\n",
      "Epoch 7/300\n",
      "1/19, train_loss: 0.6351\n",
      "2/19, train_loss: 0.6343\n",
      "3/19, train_loss: 0.6337\n",
      "4/19, train_loss: 0.6329\n",
      "5/19, train_loss: 0.6330\n",
      "6/19, train_loss: 0.6322\n",
      "7/19, train_loss: 0.6320\n",
      "8/19, train_loss: 0.6308\n",
      "9/19, train_loss: 0.6298\n",
      "10/19, train_loss: 0.6296\n",
      "11/19, train_loss: 0.6293\n",
      "12/19, train_loss: 0.6286\n",
      "13/19, train_loss: 0.6287\n",
      "14/19, train_loss: 0.6273\n",
      "15/19, train_loss: 0.6270\n",
      "16/19, train_loss: 0.6265\n",
      "17/19, train_loss: 0.6263\n",
      "18/19, train_loss: 0.6253\n",
      "19/19, train_loss: 0.6248\n",
      "Epoch 7 average loss: 0.6299\n",
      "\n",
      "Epoch 8/300\n",
      "1/19, train_loss: 0.6241\n",
      "2/19, train_loss: 0.6236\n",
      "3/19, train_loss: 0.6235\n",
      "4/19, train_loss: 0.6229\n",
      "5/19, train_loss: 0.6220\n",
      "6/19, train_loss: 0.6216\n",
      "7/19, train_loss: 0.6216\n",
      "8/19, train_loss: 0.6206\n",
      "9/19, train_loss: 0.6197\n",
      "10/19, train_loss: 0.6196\n",
      "11/19, train_loss: 0.6193\n",
      "12/19, train_loss: 0.6186\n",
      "13/19, train_loss: 0.6186\n",
      "14/19, train_loss: 0.6178\n",
      "15/19, train_loss: 0.6174\n",
      "16/19, train_loss: 0.6170\n",
      "17/19, train_loss: 0.6161\n",
      "18/19, train_loss: 0.6157\n",
      "19/19, train_loss: 0.6153\n",
      "Epoch 8 average loss: 0.6197\n",
      "\n",
      "Epoch 9/300\n",
      "1/19, train_loss: 0.6146\n",
      "2/19, train_loss: 0.6141\n",
      "3/19, train_loss: 0.6141\n",
      "4/19, train_loss: 0.6134\n",
      "5/19, train_loss: 0.6127\n",
      "6/19, train_loss: 0.6122\n",
      "7/19, train_loss: 0.6123\n",
      "8/19, train_loss: 0.6117\n",
      "9/19, train_loss: 0.6109\n",
      "10/19, train_loss: 0.6107\n",
      "11/19, train_loss: 0.6100\n",
      "12/19, train_loss: 0.6100\n",
      "13/19, train_loss: 0.6092\n",
      "14/19, train_loss: 0.6085\n",
      "15/19, train_loss: 0.6085\n",
      "16/19, train_loss: 0.6079\n",
      "17/19, train_loss: 0.6075\n",
      "18/19, train_loss: 0.6069\n",
      "19/19, train_loss: 0.6069\n",
      "Epoch 9 average loss: 0.6106\n",
      "\n",
      "Epoch 10/300\n",
      "1/19, train_loss: 0.6062\n",
      "2/19, train_loss: 0.6057\n",
      "3/19, train_loss: 0.6053\n",
      "4/19, train_loss: 0.6051\n",
      "5/19, train_loss: 0.6045\n",
      "6/19, train_loss: 0.6041\n",
      "7/19, train_loss: 0.6040\n",
      "8/19, train_loss: 0.6035\n",
      "9/19, train_loss: 0.6032\n",
      "10/19, train_loss: 0.6026\n",
      "11/19, train_loss: 0.6026\n",
      "12/19, train_loss: 0.6019\n",
      "13/19, train_loss: 0.6015\n",
      "14/19, train_loss: 0.6011\n",
      "15/19, train_loss: 0.6007\n",
      "16/19, train_loss: 0.6003\n",
      "17/19, train_loss: 0.5997\n",
      "18/19, train_loss: 0.5997\n",
      "19/19, train_loss: 0.6003\n",
      "Epoch 10 average loss: 0.6027\n",
      "Current epoch: 10, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 11/300\n",
      "1/19, train_loss: 0.5990\n",
      "2/19, train_loss: 0.5985\n",
      "3/19, train_loss: 0.5982\n",
      "4/19, train_loss: 0.5980\n",
      "5/19, train_loss: 0.5975\n",
      "6/19, train_loss: 0.5975\n",
      "7/19, train_loss: 0.5969\n",
      "8/19, train_loss: 0.5964\n",
      "9/19, train_loss: 0.5965\n",
      "10/19, train_loss: 0.5959\n",
      "11/19, train_loss: 0.5956\n",
      "12/19, train_loss: 0.5952\n",
      "13/19, train_loss: 0.5949\n",
      "14/19, train_loss: 0.5945\n",
      "15/19, train_loss: 0.5945\n",
      "16/19, train_loss: 0.5940\n",
      "17/19, train_loss: 0.5932\n",
      "18/19, train_loss: 0.5927\n",
      "19/19, train_loss: 0.5927\n",
      "Epoch 11 average loss: 0.5959\n",
      "\n",
      "Epoch 12/300\n",
      "1/19, train_loss: 0.5916\n",
      "2/19, train_loss: 0.5912\n",
      "3/19, train_loss: 0.5907\n",
      "4/19, train_loss: 0.5902\n",
      "5/19, train_loss: 0.5901\n",
      "6/19, train_loss: 0.5890\n",
      "7/19, train_loss: 0.5885\n",
      "8/19, train_loss: 0.5878\n",
      "9/19, train_loss: 0.5871\n",
      "10/19, train_loss: 0.5869\n",
      "11/19, train_loss: 0.5867\n",
      "12/19, train_loss: 0.5855\n",
      "13/19, train_loss: 0.5852\n",
      "14/19, train_loss: 0.5843\n",
      "15/19, train_loss: 0.5837\n",
      "16/19, train_loss: 0.5828\n",
      "17/19, train_loss: 0.5821\n",
      "18/19, train_loss: 0.5814\n",
      "19/19, train_loss: 0.5801\n",
      "Epoch 12 average loss: 0.5866\n",
      "\n",
      "Epoch 13/300\n",
      "1/19, train_loss: 0.5795\n",
      "2/19, train_loss: 0.5789\n",
      "3/19, train_loss: 0.5778\n",
      "4/19, train_loss: 0.5774\n",
      "5/19, train_loss: 0.5767\n",
      "6/19, train_loss: 0.5756\n",
      "7/19, train_loss: 0.5749\n",
      "8/19, train_loss: 0.5746\n",
      "9/19, train_loss: 0.5737\n",
      "10/19, train_loss: 0.5739\n",
      "11/19, train_loss: 0.5725\n",
      "12/19, train_loss: 0.5726\n",
      "13/19, train_loss: 0.5718\n",
      "14/19, train_loss: 0.5717\n",
      "15/19, train_loss: 0.5711\n",
      "16/19, train_loss: 0.5706\n",
      "17/19, train_loss: 0.5706\n",
      "18/19, train_loss: 0.5702\n",
      "19/19, train_loss: 0.5698\n",
      "Epoch 13 average loss: 0.5739\n",
      "\n",
      "Epoch 14/300\n",
      "1/19, train_loss: 0.5693\n",
      "2/19, train_loss: 0.5689\n",
      "3/19, train_loss: 0.5687\n",
      "4/19, train_loss: 0.5684\n",
      "5/19, train_loss: 0.5681\n",
      "6/19, train_loss: 0.5680\n",
      "7/19, train_loss: 0.5676\n",
      "8/19, train_loss: 0.5672\n",
      "9/19, train_loss: 0.5670\n",
      "10/19, train_loss: 0.5666\n",
      "11/19, train_loss: 0.5666\n",
      "12/19, train_loss: 0.5661\n",
      "13/19, train_loss: 0.5659\n",
      "14/19, train_loss: 0.5663\n",
      "15/19, train_loss: 0.5654\n",
      "16/19, train_loss: 0.5655\n",
      "17/19, train_loss: 0.5649\n",
      "18/19, train_loss: 0.5649\n",
      "19/19, train_loss: 0.5644\n",
      "Epoch 14 average loss: 0.5668\n",
      "\n",
      "Epoch 15/300\n",
      "1/19, train_loss: 0.5643\n",
      "2/19, train_loss: 0.5641\n",
      "3/19, train_loss: 0.5638\n",
      "4/19, train_loss: 0.5638\n",
      "5/19, train_loss: 0.5635\n",
      "6/19, train_loss: 0.5634\n",
      "7/19, train_loss: 0.5631\n",
      "8/19, train_loss: 0.5627\n",
      "9/19, train_loss: 0.5626\n",
      "10/19, train_loss: 0.5627\n",
      "11/19, train_loss: 0.5622\n",
      "12/19, train_loss: 0.5624\n",
      "13/19, train_loss: 0.5619\n",
      "14/19, train_loss: 0.5616\n",
      "15/19, train_loss: 0.5614\n",
      "16/19, train_loss: 0.5612\n",
      "17/19, train_loss: 0.5613\n",
      "18/19, train_loss: 0.5608\n",
      "19/19, train_loss: 0.5613\n",
      "Epoch 15 average loss: 0.5625\n",
      "Current epoch: 15, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 16/300\n",
      "1/19, train_loss: 0.5605\n",
      "2/19, train_loss: 0.5603\n",
      "3/19, train_loss: 0.5602\n",
      "4/19, train_loss: 0.5600\n",
      "5/19, train_loss: 0.5600\n",
      "6/19, train_loss: 0.5598\n",
      "7/19, train_loss: 0.5593\n",
      "8/19, train_loss: 0.5595\n",
      "9/19, train_loss: 0.5591\n",
      "10/19, train_loss: 0.5590\n",
      "11/19, train_loss: 0.5587\n",
      "12/19, train_loss: 0.5588\n",
      "13/19, train_loss: 0.5583\n",
      "14/19, train_loss: 0.5583\n",
      "15/19, train_loss: 0.5581\n",
      "16/19, train_loss: 0.5579\n",
      "17/19, train_loss: 0.5578\n",
      "18/19, train_loss: 0.5576\n",
      "19/19, train_loss: 0.5581\n",
      "Epoch 16 average loss: 0.5590\n",
      "\n",
      "Epoch 17/300\n",
      "1/19, train_loss: 0.5573\n",
      "2/19, train_loss: 0.5575\n",
      "3/19, train_loss: 0.5568\n",
      "4/19, train_loss: 0.5568\n",
      "5/19, train_loss: 0.5566\n",
      "6/19, train_loss: 0.5565\n",
      "7/19, train_loss: 0.5563\n",
      "8/19, train_loss: 0.5561\n",
      "9/19, train_loss: 0.5561\n",
      "10/19, train_loss: 0.5559\n",
      "11/19, train_loss: 0.5557\n",
      "12/19, train_loss: 0.5555\n",
      "13/19, train_loss: 0.5557\n",
      "14/19, train_loss: 0.5554\n",
      "15/19, train_loss: 0.5553\n",
      "16/19, train_loss: 0.5550\n",
      "17/19, train_loss: 0.5548\n",
      "18/19, train_loss: 0.5547\n",
      "19/19, train_loss: 0.5545\n",
      "Epoch 17 average loss: 0.5559\n",
      "\n",
      "Epoch 18/300\n",
      "1/19, train_loss: 0.5544\n",
      "2/19, train_loss: 0.5543\n",
      "3/19, train_loss: 0.5540\n",
      "4/19, train_loss: 0.5539\n",
      "5/19, train_loss: 0.5539\n",
      "6/19, train_loss: 0.5538\n",
      "7/19, train_loss: 0.5535\n",
      "8/19, train_loss: 0.5533\n",
      "9/19, train_loss: 0.5532\n",
      "10/19, train_loss: 0.5531\n",
      "11/19, train_loss: 0.5529\n",
      "12/19, train_loss: 0.5528\n",
      "13/19, train_loss: 0.5526\n",
      "14/19, train_loss: 0.5528\n",
      "15/19, train_loss: 0.5524\n",
      "16/19, train_loss: 0.5521\n",
      "17/19, train_loss: 0.5521\n",
      "18/19, train_loss: 0.5519\n",
      "19/19, train_loss: 0.5522\n",
      "Epoch 18 average loss: 0.5531\n",
      "\n",
      "Epoch 19/300\n",
      "1/19, train_loss: 0.5517\n",
      "2/19, train_loss: 0.5517\n",
      "3/19, train_loss: 0.5515\n",
      "4/19, train_loss: 0.5512\n",
      "5/19, train_loss: 0.5511\n",
      "6/19, train_loss: 0.5510\n",
      "7/19, train_loss: 0.5508\n",
      "8/19, train_loss: 0.5507\n",
      "9/19, train_loss: 0.5507\n",
      "10/19, train_loss: 0.5505\n",
      "11/19, train_loss: 0.5503\n",
      "12/19, train_loss: 0.5501\n",
      "13/19, train_loss: 0.5505\n",
      "14/19, train_loss: 0.5500\n",
      "15/19, train_loss: 0.5498\n",
      "16/19, train_loss: 0.5498\n",
      "17/19, train_loss: 0.5495\n",
      "18/19, train_loss: 0.5494\n",
      "19/19, train_loss: 0.5493\n",
      "Epoch 19 average loss: 0.5505\n",
      "\n",
      "Epoch 20/300\n",
      "1/19, train_loss: 0.5492\n",
      "2/19, train_loss: 0.5491\n",
      "3/19, train_loss: 0.5488\n",
      "4/19, train_loss: 0.5487\n",
      "5/19, train_loss: 0.5485\n",
      "6/19, train_loss: 0.5484\n",
      "7/19, train_loss: 0.5486\n",
      "8/19, train_loss: 0.5481\n",
      "9/19, train_loss: 0.5481\n",
      "10/19, train_loss: 0.5480\n",
      "11/19, train_loss: 0.5477\n",
      "12/19, train_loss: 0.5478\n",
      "13/19, train_loss: 0.5475\n",
      "14/19, train_loss: 0.5474\n",
      "15/19, train_loss: 0.5473\n",
      "16/19, train_loss: 0.5471\n",
      "17/19, train_loss: 0.5470\n",
      "18/19, train_loss: 0.5468\n",
      "19/19, train_loss: 0.5467\n",
      "Epoch 20 average loss: 0.5479\n",
      "Current epoch: 20, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 21/300\n",
      "1/19, train_loss: 0.5466\n",
      "2/19, train_loss: 0.5464\n",
      "3/19, train_loss: 0.5463\n",
      "4/19, train_loss: 0.5461\n",
      "5/19, train_loss: 0.5462\n",
      "6/19, train_loss: 0.5462\n",
      "7/19, train_loss: 0.5459\n",
      "8/19, train_loss: 0.5457\n",
      "9/19, train_loss: 0.5455\n",
      "10/19, train_loss: 0.5454\n",
      "11/19, train_loss: 0.5452\n",
      "12/19, train_loss: 0.5454\n",
      "13/19, train_loss: 0.5451\n",
      "14/19, train_loss: 0.5451\n",
      "15/19, train_loss: 0.5448\n",
      "16/19, train_loss: 0.5447\n",
      "17/19, train_loss: 0.5448\n",
      "18/19, train_loss: 0.5445\n",
      "19/19, train_loss: 0.5443\n",
      "Epoch 21 average loss: 0.5455\n",
      "\n",
      "Epoch 22/300\n",
      "1/19, train_loss: 0.5443\n",
      "2/19, train_loss: 0.5443\n",
      "3/19, train_loss: 0.5440\n",
      "4/19, train_loss: 0.5440\n",
      "5/19, train_loss: 0.5437\n",
      "6/19, train_loss: 0.5436\n",
      "7/19, train_loss: 0.5435\n",
      "8/19, train_loss: 0.5435\n",
      "9/19, train_loss: 0.5435\n",
      "10/19, train_loss: 0.5433\n",
      "11/19, train_loss: 0.5430\n",
      "12/19, train_loss: 0.5430\n",
      "13/19, train_loss: 0.5428\n",
      "14/19, train_loss: 0.5428\n",
      "15/19, train_loss: 0.5426\n",
      "16/19, train_loss: 0.5424\n",
      "17/19, train_loss: 0.5424\n",
      "18/19, train_loss: 0.5425\n",
      "19/19, train_loss: 0.5422\n",
      "Epoch 22 average loss: 0.5432\n",
      "\n",
      "Epoch 23/300\n",
      "1/19, train_loss: 0.5420\n",
      "2/19, train_loss: 0.5419\n",
      "3/19, train_loss: 0.5418\n",
      "4/19, train_loss: 0.5418\n",
      "5/19, train_loss: 0.5416\n",
      "6/19, train_loss: 0.5417\n",
      "7/19, train_loss: 0.5415\n",
      "8/19, train_loss: 0.5412\n",
      "9/19, train_loss: 0.5412\n",
      "10/19, train_loss: 0.5411\n",
      "11/19, train_loss: 0.5410\n",
      "12/19, train_loss: 0.5409\n",
      "13/19, train_loss: 0.5409\n",
      "14/19, train_loss: 0.5408\n",
      "15/19, train_loss: 0.5407\n",
      "16/19, train_loss: 0.5405\n",
      "17/19, train_loss: 0.5404\n",
      "18/19, train_loss: 0.5402\n",
      "19/19, train_loss: 0.5402\n",
      "Epoch 23 average loss: 0.5411\n",
      "\n",
      "Epoch 24/300\n",
      "1/19, train_loss: 0.5401\n",
      "2/19, train_loss: 0.5400\n",
      "3/19, train_loss: 0.5399\n",
      "4/19, train_loss: 0.5398\n",
      "5/19, train_loss: 0.5397\n",
      "6/19, train_loss: 0.5396\n",
      "7/19, train_loss: 0.5395\n",
      "8/19, train_loss: 0.5395\n",
      "9/19, train_loss: 0.5393\n",
      "10/19, train_loss: 0.5393\n",
      "11/19, train_loss: 0.5391\n",
      "12/19, train_loss: 0.5390\n",
      "13/19, train_loss: 0.5391\n",
      "14/19, train_loss: 0.5388\n",
      "15/19, train_loss: 0.5387\n",
      "16/19, train_loss: 0.5387\n",
      "17/19, train_loss: 0.5386\n",
      "18/19, train_loss: 0.5384\n",
      "19/19, train_loss: 0.5383\n",
      "Epoch 24 average loss: 0.5392\n",
      "\n",
      "Epoch 25/300\n",
      "1/19, train_loss: 0.5382\n",
      "2/19, train_loss: 0.5382\n",
      "3/19, train_loss: 0.5381\n",
      "4/19, train_loss: 0.5380\n",
      "5/19, train_loss: 0.5378\n",
      "6/19, train_loss: 0.5377\n",
      "7/19, train_loss: 0.5378\n",
      "8/19, train_loss: 0.5376\n",
      "9/19, train_loss: 0.5376\n",
      "10/19, train_loss: 0.5374\n",
      "11/19, train_loss: 0.5374\n",
      "12/19, train_loss: 0.5373\n",
      "13/19, train_loss: 0.5374\n",
      "14/19, train_loss: 0.5371\n",
      "15/19, train_loss: 0.5370\n",
      "16/19, train_loss: 0.5369\n",
      "17/19, train_loss: 0.5368\n",
      "18/19, train_loss: 0.5368\n",
      "19/19, train_loss: 0.5366\n",
      "Epoch 25 average loss: 0.5375\n",
      "Current epoch: 25, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 26/300\n",
      "1/19, train_loss: 0.5365\n",
      "2/19, train_loss: 0.5366\n",
      "3/19, train_loss: 0.5365\n",
      "4/19, train_loss: 0.5363\n",
      "5/19, train_loss: 0.5362\n",
      "6/19, train_loss: 0.5361\n",
      "7/19, train_loss: 0.5361\n",
      "8/19, train_loss: 0.5360\n",
      "9/19, train_loss: 0.5358\n",
      "10/19, train_loss: 0.5359\n",
      "11/19, train_loss: 0.5357\n",
      "12/19, train_loss: 0.5356\n",
      "13/19, train_loss: 0.5355\n",
      "14/19, train_loss: 0.5354\n",
      "15/19, train_loss: 0.5354\n",
      "16/19, train_loss: 0.5353\n",
      "17/19, train_loss: 0.5353\n",
      "18/19, train_loss: 0.5351\n",
      "19/19, train_loss: 0.5350\n",
      "Epoch 26 average loss: 0.5358\n",
      "\n",
      "Epoch 27/300\n",
      "1/19, train_loss: 0.5350\n",
      "2/19, train_loss: 0.5349\n",
      "3/19, train_loss: 0.5348\n",
      "4/19, train_loss: 0.5348\n",
      "5/19, train_loss: 0.5346\n",
      "6/19, train_loss: 0.5346\n",
      "7/19, train_loss: 0.5345\n",
      "8/19, train_loss: 0.5344\n",
      "9/19, train_loss: 0.5343\n",
      "10/19, train_loss: 0.5343\n",
      "11/19, train_loss: 0.5343\n",
      "12/19, train_loss: 0.5341\n",
      "13/19, train_loss: 0.5340\n",
      "14/19, train_loss: 0.5339\n",
      "15/19, train_loss: 0.5339\n",
      "16/19, train_loss: 0.5339\n",
      "17/19, train_loss: 0.5338\n",
      "18/19, train_loss: 0.5336\n",
      "19/19, train_loss: 0.5337\n",
      "Epoch 27 average loss: 0.5343\n",
      "\n",
      "Epoch 28/300\n",
      "1/19, train_loss: 0.5335\n",
      "2/19, train_loss: 0.5334\n",
      "3/19, train_loss: 0.5334\n",
      "4/19, train_loss: 0.5333\n",
      "5/19, train_loss: 0.5332\n",
      "6/19, train_loss: 0.5333\n",
      "7/19, train_loss: 0.5331\n",
      "8/19, train_loss: 0.5331\n",
      "9/19, train_loss: 0.5330\n",
      "10/19, train_loss: 0.5329\n",
      "11/19, train_loss: 0.5328\n",
      "12/19, train_loss: 0.5327\n",
      "13/19, train_loss: 0.5326\n",
      "14/19, train_loss: 0.5325\n",
      "15/19, train_loss: 0.5325\n",
      "16/19, train_loss: 0.5324\n",
      "17/19, train_loss: 0.5322\n",
      "18/19, train_loss: 0.5322\n",
      "19/19, train_loss: 0.5322\n",
      "Epoch 28 average loss: 0.5329\n",
      "\n",
      "Epoch 29/300\n",
      "1/19, train_loss: 0.5322\n",
      "2/19, train_loss: 0.5320\n",
      "3/19, train_loss: 0.5320\n",
      "4/19, train_loss: 0.5319\n",
      "5/19, train_loss: 0.5318\n",
      "6/19, train_loss: 0.5318\n",
      "7/19, train_loss: 0.5317\n",
      "8/19, train_loss: 0.5318\n",
      "9/19, train_loss: 0.5317\n",
      "10/19, train_loss: 0.5316\n",
      "11/19, train_loss: 0.5314\n",
      "12/19, train_loss: 0.5314\n",
      "13/19, train_loss: 0.5313\n",
      "14/19, train_loss: 0.5312\n",
      "15/19, train_loss: 0.5313\n",
      "16/19, train_loss: 0.5311\n",
      "17/19, train_loss: 0.5311\n",
      "18/19, train_loss: 0.5310\n",
      "19/19, train_loss: 0.5309\n",
      "Epoch 29 average loss: 0.5315\n",
      "\n",
      "Epoch 30/300\n",
      "1/19, train_loss: 0.5310\n",
      "2/19, train_loss: 0.5308\n",
      "3/19, train_loss: 0.5308\n",
      "4/19, train_loss: 0.5307\n",
      "5/19, train_loss: 0.5306\n",
      "6/19, train_loss: 0.5305\n",
      "7/19, train_loss: 0.5304\n",
      "8/19, train_loss: 0.5304\n",
      "9/19, train_loss: 0.5303\n",
      "10/19, train_loss: 0.5303\n",
      "11/19, train_loss: 0.5302\n",
      "12/19, train_loss: 0.5302\n",
      "13/19, train_loss: 0.5301\n",
      "14/19, train_loss: 0.5301\n",
      "15/19, train_loss: 0.5299\n",
      "16/19, train_loss: 0.5299\n",
      "17/19, train_loss: 0.5298\n",
      "18/19, train_loss: 0.5298\n",
      "19/19, train_loss: 0.5297\n",
      "Epoch 30 average loss: 0.5303\n",
      "Current epoch: 30, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 31/300\n",
      "1/19, train_loss: 0.5296\n",
      "2/19, train_loss: 0.5297\n",
      "3/19, train_loss: 0.5295\n",
      "4/19, train_loss: 0.5296\n",
      "5/19, train_loss: 0.5294\n",
      "6/19, train_loss: 0.5293\n",
      "7/19, train_loss: 0.5293\n",
      "8/19, train_loss: 0.5293\n",
      "9/19, train_loss: 0.5292\n",
      "10/19, train_loss: 0.5292\n",
      "11/19, train_loss: 0.5290\n",
      "12/19, train_loss: 0.5290\n",
      "13/19, train_loss: 0.5290\n",
      "14/19, train_loss: 0.5289\n",
      "15/19, train_loss: 0.5288\n",
      "16/19, train_loss: 0.5289\n",
      "17/19, train_loss: 0.5287\n",
      "18/19, train_loss: 0.5286\n",
      "19/19, train_loss: 0.5286\n",
      "Epoch 31 average loss: 0.5291\n",
      "\n",
      "Epoch 32/300\n",
      "1/19, train_loss: 0.5286\n",
      "2/19, train_loss: 0.5285\n",
      "3/19, train_loss: 0.5284\n",
      "4/19, train_loss: 0.5283\n",
      "5/19, train_loss: 0.5284\n",
      "6/19, train_loss: 0.5282\n",
      "7/19, train_loss: 0.5282\n",
      "8/19, train_loss: 0.5281\n",
      "9/19, train_loss: 0.5281\n",
      "10/19, train_loss: 0.5280\n",
      "11/19, train_loss: 0.5279\n",
      "12/19, train_loss: 0.5279\n",
      "13/19, train_loss: 0.5278\n",
      "14/19, train_loss: 0.5278\n",
      "15/19, train_loss: 0.5278\n",
      "16/19, train_loss: 0.5277\n",
      "17/19, train_loss: 0.5276\n",
      "18/19, train_loss: 0.5276\n",
      "19/19, train_loss: 0.5276\n",
      "Epoch 32 average loss: 0.5280\n",
      "\n",
      "Epoch 33/300\n",
      "1/19, train_loss: 0.5274\n",
      "2/19, train_loss: 0.5274\n",
      "3/19, train_loss: 0.5274\n",
      "4/19, train_loss: 0.5274\n",
      "5/19, train_loss: 0.5273\n",
      "6/19, train_loss: 0.5272\n",
      "7/19, train_loss: 0.5272\n",
      "8/19, train_loss: 0.5272\n",
      "9/19, train_loss: 0.5270\n",
      "10/19, train_loss: 0.5270\n",
      "11/19, train_loss: 0.5269\n",
      "12/19, train_loss: 0.5269\n",
      "13/19, train_loss: 0.5268\n",
      "14/19, train_loss: 0.5268\n",
      "15/19, train_loss: 0.5267\n",
      "16/19, train_loss: 0.5266\n",
      "17/19, train_loss: 0.5266\n",
      "18/19, train_loss: 0.5266\n",
      "19/19, train_loss: 0.5267\n",
      "Epoch 33 average loss: 0.5270\n",
      "\n",
      "Epoch 34/300\n",
      "1/19, train_loss: 0.5265\n",
      "2/19, train_loss: 0.5265\n",
      "3/19, train_loss: 0.5264\n",
      "4/19, train_loss: 0.5263\n",
      "5/19, train_loss: 0.5262\n",
      "6/19, train_loss: 0.5262\n",
      "7/19, train_loss: 0.5262\n",
      "8/19, train_loss: 0.5261\n",
      "9/19, train_loss: 0.5261\n",
      "10/19, train_loss: 0.5260\n",
      "11/19, train_loss: 0.5261\n",
      "12/19, train_loss: 0.5260\n",
      "13/19, train_loss: 0.5260\n",
      "14/19, train_loss: 0.5258\n",
      "15/19, train_loss: 0.5258\n",
      "16/19, train_loss: 0.5257\n",
      "17/19, train_loss: 0.5257\n",
      "18/19, train_loss: 0.5257\n",
      "19/19, train_loss: 0.5256\n",
      "Epoch 34 average loss: 0.5260\n",
      "\n",
      "Epoch 35/300\n",
      "1/19, train_loss: 0.5255\n",
      "2/19, train_loss: 0.5256\n",
      "3/19, train_loss: 0.5255\n",
      "4/19, train_loss: 0.5254\n",
      "5/19, train_loss: 0.5253\n",
      "6/19, train_loss: 0.5253\n",
      "7/19, train_loss: 0.5253\n",
      "8/19, train_loss: 0.5252\n",
      "9/19, train_loss: 0.5251\n",
      "10/19, train_loss: 0.5252\n",
      "11/19, train_loss: 0.5251\n",
      "12/19, train_loss: 0.5250\n",
      "13/19, train_loss: 0.5249\n",
      "14/19, train_loss: 0.5249\n",
      "15/19, train_loss: 0.5249\n",
      "16/19, train_loss: 0.5248\n",
      "17/19, train_loss: 0.5248\n",
      "18/19, train_loss: 0.5248\n",
      "19/19, train_loss: 0.5247\n",
      "Epoch 35 average loss: 0.5251\n",
      "Current epoch: 35, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 36/300\n",
      "1/19, train_loss: 0.5246\n",
      "2/19, train_loss: 0.5246\n",
      "3/19, train_loss: 0.5246\n",
      "4/19, train_loss: 0.5246\n",
      "5/19, train_loss: 0.5245\n",
      "6/19, train_loss: 0.5244\n",
      "7/19, train_loss: 0.5244\n",
      "8/19, train_loss: 0.5244\n",
      "9/19, train_loss: 0.5243\n",
      "10/19, train_loss: 0.5242\n",
      "11/19, train_loss: 0.5242\n",
      "12/19, train_loss: 0.5241\n",
      "13/19, train_loss: 0.5242\n",
      "14/19, train_loss: 0.5241\n",
      "15/19, train_loss: 0.5240\n",
      "16/19, train_loss: 0.5239\n",
      "17/19, train_loss: 0.5239\n",
      "18/19, train_loss: 0.5239\n",
      "19/19, train_loss: 0.5239\n",
      "Epoch 36 average loss: 0.5242\n",
      "\n",
      "Epoch 37/300\n",
      "1/19, train_loss: 0.5238\n",
      "2/19, train_loss: 0.5238\n",
      "3/19, train_loss: 0.5237\n",
      "4/19, train_loss: 0.5236\n",
      "5/19, train_loss: 0.5236\n",
      "6/19, train_loss: 0.5236\n",
      "7/19, train_loss: 0.5235\n",
      "8/19, train_loss: 0.5236\n",
      "9/19, train_loss: 0.5235\n",
      "10/19, train_loss: 0.5234\n",
      "11/19, train_loss: 0.5234\n",
      "12/19, train_loss: 0.5233\n",
      "13/19, train_loss: 0.5233\n",
      "14/19, train_loss: 0.5232\n",
      "15/19, train_loss: 0.5232\n",
      "16/19, train_loss: 0.5232\n",
      "17/19, train_loss: 0.5231\n",
      "18/19, train_loss: 0.5231\n",
      "19/19, train_loss: 0.5231\n",
      "Epoch 37 average loss: 0.5234\n",
      "\n",
      "Epoch 38/300\n",
      "1/19, train_loss: 0.5231\n",
      "2/19, train_loss: 0.5230\n",
      "3/19, train_loss: 0.5230\n",
      "4/19, train_loss: 0.5229\n",
      "5/19, train_loss: 0.5228\n",
      "6/19, train_loss: 0.5228\n",
      "7/19, train_loss: 0.5228\n",
      "8/19, train_loss: 0.5227\n",
      "9/19, train_loss: 0.5227\n",
      "10/19, train_loss: 0.5226\n",
      "11/19, train_loss: 0.5226\n",
      "12/19, train_loss: 0.5225\n",
      "13/19, train_loss: 0.5225\n",
      "14/19, train_loss: 0.5225\n",
      "15/19, train_loss: 0.5224\n",
      "16/19, train_loss: 0.5224\n",
      "17/19, train_loss: 0.5224\n",
      "18/19, train_loss: 0.5224\n",
      "19/19, train_loss: 0.5223\n",
      "Epoch 38 average loss: 0.5227\n",
      "\n",
      "Epoch 39/300\n",
      "1/19, train_loss: 0.5223\n",
      "2/19, train_loss: 0.5222\n",
      "3/19, train_loss: 0.5222\n",
      "4/19, train_loss: 0.5221\n",
      "5/19, train_loss: 0.5221\n",
      "6/19, train_loss: 0.5221\n",
      "7/19, train_loss: 0.5220\n",
      "8/19, train_loss: 0.5220\n",
      "9/19, train_loss: 0.5219\n",
      "10/19, train_loss: 0.5219\n",
      "11/19, train_loss: 0.5219\n",
      "12/19, train_loss: 0.5219\n",
      "13/19, train_loss: 0.5218\n",
      "14/19, train_loss: 0.5217\n",
      "15/19, train_loss: 0.5217\n",
      "16/19, train_loss: 0.5217\n",
      "17/19, train_loss: 0.5217\n",
      "18/19, train_loss: 0.5216\n",
      "19/19, train_loss: 0.5216\n",
      "Epoch 39 average loss: 0.5219\n",
      "\n",
      "Epoch 40/300\n",
      "1/19, train_loss: 0.5215\n",
      "2/19, train_loss: 0.5215\n",
      "3/19, train_loss: 0.5215\n",
      "4/19, train_loss: 0.5214\n",
      "5/19, train_loss: 0.5214\n",
      "6/19, train_loss: 0.5214\n",
      "7/19, train_loss: 0.5213\n",
      "8/19, train_loss: 0.5213\n",
      "9/19, train_loss: 0.5213\n",
      "10/19, train_loss: 0.5212\n",
      "11/19, train_loss: 0.5212\n",
      "12/19, train_loss: 0.5212\n",
      "13/19, train_loss: 0.5211\n",
      "14/19, train_loss: 0.5211\n",
      "15/19, train_loss: 0.5210\n",
      "16/19, train_loss: 0.5210\n",
      "17/19, train_loss: 0.5210\n",
      "18/19, train_loss: 0.5209\n",
      "19/19, train_loss: 0.5210\n",
      "Epoch 40 average loss: 0.5212\n",
      "Current epoch: 40, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 41/300\n",
      "1/19, train_loss: 0.5208\n",
      "2/19, train_loss: 0.5209\n",
      "3/19, train_loss: 0.5208\n",
      "4/19, train_loss: 0.5207\n",
      "5/19, train_loss: 0.5207\n",
      "6/19, train_loss: 0.5207\n",
      "7/19, train_loss: 0.5206\n",
      "8/19, train_loss: 0.5206\n",
      "9/19, train_loss: 0.5205\n",
      "10/19, train_loss: 0.5205\n",
      "11/19, train_loss: 0.5205\n",
      "12/19, train_loss: 0.5204\n",
      "13/19, train_loss: 0.5205\n",
      "14/19, train_loss: 0.5204\n",
      "15/19, train_loss: 0.5204\n",
      "16/19, train_loss: 0.5203\n",
      "17/19, train_loss: 0.5203\n",
      "18/19, train_loss: 0.5203\n",
      "19/19, train_loss: 0.5203\n",
      "Epoch 41 average loss: 0.5205\n",
      "\n",
      "Epoch 42/300\n",
      "1/19, train_loss: 0.5202\n",
      "2/19, train_loss: 0.5202\n",
      "3/19, train_loss: 0.5201\n",
      "4/19, train_loss: 0.5201\n",
      "5/19, train_loss: 0.5201\n",
      "6/19, train_loss: 0.5201\n",
      "7/19, train_loss: 0.5200\n",
      "8/19, train_loss: 0.5200\n",
      "9/19, train_loss: 0.5199\n",
      "10/19, train_loss: 0.5199\n",
      "11/19, train_loss: 0.5199\n",
      "12/19, train_loss: 0.5198\n",
      "13/19, train_loss: 0.5199\n",
      "14/19, train_loss: 0.5198\n",
      "15/19, train_loss: 0.5197\n",
      "16/19, train_loss: 0.5197\n",
      "17/19, train_loss: 0.5197\n",
      "18/19, train_loss: 0.5196\n",
      "19/19, train_loss: 0.5196\n",
      "Epoch 42 average loss: 0.5199\n",
      "\n",
      "Epoch 43/300\n",
      "1/19, train_loss: 0.5196\n",
      "2/19, train_loss: 0.5195\n",
      "3/19, train_loss: 0.5195\n",
      "4/19, train_loss: 0.5195\n",
      "5/19, train_loss: 0.5195\n",
      "6/19, train_loss: 0.5194\n",
      "7/19, train_loss: 0.5194\n",
      "8/19, train_loss: 0.5193\n",
      "9/19, train_loss: 0.5193\n",
      "10/19, train_loss: 0.5193\n",
      "11/19, train_loss: 0.5193\n",
      "12/19, train_loss: 0.5193\n",
      "13/19, train_loss: 0.5192\n",
      "14/19, train_loss: 0.5192\n",
      "15/19, train_loss: 0.5191\n",
      "16/19, train_loss: 0.5191\n",
      "17/19, train_loss: 0.5191\n",
      "18/19, train_loss: 0.5191\n",
      "19/19, train_loss: 0.5190\n",
      "Epoch 43 average loss: 0.5193\n",
      "\n",
      "Epoch 44/300\n",
      "1/19, train_loss: 0.5190\n",
      "2/19, train_loss: 0.5190\n",
      "3/19, train_loss: 0.5189\n",
      "4/19, train_loss: 0.5189\n",
      "5/19, train_loss: 0.5189\n",
      "6/19, train_loss: 0.5189\n",
      "7/19, train_loss: 0.5188\n",
      "8/19, train_loss: 0.5188\n",
      "9/19, train_loss: 0.5188\n",
      "10/19, train_loss: 0.5188\n",
      "11/19, train_loss: 0.5187\n",
      "12/19, train_loss: 0.5187\n",
      "13/19, train_loss: 0.5187\n",
      "14/19, train_loss: 0.5186\n",
      "15/19, train_loss: 0.5186\n",
      "16/19, train_loss: 0.5185\n",
      "17/19, train_loss: 0.5185\n",
      "18/19, train_loss: 0.5185\n",
      "19/19, train_loss: 0.5184\n",
      "Epoch 44 average loss: 0.5187\n",
      "\n",
      "Epoch 45/300\n",
      "1/19, train_loss: 0.5185\n",
      "2/19, train_loss: 0.5184\n",
      "3/19, train_loss: 0.5184\n",
      "4/19, train_loss: 0.5183\n",
      "5/19, train_loss: 0.5183\n",
      "6/19, train_loss: 0.5183\n",
      "7/19, train_loss: 0.5182\n",
      "8/19, train_loss: 0.5182\n",
      "9/19, train_loss: 0.5182\n",
      "10/19, train_loss: 0.5182\n",
      "11/19, train_loss: 0.5182\n",
      "12/19, train_loss: 0.5181\n",
      "13/19, train_loss: 0.5181\n",
      "14/19, train_loss: 0.5180\n",
      "15/19, train_loss: 0.5180\n",
      "16/19, train_loss: 0.5180\n",
      "17/19, train_loss: 0.5180\n",
      "18/19, train_loss: 0.5179\n",
      "19/19, train_loss: 0.5179\n",
      "Epoch 45 average loss: 0.5182\n",
      "Current epoch: 45, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 46/300\n",
      "1/19, train_loss: 0.5179\n",
      "2/19, train_loss: 0.5179\n",
      "3/19, train_loss: 0.5178\n",
      "4/19, train_loss: 0.5178\n",
      "5/19, train_loss: 0.5178\n",
      "6/19, train_loss: 0.5178\n",
      "7/19, train_loss: 0.5177\n",
      "8/19, train_loss: 0.5177\n",
      "9/19, train_loss: 0.5177\n",
      "10/19, train_loss: 0.5177\n",
      "11/19, train_loss: 0.5176\n",
      "12/19, train_loss: 0.5175\n",
      "13/19, train_loss: 0.5175\n",
      "14/19, train_loss: 0.5175\n",
      "15/19, train_loss: 0.5175\n",
      "16/19, train_loss: 0.5174\n",
      "17/19, train_loss: 0.5174\n",
      "18/19, train_loss: 0.5174\n",
      "19/19, train_loss: 0.5174\n",
      "Epoch 46 average loss: 0.5176\n",
      "\n",
      "Epoch 47/300\n",
      "1/19, train_loss: 0.5173\n",
      "2/19, train_loss: 0.5173\n",
      "3/19, train_loss: 0.5173\n",
      "4/19, train_loss: 0.5173\n",
      "5/19, train_loss: 0.5173\n",
      "6/19, train_loss: 0.5172\n",
      "7/19, train_loss: 0.5172\n",
      "8/19, train_loss: 0.5172\n",
      "9/19, train_loss: 0.5171\n",
      "10/19, train_loss: 0.5171\n",
      "11/19, train_loss: 0.5171\n",
      "12/19, train_loss: 0.5171\n",
      "13/19, train_loss: 0.5170\n",
      "14/19, train_loss: 0.5170\n",
      "15/19, train_loss: 0.5170\n",
      "16/19, train_loss: 0.5169\n",
      "17/19, train_loss: 0.5170\n",
      "18/19, train_loss: 0.5169\n",
      "19/19, train_loss: 0.5169\n",
      "Epoch 47 average loss: 0.5171\n",
      "\n",
      "Epoch 48/300\n",
      "1/19, train_loss: 0.5168\n",
      "2/19, train_loss: 0.5168\n",
      "3/19, train_loss: 0.5168\n",
      "4/19, train_loss: 0.5168\n",
      "5/19, train_loss: 0.5167\n",
      "6/19, train_loss: 0.5167\n",
      "7/19, train_loss: 0.5167\n",
      "8/19, train_loss: 0.5167\n",
      "9/19, train_loss: 0.5166\n",
      "10/19, train_loss: 0.5166\n",
      "11/19, train_loss: 0.5166\n",
      "12/19, train_loss: 0.5166\n",
      "13/19, train_loss: 0.5165\n",
      "14/19, train_loss: 0.5165\n",
      "15/19, train_loss: 0.5165\n",
      "16/19, train_loss: 0.5165\n",
      "17/19, train_loss: 0.5165\n",
      "18/19, train_loss: 0.5165\n",
      "19/19, train_loss: 0.5164\n",
      "Epoch 48 average loss: 0.5166\n",
      "\n",
      "Epoch 49/300\n",
      "1/19, train_loss: 0.5164\n",
      "2/19, train_loss: 0.5163\n",
      "3/19, train_loss: 0.5163\n",
      "4/19, train_loss: 0.5163\n",
      "5/19, train_loss: 0.5163\n",
      "6/19, train_loss: 0.5163\n",
      "7/19, train_loss: 0.5162\n",
      "8/19, train_loss: 0.5162\n",
      "9/19, train_loss: 0.5162\n",
      "10/19, train_loss: 0.5161\n",
      "11/19, train_loss: 0.5161\n",
      "12/19, train_loss: 0.5161\n",
      "13/19, train_loss: 0.5161\n",
      "14/19, train_loss: 0.5160\n",
      "15/19, train_loss: 0.5161\n",
      "16/19, train_loss: 0.5160\n",
      "17/19, train_loss: 0.5160\n",
      "18/19, train_loss: 0.5159\n",
      "19/19, train_loss: 0.5159\n",
      "Epoch 49 average loss: 0.5161\n",
      "\n",
      "Epoch 50/300\n",
      "1/19, train_loss: 0.5159\n",
      "2/19, train_loss: 0.5159\n",
      "3/19, train_loss: 0.5158\n",
      "4/19, train_loss: 0.5158\n",
      "5/19, train_loss: 0.5159\n",
      "6/19, train_loss: 0.5158\n",
      "7/19, train_loss: 0.5157\n",
      "8/19, train_loss: 0.5158\n",
      "9/19, train_loss: 0.5157\n",
      "10/19, train_loss: 0.5157\n",
      "11/19, train_loss: 0.5157\n",
      "12/19, train_loss: 0.5156\n",
      "13/19, train_loss: 0.5156\n",
      "14/19, train_loss: 0.5156\n",
      "15/19, train_loss: 0.5156\n",
      "16/19, train_loss: 0.5156\n",
      "17/19, train_loss: 0.5155\n",
      "18/19, train_loss: 0.5155\n",
      "19/19, train_loss: 0.5155\n",
      "Epoch 50 average loss: 0.5157\n",
      "Current epoch: 50, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 51/300\n",
      "1/19, train_loss: 0.5155\n",
      "2/19, train_loss: 0.5154\n",
      "3/19, train_loss: 0.5154\n",
      "4/19, train_loss: 0.5154\n",
      "5/19, train_loss: 0.5154\n",
      "6/19, train_loss: 0.5153\n",
      "7/19, train_loss: 0.5153\n",
      "8/19, train_loss: 0.5153\n",
      "9/19, train_loss: 0.5153\n",
      "10/19, train_loss: 0.5152\n",
      "11/19, train_loss: 0.5152\n",
      "12/19, train_loss: 0.5152\n",
      "13/19, train_loss: 0.5152\n",
      "14/19, train_loss: 0.5151\n",
      "15/19, train_loss: 0.5152\n",
      "16/19, train_loss: 0.5151\n",
      "17/19, train_loss: 0.5151\n",
      "18/19, train_loss: 0.5151\n",
      "19/19, train_loss: 0.5150\n",
      "Epoch 51 average loss: 0.5152\n",
      "\n",
      "Epoch 52/300\n",
      "1/19, train_loss: 0.5150\n",
      "2/19, train_loss: 0.5150\n",
      "3/19, train_loss: 0.5150\n",
      "4/19, train_loss: 0.5149\n",
      "5/19, train_loss: 0.5149\n",
      "6/19, train_loss: 0.5149\n",
      "7/19, train_loss: 0.5149\n",
      "8/19, train_loss: 0.5149\n",
      "9/19, train_loss: 0.5148\n",
      "10/19, train_loss: 0.5148\n",
      "11/19, train_loss: 0.5148\n",
      "12/19, train_loss: 0.5148\n",
      "13/19, train_loss: 0.5147\n",
      "14/19, train_loss: 0.5147\n",
      "15/19, train_loss: 0.5147\n",
      "16/19, train_loss: 0.5147\n",
      "17/19, train_loss: 0.5147\n",
      "18/19, train_loss: 0.5147\n",
      "19/19, train_loss: 0.5146\n",
      "Epoch 52 average loss: 0.5148\n",
      "\n",
      "Epoch 53/300\n",
      "1/19, train_loss: 0.5146\n",
      "2/19, train_loss: 0.5146\n",
      "3/19, train_loss: 0.5146\n",
      "4/19, train_loss: 0.5146\n",
      "5/19, train_loss: 0.5145\n",
      "6/19, train_loss: 0.5145\n",
      "7/19, train_loss: 0.5145\n",
      "8/19, train_loss: 0.5144\n",
      "9/19, train_loss: 0.5144\n",
      "10/19, train_loss: 0.5144\n",
      "11/19, train_loss: 0.5144\n",
      "12/19, train_loss: 0.5144\n",
      "13/19, train_loss: 0.5144\n",
      "14/19, train_loss: 0.5144\n",
      "15/19, train_loss: 0.5143\n",
      "16/19, train_loss: 0.5143\n",
      "17/19, train_loss: 0.5143\n",
      "18/19, train_loss: 0.5143\n",
      "19/19, train_loss: 0.5142\n",
      "Epoch 53 average loss: 0.5144\n",
      "\n",
      "Epoch 54/300\n",
      "1/19, train_loss: 0.5142\n",
      "2/19, train_loss: 0.5142\n",
      "3/19, train_loss: 0.5142\n",
      "4/19, train_loss: 0.5142\n",
      "5/19, train_loss: 0.5141\n",
      "6/19, train_loss: 0.5141\n",
      "7/19, train_loss: 0.5141\n",
      "8/19, train_loss: 0.5141\n",
      "9/19, train_loss: 0.5140\n",
      "10/19, train_loss: 0.5140\n",
      "11/19, train_loss: 0.5140\n",
      "12/19, train_loss: 0.5140\n",
      "13/19, train_loss: 0.5140\n",
      "14/19, train_loss: 0.5140\n",
      "15/19, train_loss: 0.5139\n",
      "16/19, train_loss: 0.5139\n",
      "17/19, train_loss: 0.5139\n",
      "18/19, train_loss: 0.5139\n",
      "19/19, train_loss: 0.5138\n",
      "Epoch 54 average loss: 0.5140\n",
      "\n",
      "Epoch 55/300\n",
      "1/19, train_loss: 0.5138\n",
      "2/19, train_loss: 0.5138\n",
      "3/19, train_loss: 0.5138\n",
      "4/19, train_loss: 0.5138\n",
      "5/19, train_loss: 0.5137\n",
      "6/19, train_loss: 0.5138\n",
      "7/19, train_loss: 0.5137\n",
      "8/19, train_loss: 0.5137\n",
      "9/19, train_loss: 0.5137\n",
      "10/19, train_loss: 0.5137\n",
      "11/19, train_loss: 0.5137\n",
      "12/19, train_loss: 0.5136\n",
      "13/19, train_loss: 0.5136\n",
      "14/19, train_loss: 0.5136\n",
      "15/19, train_loss: 0.5136\n",
      "16/19, train_loss: 0.5136\n",
      "17/19, train_loss: 0.5135\n",
      "18/19, train_loss: 0.5135\n",
      "19/19, train_loss: 0.5135\n",
      "Epoch 55 average loss: 0.5137\n",
      "Current epoch: 55, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 56/300\n",
      "1/19, train_loss: 0.5135\n",
      "2/19, train_loss: 0.5134\n",
      "3/19, train_loss: 0.5134\n",
      "4/19, train_loss: 0.5134\n",
      "5/19, train_loss: 0.5134\n",
      "6/19, train_loss: 0.5134\n",
      "7/19, train_loss: 0.5134\n",
      "8/19, train_loss: 0.5133\n",
      "9/19, train_loss: 0.5133\n",
      "10/19, train_loss: 0.5133\n",
      "11/19, train_loss: 0.5133\n",
      "12/19, train_loss: 0.5133\n",
      "13/19, train_loss: 0.5133\n",
      "14/19, train_loss: 0.5133\n",
      "15/19, train_loss: 0.5132\n",
      "16/19, train_loss: 0.5132\n",
      "17/19, train_loss: 0.5132\n",
      "18/19, train_loss: 0.5132\n",
      "19/19, train_loss: 0.5132\n",
      "Epoch 56 average loss: 0.5133\n",
      "\n",
      "Epoch 57/300\n",
      "1/19, train_loss: 0.5131\n",
      "2/19, train_loss: 0.5131\n",
      "3/19, train_loss: 0.5131\n",
      "4/19, train_loss: 0.5131\n",
      "5/19, train_loss: 0.5130\n",
      "6/19, train_loss: 0.5130\n",
      "7/19, train_loss: 0.5130\n",
      "8/19, train_loss: 0.5130\n",
      "9/19, train_loss: 0.5130\n",
      "10/19, train_loss: 0.5130\n",
      "11/19, train_loss: 0.5129\n",
      "12/19, train_loss: 0.5130\n",
      "13/19, train_loss: 0.5129\n",
      "14/19, train_loss: 0.5129\n",
      "15/19, train_loss: 0.5129\n",
      "16/19, train_loss: 0.5129\n",
      "17/19, train_loss: 0.5128\n",
      "18/19, train_loss: 0.5128\n",
      "19/19, train_loss: 0.5128\n",
      "Epoch 57 average loss: 0.5130\n",
      "\n",
      "Epoch 58/300\n",
      "1/19, train_loss: 0.5128\n",
      "2/19, train_loss: 0.5128\n",
      "3/19, train_loss: 0.5127\n",
      "4/19, train_loss: 0.5127\n",
      "5/19, train_loss: 0.5127\n",
      "6/19, train_loss: 0.5127\n",
      "7/19, train_loss: 0.5127\n",
      "8/19, train_loss: 0.5127\n",
      "9/19, train_loss: 0.5126\n",
      "10/19, train_loss: 0.5126\n",
      "11/19, train_loss: 0.5126\n",
      "12/19, train_loss: 0.5126\n",
      "13/19, train_loss: 0.5126\n",
      "14/19, train_loss: 0.5126\n",
      "15/19, train_loss: 0.5125\n",
      "16/19, train_loss: 0.5125\n",
      "17/19, train_loss: 0.5125\n",
      "18/19, train_loss: 0.5125\n",
      "19/19, train_loss: 0.5125\n",
      "Epoch 58 average loss: 0.5126\n",
      "\n",
      "Epoch 59/300\n",
      "1/19, train_loss: 0.5125\n",
      "2/19, train_loss: 0.5124\n",
      "3/19, train_loss: 0.5124\n",
      "4/19, train_loss: 0.5124\n",
      "5/19, train_loss: 0.5124\n",
      "6/19, train_loss: 0.5124\n",
      "7/19, train_loss: 0.5124\n",
      "8/19, train_loss: 0.5123\n",
      "9/19, train_loss: 0.5123\n",
      "10/19, train_loss: 0.5123\n",
      "11/19, train_loss: 0.5123\n",
      "12/19, train_loss: 0.5123\n",
      "13/19, train_loss: 0.5123\n",
      "14/19, train_loss: 0.5122\n",
      "15/19, train_loss: 0.5122\n",
      "16/19, train_loss: 0.5122\n",
      "17/19, train_loss: 0.5122\n",
      "18/19, train_loss: 0.5122\n",
      "19/19, train_loss: 0.5122\n",
      "Epoch 59 average loss: 0.5123\n",
      "\n",
      "Epoch 60/300\n",
      "1/19, train_loss: 0.5122\n",
      "2/19, train_loss: 0.5121\n",
      "3/19, train_loss: 0.5121\n",
      "4/19, train_loss: 0.5121\n",
      "5/19, train_loss: 0.5121\n",
      "6/19, train_loss: 0.5121\n",
      "7/19, train_loss: 0.5121\n",
      "8/19, train_loss: 0.5120\n",
      "9/19, train_loss: 0.5120\n",
      "10/19, train_loss: 0.5120\n",
      "11/19, train_loss: 0.5120\n",
      "12/19, train_loss: 0.5120\n",
      "13/19, train_loss: 0.5120\n",
      "14/19, train_loss: 0.5119\n",
      "15/19, train_loss: 0.5119\n",
      "16/19, train_loss: 0.5119\n",
      "17/19, train_loss: 0.5119\n",
      "18/19, train_loss: 0.5119\n",
      "19/19, train_loss: 0.5119\n",
      "Epoch 60 average loss: 0.5120\n",
      "Current epoch: 60, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 61/300\n",
      "1/19, train_loss: 0.5118\n",
      "2/19, train_loss: 0.5118\n",
      "3/19, train_loss: 0.5118\n",
      "4/19, train_loss: 0.5118\n",
      "5/19, train_loss: 0.5118\n",
      "6/19, train_loss: 0.5118\n",
      "7/19, train_loss: 0.5118\n",
      "8/19, train_loss: 0.5117\n",
      "9/19, train_loss: 0.5117\n",
      "10/19, train_loss: 0.5117\n",
      "11/19, train_loss: 0.5117\n",
      "12/19, train_loss: 0.5117\n",
      "13/19, train_loss: 0.5117\n",
      "14/19, train_loss: 0.5117\n",
      "15/19, train_loss: 0.5116\n",
      "16/19, train_loss: 0.5116\n",
      "17/19, train_loss: 0.5116\n",
      "18/19, train_loss: 0.5116\n",
      "19/19, train_loss: 0.5116\n",
      "Epoch 61 average loss: 0.5117\n",
      "\n",
      "Epoch 62/300\n",
      "1/19, train_loss: 0.5116\n",
      "2/19, train_loss: 0.5115\n",
      "3/19, train_loss: 0.5115\n",
      "4/19, train_loss: 0.5115\n",
      "5/19, train_loss: 0.5115\n",
      "6/19, train_loss: 0.5115\n",
      "7/19, train_loss: 0.5115\n",
      "8/19, train_loss: 0.5115\n",
      "9/19, train_loss: 0.5115\n",
      "10/19, train_loss: 0.5114\n",
      "11/19, train_loss: 0.5114\n",
      "12/19, train_loss: 0.5114\n",
      "13/19, train_loss: 0.5114\n",
      "14/19, train_loss: 0.5114\n",
      "15/19, train_loss: 0.5114\n",
      "16/19, train_loss: 0.5113\n",
      "17/19, train_loss: 0.5113\n",
      "18/19, train_loss: 0.5113\n",
      "19/19, train_loss: 0.5113\n",
      "Epoch 62 average loss: 0.5114\n",
      "\n",
      "Epoch 63/300\n",
      "1/19, train_loss: 0.5113\n",
      "2/19, train_loss: 0.5113\n",
      "3/19, train_loss: 0.5113\n",
      "4/19, train_loss: 0.5112\n",
      "5/19, train_loss: 0.5112\n",
      "6/19, train_loss: 0.5112\n",
      "7/19, train_loss: 0.5112\n",
      "8/19, train_loss: 0.5112\n",
      "9/19, train_loss: 0.5112\n",
      "10/19, train_loss: 0.5112\n",
      "11/19, train_loss: 0.5111\n",
      "12/19, train_loss: 0.5111\n",
      "13/19, train_loss: 0.5111\n",
      "14/19, train_loss: 0.5111\n",
      "15/19, train_loss: 0.5111\n",
      "16/19, train_loss: 0.5111\n",
      "17/19, train_loss: 0.5111\n",
      "18/19, train_loss: 0.5110\n",
      "19/19, train_loss: 0.5111\n",
      "Epoch 63 average loss: 0.5112\n",
      "\n",
      "Epoch 64/300\n",
      "1/19, train_loss: 0.5110\n",
      "2/19, train_loss: 0.5110\n",
      "3/19, train_loss: 0.5110\n",
      "4/19, train_loss: 0.5110\n",
      "5/19, train_loss: 0.5110\n",
      "6/19, train_loss: 0.5109\n",
      "7/19, train_loss: 0.5109\n",
      "8/19, train_loss: 0.5109\n",
      "9/19, train_loss: 0.5109\n",
      "10/19, train_loss: 0.5109\n",
      "11/19, train_loss: 0.5109\n",
      "12/19, train_loss: 0.5109\n",
      "13/19, train_loss: 0.5108\n",
      "14/19, train_loss: 0.5108\n",
      "15/19, train_loss: 0.5108\n",
      "16/19, train_loss: 0.5108\n",
      "17/19, train_loss: 0.5108\n",
      "18/19, train_loss: 0.5108\n",
      "19/19, train_loss: 0.5108\n",
      "Epoch 64 average loss: 0.5109\n",
      "\n",
      "Epoch 65/300\n",
      "1/19, train_loss: 0.5108\n",
      "2/19, train_loss: 0.5107\n",
      "3/19, train_loss: 0.5107\n",
      "4/19, train_loss: 0.5107\n",
      "5/19, train_loss: 0.5107\n",
      "6/19, train_loss: 0.5107\n",
      "7/19, train_loss: 0.5107\n",
      "8/19, train_loss: 0.5107\n",
      "9/19, train_loss: 0.5107\n",
      "10/19, train_loss: 0.5106\n",
      "11/19, train_loss: 0.5106\n",
      "12/19, train_loss: 0.5106\n",
      "13/19, train_loss: 0.5106\n",
      "14/19, train_loss: 0.5106\n",
      "15/19, train_loss: 0.5106\n",
      "16/19, train_loss: 0.5106\n",
      "17/19, train_loss: 0.5106\n",
      "18/19, train_loss: 0.5105\n",
      "19/19, train_loss: 0.5105\n",
      "Epoch 65 average loss: 0.5106\n",
      "Current epoch: 65, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 66/300\n",
      "1/19, train_loss: 0.5105\n",
      "2/19, train_loss: 0.5105\n",
      "3/19, train_loss: 0.5105\n",
      "4/19, train_loss: 0.5105\n",
      "5/19, train_loss: 0.5105\n",
      "6/19, train_loss: 0.5104\n",
      "7/19, train_loss: 0.5105\n",
      "8/19, train_loss: 0.5105\n",
      "9/19, train_loss: 0.5104\n",
      "10/19, train_loss: 0.5104\n",
      "11/19, train_loss: 0.5104\n",
      "12/19, train_loss: 0.5104\n",
      "13/19, train_loss: 0.5103\n",
      "14/19, train_loss: 0.5103\n",
      "15/19, train_loss: 0.5103\n",
      "16/19, train_loss: 0.5103\n",
      "17/19, train_loss: 0.5103\n",
      "18/19, train_loss: 0.5103\n",
      "19/19, train_loss: 0.5103\n",
      "Epoch 66 average loss: 0.5104\n",
      "\n",
      "Epoch 67/300\n",
      "1/19, train_loss: 0.5103\n",
      "2/19, train_loss: 0.5103\n",
      "3/19, train_loss: 0.5102\n",
      "4/19, train_loss: 0.5103\n",
      "5/19, train_loss: 0.5102\n",
      "6/19, train_loss: 0.5102\n",
      "7/19, train_loss: 0.5102\n",
      "8/19, train_loss: 0.5102\n",
      "9/19, train_loss: 0.5102\n",
      "10/19, train_loss: 0.5102\n",
      "11/19, train_loss: 0.5102\n",
      "12/19, train_loss: 0.5101\n",
      "13/19, train_loss: 0.5101\n",
      "14/19, train_loss: 0.5101\n",
      "15/19, train_loss: 0.5101\n",
      "16/19, train_loss: 0.5101\n",
      "17/19, train_loss: 0.5101\n",
      "18/19, train_loss: 0.5101\n",
      "19/19, train_loss: 0.5101\n",
      "Epoch 67 average loss: 0.5102\n",
      "\n",
      "Epoch 68/300\n",
      "1/19, train_loss: 0.5101\n",
      "2/19, train_loss: 0.5101\n",
      "3/19, train_loss: 0.5100\n",
      "4/19, train_loss: 0.5100\n",
      "5/19, train_loss: 0.5100\n",
      "6/19, train_loss: 0.5100\n",
      "7/19, train_loss: 0.5100\n",
      "8/19, train_loss: 0.5100\n",
      "9/19, train_loss: 0.5099\n",
      "10/19, train_loss: 0.5099\n",
      "11/19, train_loss: 0.5099\n",
      "12/19, train_loss: 0.5099\n",
      "13/19, train_loss: 0.5099\n",
      "14/19, train_loss: 0.5099\n",
      "15/19, train_loss: 0.5099\n",
      "16/19, train_loss: 0.5099\n",
      "17/19, train_loss: 0.5099\n",
      "18/19, train_loss: 0.5099\n",
      "19/19, train_loss: 0.5098\n",
      "Epoch 68 average loss: 0.5099\n",
      "\n",
      "Epoch 69/300\n",
      "1/19, train_loss: 0.5098\n",
      "2/19, train_loss: 0.5098\n",
      "3/19, train_loss: 0.5098\n",
      "4/19, train_loss: 0.5098\n",
      "5/19, train_loss: 0.5098\n",
      "6/19, train_loss: 0.5098\n",
      "7/19, train_loss: 0.5097\n",
      "8/19, train_loss: 0.5097\n",
      "9/19, train_loss: 0.5097\n",
      "10/19, train_loss: 0.5097\n",
      "11/19, train_loss: 0.5097\n",
      "12/19, train_loss: 0.5097\n",
      "13/19, train_loss: 0.5097\n",
      "14/19, train_loss: 0.5097\n",
      "15/19, train_loss: 0.5096\n",
      "16/19, train_loss: 0.5097\n",
      "17/19, train_loss: 0.5096\n",
      "18/19, train_loss: 0.5096\n",
      "19/19, train_loss: 0.5096\n",
      "Epoch 69 average loss: 0.5097\n",
      "\n",
      "Epoch 70/300\n",
      "1/19, train_loss: 0.5096\n",
      "2/19, train_loss: 0.5096\n",
      "3/19, train_loss: 0.5096\n",
      "4/19, train_loss: 0.5096\n",
      "5/19, train_loss: 0.5096\n",
      "6/19, train_loss: 0.5096\n",
      "7/19, train_loss: 0.5095\n",
      "8/19, train_loss: 0.5095\n",
      "9/19, train_loss: 0.5095\n",
      "10/19, train_loss: 0.5095\n",
      "11/19, train_loss: 0.5095\n",
      "12/19, train_loss: 0.5095\n",
      "13/19, train_loss: 0.5095\n",
      "14/19, train_loss: 0.5094\n",
      "15/19, train_loss: 0.5094\n",
      "16/19, train_loss: 0.5094\n",
      "17/19, train_loss: 0.5094\n",
      "18/19, train_loss: 0.5094\n",
      "19/19, train_loss: 0.5094\n",
      "Epoch 70 average loss: 0.5095\n",
      "Current epoch: 70, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 71/300\n",
      "1/19, train_loss: 0.5094\n",
      "2/19, train_loss: 0.5094\n",
      "3/19, train_loss: 0.5094\n",
      "4/19, train_loss: 0.5094\n",
      "5/19, train_loss: 0.5093\n",
      "6/19, train_loss: 0.5094\n",
      "7/19, train_loss: 0.5093\n",
      "8/19, train_loss: 0.5093\n",
      "9/19, train_loss: 0.5093\n",
      "10/19, train_loss: 0.5093\n",
      "11/19, train_loss: 0.5093\n",
      "12/19, train_loss: 0.5093\n",
      "13/19, train_loss: 0.5093\n",
      "14/19, train_loss: 0.5092\n",
      "15/19, train_loss: 0.5092\n",
      "16/19, train_loss: 0.5092\n",
      "17/19, train_loss: 0.5092\n",
      "18/19, train_loss: 0.5092\n",
      "19/19, train_loss: 0.5092\n",
      "Epoch 71 average loss: 0.5093\n",
      "\n",
      "Epoch 72/300\n",
      "1/19, train_loss: 0.5092\n",
      "2/19, train_loss: 0.5092\n",
      "3/19, train_loss: 0.5092\n",
      "4/19, train_loss: 0.5091\n",
      "5/19, train_loss: 0.5091\n",
      "6/19, train_loss: 0.5091\n",
      "7/19, train_loss: 0.5091\n",
      "8/19, train_loss: 0.5091\n",
      "9/19, train_loss: 0.5091\n",
      "10/19, train_loss: 0.5091\n",
      "11/19, train_loss: 0.5091\n",
      "12/19, train_loss: 0.5091\n",
      "13/19, train_loss: 0.5091\n",
      "14/19, train_loss: 0.5090\n",
      "15/19, train_loss: 0.5091\n",
      "16/19, train_loss: 0.5090\n",
      "17/19, train_loss: 0.5090\n",
      "18/19, train_loss: 0.5090\n",
      "19/19, train_loss: 0.5090\n",
      "Epoch 72 average loss: 0.5091\n",
      "\n",
      "Epoch 73/300\n",
      "1/19, train_loss: 0.5090\n",
      "2/19, train_loss: 0.5090\n",
      "3/19, train_loss: 0.5090\n",
      "4/19, train_loss: 0.5090\n",
      "5/19, train_loss: 0.5089\n",
      "6/19, train_loss: 0.5089\n",
      "7/19, train_loss: 0.5089\n",
      "8/19, train_loss: 0.5090\n",
      "9/19, train_loss: 0.5089\n",
      "10/19, train_loss: 0.5089\n",
      "11/19, train_loss: 0.5089\n",
      "12/19, train_loss: 0.5089\n",
      "13/19, train_loss: 0.5089\n",
      "14/19, train_loss: 0.5088\n",
      "15/19, train_loss: 0.5088\n",
      "16/19, train_loss: 0.5088\n",
      "17/19, train_loss: 0.5088\n",
      "18/19, train_loss: 0.5088\n",
      "19/19, train_loss: 0.5088\n",
      "Epoch 73 average loss: 0.5089\n",
      "\n",
      "Epoch 74/300\n",
      "1/19, train_loss: 0.5088\n",
      "2/19, train_loss: 0.5088\n",
      "3/19, train_loss: 0.5088\n",
      "4/19, train_loss: 0.5088\n",
      "5/19, train_loss: 0.5088\n",
      "6/19, train_loss: 0.5087\n",
      "7/19, train_loss: 0.5087\n",
      "8/19, train_loss: 0.5087\n",
      "9/19, train_loss: 0.5087\n",
      "10/19, train_loss: 0.5087\n",
      "11/19, train_loss: 0.5087\n",
      "12/19, train_loss: 0.5087\n",
      "13/19, train_loss: 0.5087\n",
      "14/19, train_loss: 0.5087\n",
      "15/19, train_loss: 0.5087\n",
      "16/19, train_loss: 0.5086\n",
      "17/19, train_loss: 0.5086\n",
      "18/19, train_loss: 0.5087\n",
      "19/19, train_loss: 0.5086\n",
      "Epoch 74 average loss: 0.5087\n",
      "\n",
      "Epoch 75/300\n",
      "1/19, train_loss: 0.5086\n",
      "2/19, train_loss: 0.5086\n",
      "3/19, train_loss: 0.5086\n",
      "4/19, train_loss: 0.5086\n",
      "5/19, train_loss: 0.5086\n",
      "6/19, train_loss: 0.5086\n",
      "7/19, train_loss: 0.5085\n",
      "8/19, train_loss: 0.5085\n",
      "9/19, train_loss: 0.5085\n",
      "10/19, train_loss: 0.5085\n",
      "11/19, train_loss: 0.5085\n",
      "12/19, train_loss: 0.5085\n",
      "13/19, train_loss: 0.5085\n",
      "14/19, train_loss: 0.5085\n",
      "15/19, train_loss: 0.5085\n",
      "16/19, train_loss: 0.5085\n",
      "17/19, train_loss: 0.5085\n",
      "18/19, train_loss: 0.5084\n",
      "19/19, train_loss: 0.5085\n",
      "Epoch 75 average loss: 0.5085\n",
      "Current epoch: 75, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 76/300\n",
      "1/19, train_loss: 0.5084\n",
      "2/19, train_loss: 0.5084\n",
      "3/19, train_loss: 0.5084\n",
      "4/19, train_loss: 0.5084\n",
      "5/19, train_loss: 0.5084\n",
      "6/19, train_loss: 0.5084\n",
      "7/19, train_loss: 0.5084\n",
      "8/19, train_loss: 0.5084\n",
      "9/19, train_loss: 0.5084\n",
      "10/19, train_loss: 0.5083\n",
      "11/19, train_loss: 0.5083\n",
      "12/19, train_loss: 0.5083\n",
      "13/19, train_loss: 0.5083\n",
      "14/19, train_loss: 0.5083\n",
      "15/19, train_loss: 0.5083\n",
      "16/19, train_loss: 0.5083\n",
      "17/19, train_loss: 0.5083\n",
      "18/19, train_loss: 0.5083\n",
      "19/19, train_loss: 0.5083\n",
      "Epoch 76 average loss: 0.5083\n",
      "\n",
      "Epoch 77/300\n",
      "1/19, train_loss: 0.5082\n",
      "2/19, train_loss: 0.5082\n",
      "3/19, train_loss: 0.5082\n",
      "4/19, train_loss: 0.5082\n",
      "5/19, train_loss: 0.5082\n",
      "6/19, train_loss: 0.5082\n",
      "7/19, train_loss: 0.5082\n",
      "8/19, train_loss: 0.5082\n",
      "9/19, train_loss: 0.5082\n",
      "10/19, train_loss: 0.5082\n",
      "11/19, train_loss: 0.5082\n",
      "12/19, train_loss: 0.5082\n",
      "13/19, train_loss: 0.5081\n",
      "14/19, train_loss: 0.5081\n",
      "15/19, train_loss: 0.5081\n",
      "16/19, train_loss: 0.5081\n",
      "17/19, train_loss: 0.5081\n",
      "18/19, train_loss: 0.5081\n",
      "19/19, train_loss: 0.5081\n",
      "Epoch 77 average loss: 0.5082\n",
      "\n",
      "Epoch 78/300\n",
      "1/19, train_loss: 0.5081\n",
      "2/19, train_loss: 0.5081\n",
      "3/19, train_loss: 0.5081\n",
      "4/19, train_loss: 0.5081\n",
      "5/19, train_loss: 0.5080\n",
      "6/19, train_loss: 0.5080\n",
      "7/19, train_loss: 0.5080\n",
      "8/19, train_loss: 0.5080\n",
      "9/19, train_loss: 0.5080\n",
      "10/19, train_loss: 0.5080\n",
      "11/19, train_loss: 0.5080\n",
      "12/19, train_loss: 0.5080\n",
      "13/19, train_loss: 0.5080\n",
      "14/19, train_loss: 0.5080\n",
      "15/19, train_loss: 0.5080\n",
      "16/19, train_loss: 0.5080\n",
      "17/19, train_loss: 0.5079\n",
      "18/19, train_loss: 0.5079\n",
      "19/19, train_loss: 0.5080\n",
      "Epoch 78 average loss: 0.5080\n",
      "\n",
      "Epoch 79/300\n",
      "1/19, train_loss: 0.5079\n",
      "2/19, train_loss: 0.5079\n",
      "3/19, train_loss: 0.5079\n",
      "4/19, train_loss: 0.5079\n",
      "5/19, train_loss: 0.5079\n",
      "6/19, train_loss: 0.5079\n",
      "7/19, train_loss: 0.5079\n",
      "8/19, train_loss: 0.5079\n",
      "9/19, train_loss: 0.5079\n",
      "10/19, train_loss: 0.5078\n",
      "11/19, train_loss: 0.5078\n",
      "12/19, train_loss: 0.5078\n",
      "13/19, train_loss: 0.5078\n",
      "14/19, train_loss: 0.5078\n",
      "15/19, train_loss: 0.5078\n",
      "16/19, train_loss: 0.5078\n",
      "17/19, train_loss: 0.5078\n",
      "18/19, train_loss: 0.5078\n",
      "19/19, train_loss: 0.5078\n",
      "Epoch 79 average loss: 0.5078\n",
      "\n",
      "Epoch 80/300\n",
      "1/19, train_loss: 0.5078\n",
      "2/19, train_loss: 0.5078\n",
      "3/19, train_loss: 0.5077\n",
      "4/19, train_loss: 0.5078\n",
      "5/19, train_loss: 0.5077\n",
      "6/19, train_loss: 0.5077\n",
      "7/19, train_loss: 0.5077\n",
      "8/19, train_loss: 0.5077\n",
      "9/19, train_loss: 0.5077\n",
      "10/19, train_loss: 0.5077\n",
      "11/19, train_loss: 0.5077\n",
      "12/19, train_loss: 0.5077\n",
      "13/19, train_loss: 0.5077\n",
      "14/19, train_loss: 0.5077\n",
      "15/19, train_loss: 0.5076\n",
      "16/19, train_loss: 0.5076\n",
      "17/19, train_loss: 0.5076\n",
      "18/19, train_loss: 0.5076\n",
      "19/19, train_loss: 0.5076\n",
      "Epoch 80 average loss: 0.5077\n",
      "Current epoch: 80, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 81/300\n",
      "1/19, train_loss: 0.5076\n",
      "2/19, train_loss: 0.5076\n",
      "3/19, train_loss: 0.5076\n",
      "4/19, train_loss: 0.5076\n",
      "5/19, train_loss: 0.5076\n",
      "6/19, train_loss: 0.5076\n",
      "7/19, train_loss: 0.5076\n",
      "8/19, train_loss: 0.5075\n",
      "9/19, train_loss: 0.5075\n",
      "10/19, train_loss: 0.5075\n",
      "11/19, train_loss: 0.5075\n",
      "12/19, train_loss: 0.5075\n",
      "13/19, train_loss: 0.5075\n",
      "14/19, train_loss: 0.5075\n",
      "15/19, train_loss: 0.5075\n",
      "16/19, train_loss: 0.5075\n",
      "17/19, train_loss: 0.5075\n",
      "18/19, train_loss: 0.5075\n",
      "19/19, train_loss: 0.5075\n",
      "Epoch 81 average loss: 0.5075\n",
      "\n",
      "Epoch 82/300\n",
      "1/19, train_loss: 0.5075\n",
      "2/19, train_loss: 0.5074\n",
      "3/19, train_loss: 0.5074\n",
      "4/19, train_loss: 0.5074\n",
      "5/19, train_loss: 0.5074\n",
      "6/19, train_loss: 0.5074\n",
      "7/19, train_loss: 0.5074\n",
      "8/19, train_loss: 0.5074\n",
      "9/19, train_loss: 0.5074\n",
      "10/19, train_loss: 0.5074\n",
      "11/19, train_loss: 0.5074\n",
      "12/19, train_loss: 0.5074\n",
      "13/19, train_loss: 0.5074\n",
      "14/19, train_loss: 0.5074\n",
      "15/19, train_loss: 0.5073\n",
      "16/19, train_loss: 0.5073\n",
      "17/19, train_loss: 0.5073\n",
      "18/19, train_loss: 0.5073\n",
      "19/19, train_loss: 0.5073\n",
      "Epoch 82 average loss: 0.5074\n",
      "\n",
      "Epoch 83/300\n",
      "1/19, train_loss: 0.5073\n",
      "2/19, train_loss: 0.5073\n",
      "3/19, train_loss: 0.5073\n",
      "4/19, train_loss: 0.5073\n",
      "5/19, train_loss: 0.5073\n",
      "6/19, train_loss: 0.5073\n",
      "7/19, train_loss: 0.5073\n",
      "8/19, train_loss: 0.5073\n",
      "9/19, train_loss: 0.5073\n",
      "10/19, train_loss: 0.5072\n",
      "11/19, train_loss: 0.5072\n",
      "12/19, train_loss: 0.5072\n",
      "13/19, train_loss: 0.5072\n",
      "14/19, train_loss: 0.5072\n",
      "15/19, train_loss: 0.5072\n",
      "16/19, train_loss: 0.5072\n",
      "17/19, train_loss: 0.5072\n",
      "18/19, train_loss: 0.5072\n",
      "19/19, train_loss: 0.5072\n",
      "Epoch 83 average loss: 0.5072\n",
      "\n",
      "Epoch 84/300\n",
      "1/19, train_loss: 0.5072\n",
      "2/19, train_loss: 0.5072\n",
      "3/19, train_loss: 0.5072\n",
      "4/19, train_loss: 0.5071\n",
      "5/19, train_loss: 0.5071\n",
      "6/19, train_loss: 0.5071\n",
      "7/19, train_loss: 0.5071\n",
      "8/19, train_loss: 0.5071\n",
      "9/19, train_loss: 0.5071\n",
      "10/19, train_loss: 0.5071\n",
      "11/19, train_loss: 0.5071\n",
      "12/19, train_loss: 0.5071\n",
      "13/19, train_loss: 0.5071\n",
      "14/19, train_loss: 0.5071\n",
      "15/19, train_loss: 0.5071\n",
      "16/19, train_loss: 0.5071\n",
      "17/19, train_loss: 0.5070\n",
      "18/19, train_loss: 0.5071\n",
      "19/19, train_loss: 0.5070\n",
      "Epoch 84 average loss: 0.5071\n",
      "\n",
      "Epoch 85/300\n",
      "1/19, train_loss: 0.5070\n",
      "2/19, train_loss: 0.5070\n",
      "3/19, train_loss: 0.5070\n",
      "4/19, train_loss: 0.5070\n",
      "5/19, train_loss: 0.5070\n",
      "6/19, train_loss: 0.5070\n",
      "7/19, train_loss: 0.5070\n",
      "8/19, train_loss: 0.5070\n",
      "9/19, train_loss: 0.5070\n",
      "10/19, train_loss: 0.5070\n",
      "11/19, train_loss: 0.5070\n",
      "12/19, train_loss: 0.5069\n",
      "13/19, train_loss: 0.5070\n",
      "14/19, train_loss: 0.5069\n",
      "15/19, train_loss: 0.5069\n",
      "16/19, train_loss: 0.5069\n",
      "17/19, train_loss: 0.5069\n",
      "18/19, train_loss: 0.5069\n",
      "19/19, train_loss: 0.5069\n",
      "Epoch 85 average loss: 0.5070\n",
      "Current epoch: 85, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 86/300\n",
      "1/19, train_loss: 0.5069\n",
      "2/19, train_loss: 0.5069\n",
      "3/19, train_loss: 0.5069\n",
      "4/19, train_loss: 0.5069\n",
      "5/19, train_loss: 0.5069\n",
      "6/19, train_loss: 0.5069\n",
      "7/19, train_loss: 0.5068\n",
      "8/19, train_loss: 0.5068\n",
      "9/19, train_loss: 0.5068\n",
      "10/19, train_loss: 0.5068\n",
      "11/19, train_loss: 0.5068\n",
      "12/19, train_loss: 0.5068\n",
      "13/19, train_loss: 0.5068\n",
      "14/19, train_loss: 0.5068\n",
      "15/19, train_loss: 0.5068\n",
      "16/19, train_loss: 0.5068\n",
      "17/19, train_loss: 0.5068\n",
      "18/19, train_loss: 0.5068\n",
      "19/19, train_loss: 0.5068\n",
      "Epoch 86 average loss: 0.5068\n",
      "\n",
      "Epoch 87/300\n",
      "1/19, train_loss: 0.5068\n",
      "2/19, train_loss: 0.5068\n",
      "3/19, train_loss: 0.5068\n",
      "4/19, train_loss: 0.5067\n",
      "5/19, train_loss: 0.5067\n",
      "6/19, train_loss: 0.5067\n",
      "7/19, train_loss: 0.5067\n",
      "8/19, train_loss: 0.5067\n",
      "9/19, train_loss: 0.5067\n",
      "10/19, train_loss: 0.5067\n",
      "11/19, train_loss: 0.5067\n",
      "12/19, train_loss: 0.5067\n",
      "13/19, train_loss: 0.5067\n",
      "14/19, train_loss: 0.5067\n",
      "15/19, train_loss: 0.5067\n",
      "16/19, train_loss: 0.5067\n",
      "17/19, train_loss: 0.5066\n",
      "18/19, train_loss: 0.5067\n",
      "19/19, train_loss: 0.5066\n",
      "Epoch 87 average loss: 0.5067\n",
      "\n",
      "Epoch 88/300\n",
      "1/19, train_loss: 0.5066\n",
      "2/19, train_loss: 0.5066\n",
      "3/19, train_loss: 0.5066\n",
      "4/19, train_loss: 0.5066\n",
      "5/19, train_loss: 0.5066\n",
      "6/19, train_loss: 0.5066\n",
      "7/19, train_loss: 0.5066\n",
      "8/19, train_loss: 0.5066\n",
      "9/19, train_loss: 0.5066\n",
      "10/19, train_loss: 0.5066\n",
      "11/19, train_loss: 0.5066\n",
      "12/19, train_loss: 0.5066\n",
      "13/19, train_loss: 0.5066\n",
      "14/19, train_loss: 0.5065\n",
      "15/19, train_loss: 0.5065\n",
      "16/19, train_loss: 0.5065\n",
      "17/19, train_loss: 0.5065\n",
      "18/19, train_loss: 0.5065\n",
      "19/19, train_loss: 0.5065\n",
      "Epoch 88 average loss: 0.5066\n",
      "\n",
      "Epoch 89/300\n",
      "1/19, train_loss: 0.5065\n",
      "2/19, train_loss: 0.5065\n",
      "3/19, train_loss: 0.5065\n",
      "4/19, train_loss: 0.5065\n",
      "5/19, train_loss: 0.5065\n",
      "6/19, train_loss: 0.5065\n",
      "7/19, train_loss: 0.5065\n",
      "8/19, train_loss: 0.5065\n",
      "9/19, train_loss: 0.5065\n",
      "10/19, train_loss: 0.5064\n",
      "11/19, train_loss: 0.5064\n",
      "12/19, train_loss: 0.5064\n",
      "13/19, train_loss: 0.5064\n",
      "14/19, train_loss: 0.5064\n",
      "15/19, train_loss: 0.5064\n",
      "16/19, train_loss: 0.5064\n",
      "17/19, train_loss: 0.5064\n",
      "18/19, train_loss: 0.5064\n",
      "19/19, train_loss: 0.5064\n",
      "Epoch 89 average loss: 0.5065\n",
      "\n",
      "Epoch 90/300\n",
      "1/19, train_loss: 0.5064\n",
      "2/19, train_loss: 0.5064\n",
      "3/19, train_loss: 0.5064\n",
      "4/19, train_loss: 0.5064\n",
      "5/19, train_loss: 0.5064\n",
      "6/19, train_loss: 0.5064\n",
      "7/19, train_loss: 0.5064\n",
      "8/19, train_loss: 0.5063\n",
      "9/19, train_loss: 0.5063\n",
      "10/19, train_loss: 0.5063\n",
      "11/19, train_loss: 0.5063\n",
      "12/19, train_loss: 0.5063\n",
      "13/19, train_loss: 0.5063\n",
      "14/19, train_loss: 0.5063\n",
      "15/19, train_loss: 0.5063\n",
      "16/19, train_loss: 0.5063\n",
      "17/19, train_loss: 0.5063\n",
      "18/19, train_loss: 0.5063\n",
      "19/19, train_loss: 0.5063\n",
      "Epoch 90 average loss: 0.5063\n",
      "Current epoch: 90, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 91/300\n",
      "1/19, train_loss: 0.5063\n",
      "2/19, train_loss: 0.5063\n",
      "3/19, train_loss: 0.5063\n",
      "4/19, train_loss: 0.5063\n",
      "5/19, train_loss: 0.5063\n",
      "6/19, train_loss: 0.5062\n",
      "7/19, train_loss: 0.5062\n",
      "8/19, train_loss: 0.5062\n",
      "9/19, train_loss: 0.5062\n",
      "10/19, train_loss: 0.5062\n",
      "11/19, train_loss: 0.5062\n",
      "12/19, train_loss: 0.5062\n",
      "13/19, train_loss: 0.5062\n",
      "14/19, train_loss: 0.5062\n",
      "15/19, train_loss: 0.5062\n",
      "16/19, train_loss: 0.5062\n",
      "17/19, train_loss: 0.5062\n",
      "18/19, train_loss: 0.5062\n",
      "19/19, train_loss: 0.5062\n",
      "Epoch 91 average loss: 0.5062\n",
      "\n",
      "Epoch 92/300\n",
      "1/19, train_loss: 0.5062\n",
      "2/19, train_loss: 0.5062\n",
      "3/19, train_loss: 0.5062\n",
      "4/19, train_loss: 0.5061\n",
      "5/19, train_loss: 0.5061\n",
      "6/19, train_loss: 0.5061\n",
      "7/19, train_loss: 0.5061\n",
      "8/19, train_loss: 0.5061\n",
      "9/19, train_loss: 0.5061\n",
      "10/19, train_loss: 0.5061\n",
      "11/19, train_loss: 0.5061\n",
      "12/19, train_loss: 0.5061\n",
      "13/19, train_loss: 0.5061\n",
      "14/19, train_loss: 0.5061\n",
      "15/19, train_loss: 0.5061\n",
      "16/19, train_loss: 0.5061\n",
      "17/19, train_loss: 0.5061\n",
      "18/19, train_loss: 0.5060\n",
      "19/19, train_loss: 0.5060\n",
      "Epoch 92 average loss: 0.5061\n",
      "\n",
      "Epoch 93/300\n",
      "1/19, train_loss: 0.5061\n",
      "2/19, train_loss: 0.5060\n",
      "3/19, train_loss: 0.5060\n",
      "4/19, train_loss: 0.5060\n",
      "5/19, train_loss: 0.5060\n",
      "6/19, train_loss: 0.5060\n",
      "7/19, train_loss: 0.5060\n",
      "8/19, train_loss: 0.5060\n",
      "9/19, train_loss: 0.5060\n",
      "10/19, train_loss: 0.5060\n",
      "11/19, train_loss: 0.5060\n",
      "12/19, train_loss: 0.5060\n",
      "13/19, train_loss: 0.5060\n",
      "14/19, train_loss: 0.5060\n",
      "15/19, train_loss: 0.5060\n",
      "16/19, train_loss: 0.5060\n",
      "17/19, train_loss: 0.5060\n",
      "18/19, train_loss: 0.5059\n",
      "19/19, train_loss: 0.5059\n",
      "Epoch 93 average loss: 0.5060\n",
      "\n",
      "Epoch 94/300\n",
      "1/19, train_loss: 0.5059\n",
      "2/19, train_loss: 0.5059\n",
      "3/19, train_loss: 0.5059\n",
      "4/19, train_loss: 0.5059\n",
      "5/19, train_loss: 0.5059\n",
      "6/19, train_loss: 0.5059\n",
      "7/19, train_loss: 0.5059\n",
      "8/19, train_loss: 0.5059\n",
      "9/19, train_loss: 0.5059\n",
      "10/19, train_loss: 0.5059\n",
      "11/19, train_loss: 0.5059\n",
      "12/19, train_loss: 0.5059\n",
      "13/19, train_loss: 0.5059\n",
      "14/19, train_loss: 0.5059\n",
      "15/19, train_loss: 0.5059\n",
      "16/19, train_loss: 0.5059\n",
      "17/19, train_loss: 0.5058\n",
      "18/19, train_loss: 0.5058\n",
      "19/19, train_loss: 0.5058\n",
      "Epoch 94 average loss: 0.5059\n",
      "\n",
      "Epoch 95/300\n",
      "1/19, train_loss: 0.5058\n",
      "2/19, train_loss: 0.5058\n",
      "3/19, train_loss: 0.5058\n",
      "4/19, train_loss: 0.5058\n",
      "5/19, train_loss: 0.5058\n",
      "6/19, train_loss: 0.5058\n",
      "7/19, train_loss: 0.5058\n",
      "8/19, train_loss: 0.5058\n",
      "9/19, train_loss: 0.5058\n",
      "10/19, train_loss: 0.5058\n",
      "11/19, train_loss: 0.5058\n",
      "12/19, train_loss: 0.5058\n",
      "13/19, train_loss: 0.5058\n",
      "14/19, train_loss: 0.5058\n",
      "15/19, train_loss: 0.5057\n",
      "16/19, train_loss: 0.5057\n",
      "17/19, train_loss: 0.5057\n",
      "18/19, train_loss: 0.5057\n",
      "19/19, train_loss: 0.5057\n",
      "Epoch 95 average loss: 0.5058\n",
      "Current epoch: 95, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 96/300\n",
      "1/19, train_loss: 0.5057\n",
      "2/19, train_loss: 0.5057\n",
      "3/19, train_loss: 0.5057\n",
      "4/19, train_loss: 0.5057\n",
      "5/19, train_loss: 0.5057\n",
      "6/19, train_loss: 0.5057\n",
      "7/19, train_loss: 0.5057\n",
      "8/19, train_loss: 0.5057\n",
      "9/19, train_loss: 0.5057\n",
      "10/19, train_loss: 0.5057\n",
      "11/19, train_loss: 0.5057\n",
      "12/19, train_loss: 0.5057\n",
      "13/19, train_loss: 0.5057\n",
      "14/19, train_loss: 0.5057\n",
      "15/19, train_loss: 0.5056\n",
      "16/19, train_loss: 0.5056\n",
      "17/19, train_loss: 0.5056\n",
      "18/19, train_loss: 0.5056\n",
      "19/19, train_loss: 0.5056\n",
      "Epoch 96 average loss: 0.5057\n",
      "\n",
      "Epoch 97/300\n",
      "1/19, train_loss: 0.5056\n",
      "2/19, train_loss: 0.5056\n",
      "3/19, train_loss: 0.5056\n",
      "4/19, train_loss: 0.5056\n",
      "5/19, train_loss: 0.5056\n",
      "6/19, train_loss: 0.5056\n",
      "7/19, train_loss: 0.5056\n",
      "8/19, train_loss: 0.5056\n",
      "9/19, train_loss: 0.5056\n",
      "10/19, train_loss: 0.5056\n",
      "11/19, train_loss: 0.5056\n",
      "12/19, train_loss: 0.5056\n",
      "13/19, train_loss: 0.5056\n",
      "14/19, train_loss: 0.5056\n",
      "15/19, train_loss: 0.5055\n",
      "16/19, train_loss: 0.5055\n",
      "17/19, train_loss: 0.5055\n",
      "18/19, train_loss: 0.5055\n",
      "19/19, train_loss: 0.5056\n",
      "Epoch 97 average loss: 0.5056\n",
      "\n",
      "Epoch 98/300\n",
      "1/19, train_loss: 0.5055\n",
      "2/19, train_loss: 0.5055\n",
      "3/19, train_loss: 0.5055\n",
      "4/19, train_loss: 0.5055\n",
      "5/19, train_loss: 0.5055\n",
      "6/19, train_loss: 0.5055\n",
      "7/19, train_loss: 0.5055\n",
      "8/19, train_loss: 0.5055\n",
      "9/19, train_loss: 0.5055\n",
      "10/19, train_loss: 0.5055\n",
      "11/19, train_loss: 0.5055\n",
      "12/19, train_loss: 0.5055\n",
      "13/19, train_loss: 0.5055\n",
      "14/19, train_loss: 0.5055\n",
      "15/19, train_loss: 0.5054\n",
      "16/19, train_loss: 0.5054\n",
      "17/19, train_loss: 0.5054\n",
      "18/19, train_loss: 0.5054\n",
      "19/19, train_loss: 0.5054\n",
      "Epoch 98 average loss: 0.5055\n",
      "\n",
      "Epoch 99/300\n",
      "1/19, train_loss: 0.5054\n",
      "2/19, train_loss: 0.5054\n",
      "3/19, train_loss: 0.5054\n",
      "4/19, train_loss: 0.5054\n",
      "5/19, train_loss: 0.5054\n",
      "6/19, train_loss: 0.5054\n",
      "7/19, train_loss: 0.5054\n",
      "8/19, train_loss: 0.5054\n",
      "9/19, train_loss: 0.5054\n",
      "10/19, train_loss: 0.5054\n",
      "11/19, train_loss: 0.5054\n",
      "12/19, train_loss: 0.5054\n",
      "13/19, train_loss: 0.5054\n",
      "14/19, train_loss: 0.5054\n",
      "15/19, train_loss: 0.5054\n",
      "16/19, train_loss: 0.5053\n",
      "17/19, train_loss: 0.5053\n",
      "18/19, train_loss: 0.5053\n",
      "19/19, train_loss: 0.5053\n",
      "Epoch 99 average loss: 0.5054\n",
      "\n",
      "Epoch 100/300\n",
      "1/19, train_loss: 0.5053\n",
      "2/19, train_loss: 0.5053\n",
      "3/19, train_loss: 0.5053\n",
      "4/19, train_loss: 0.5053\n",
      "5/19, train_loss: 0.5053\n",
      "6/19, train_loss: 0.5053\n",
      "7/19, train_loss: 0.5053\n",
      "8/19, train_loss: 0.5053\n",
      "9/19, train_loss: 0.5053\n",
      "10/19, train_loss: 0.5053\n",
      "11/19, train_loss: 0.5053\n",
      "12/19, train_loss: 0.5053\n",
      "13/19, train_loss: 0.5053\n",
      "14/19, train_loss: 0.5053\n",
      "15/19, train_loss: 0.5053\n",
      "16/19, train_loss: 0.5053\n",
      "17/19, train_loss: 0.5053\n",
      "18/19, train_loss: 0.5052\n",
      "19/19, train_loss: 0.5052\n",
      "Epoch 100 average loss: 0.5053\n",
      "Current epoch: 100, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 101/300\n",
      "1/19, train_loss: 0.5052\n",
      "2/19, train_loss: 0.5052\n",
      "3/19, train_loss: 0.5052\n",
      "4/19, train_loss: 0.5052\n",
      "5/19, train_loss: 0.5052\n",
      "6/19, train_loss: 0.5052\n",
      "7/19, train_loss: 0.5052\n",
      "8/19, train_loss: 0.5052\n",
      "9/19, train_loss: 0.5052\n",
      "10/19, train_loss: 0.5052\n",
      "11/19, train_loss: 0.5052\n",
      "12/19, train_loss: 0.5052\n",
      "13/19, train_loss: 0.5052\n",
      "14/19, train_loss: 0.5052\n",
      "15/19, train_loss: 0.5052\n",
      "16/19, train_loss: 0.5052\n",
      "17/19, train_loss: 0.5052\n",
      "18/19, train_loss: 0.5052\n",
      "19/19, train_loss: 0.5052\n",
      "Epoch 101 average loss: 0.5052\n",
      "\n",
      "Epoch 102/300\n",
      "1/19, train_loss: 0.5052\n",
      "2/19, train_loss: 0.5051\n",
      "3/19, train_loss: 0.5051\n",
      "4/19, train_loss: 0.5051\n",
      "5/19, train_loss: 0.5051\n",
      "6/19, train_loss: 0.5051\n",
      "7/19, train_loss: 0.5051\n",
      "8/19, train_loss: 0.5051\n",
      "9/19, train_loss: 0.5051\n",
      "10/19, train_loss: 0.5051\n",
      "11/19, train_loss: 0.5051\n",
      "12/19, train_loss: 0.5051\n",
      "13/19, train_loss: 0.5051\n",
      "14/19, train_loss: 0.5051\n",
      "15/19, train_loss: 0.5051\n",
      "16/19, train_loss: 0.5051\n",
      "17/19, train_loss: 0.5051\n",
      "18/19, train_loss: 0.5051\n",
      "19/19, train_loss: 0.5051\n",
      "Epoch 102 average loss: 0.5051\n",
      "\n",
      "Epoch 103/300\n",
      "1/19, train_loss: 0.5051\n",
      "2/19, train_loss: 0.5051\n",
      "3/19, train_loss: 0.5051\n",
      "4/19, train_loss: 0.5050\n",
      "5/19, train_loss: 0.5051\n",
      "6/19, train_loss: 0.5050\n",
      "7/19, train_loss: 0.5050\n",
      "8/19, train_loss: 0.5050\n",
      "9/19, train_loss: 0.5050\n",
      "10/19, train_loss: 0.5050\n",
      "11/19, train_loss: 0.5050\n",
      "12/19, train_loss: 0.5050\n",
      "13/19, train_loss: 0.5050\n",
      "14/19, train_loss: 0.5050\n",
      "15/19, train_loss: 0.5050\n",
      "16/19, train_loss: 0.5050\n",
      "17/19, train_loss: 0.5050\n",
      "18/19, train_loss: 0.5050\n",
      "19/19, train_loss: 0.5050\n",
      "Epoch 103 average loss: 0.5050\n",
      "\n",
      "Epoch 104/300\n",
      "1/19, train_loss: 0.5050\n",
      "2/19, train_loss: 0.5050\n",
      "3/19, train_loss: 0.5050\n",
      "4/19, train_loss: 0.5050\n",
      "5/19, train_loss: 0.5050\n",
      "6/19, train_loss: 0.5050\n",
      "7/19, train_loss: 0.5049\n",
      "8/19, train_loss: 0.5050\n",
      "9/19, train_loss: 0.5049\n",
      "10/19, train_loss: 0.5049\n",
      "11/19, train_loss: 0.5049\n",
      "12/19, train_loss: 0.5049\n",
      "13/19, train_loss: 0.5049\n",
      "14/19, train_loss: 0.5049\n",
      "15/19, train_loss: 0.5049\n",
      "16/19, train_loss: 0.5049\n",
      "17/19, train_loss: 0.5049\n",
      "18/19, train_loss: 0.5049\n",
      "19/19, train_loss: 0.5049\n",
      "Epoch 104 average loss: 0.5049\n",
      "\n",
      "Epoch 105/300\n",
      "1/19, train_loss: 0.5049\n",
      "2/19, train_loss: 0.5049\n",
      "3/19, train_loss: 0.5049\n",
      "4/19, train_loss: 0.5049\n",
      "5/19, train_loss: 0.5049\n",
      "6/19, train_loss: 0.5049\n",
      "7/19, train_loss: 0.5049\n",
      "8/19, train_loss: 0.5049\n",
      "9/19, train_loss: 0.5049\n",
      "10/19, train_loss: 0.5049\n",
      "11/19, train_loss: 0.5048\n",
      "12/19, train_loss: 0.5049\n",
      "13/19, train_loss: 0.5048\n",
      "14/19, train_loss: 0.5048\n",
      "15/19, train_loss: 0.5048\n",
      "16/19, train_loss: 0.5048\n",
      "17/19, train_loss: 0.5048\n",
      "18/19, train_loss: 0.5048\n",
      "19/19, train_loss: 0.5048\n",
      "Epoch 105 average loss: 0.5049\n",
      "Current epoch: 105, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 106/300\n",
      "1/19, train_loss: 0.5048\n",
      "2/19, train_loss: 0.5048\n",
      "3/19, train_loss: 0.5048\n",
      "4/19, train_loss: 0.5048\n",
      "5/19, train_loss: 0.5048\n",
      "6/19, train_loss: 0.5048\n",
      "7/19, train_loss: 0.5048\n",
      "8/19, train_loss: 0.5048\n",
      "9/19, train_loss: 0.5048\n",
      "10/19, train_loss: 0.5048\n",
      "11/19, train_loss: 0.5048\n",
      "12/19, train_loss: 0.5048\n",
      "13/19, train_loss: 0.5048\n",
      "14/19, train_loss: 0.5048\n",
      "15/19, train_loss: 0.5048\n",
      "16/19, train_loss: 0.5048\n",
      "17/19, train_loss: 0.5047\n",
      "18/19, train_loss: 0.5047\n",
      "19/19, train_loss: 0.5047\n",
      "Epoch 106 average loss: 0.5048\n",
      "\n",
      "Epoch 107/300\n",
      "1/19, train_loss: 0.5047\n",
      "2/19, train_loss: 0.5047\n",
      "3/19, train_loss: 0.5047\n",
      "4/19, train_loss: 0.5047\n",
      "5/19, train_loss: 0.5047\n",
      "6/19, train_loss: 0.5047\n",
      "7/19, train_loss: 0.5047\n",
      "8/19, train_loss: 0.5047\n",
      "9/19, train_loss: 0.5047\n",
      "10/19, train_loss: 0.5047\n",
      "11/19, train_loss: 0.5047\n",
      "12/19, train_loss: 0.5047\n",
      "13/19, train_loss: 0.5047\n",
      "14/19, train_loss: 0.5047\n",
      "15/19, train_loss: 0.5047\n",
      "16/19, train_loss: 0.5047\n",
      "17/19, train_loss: 0.5047\n",
      "18/19, train_loss: 0.5047\n",
      "19/19, train_loss: 0.5047\n",
      "Epoch 107 average loss: 0.5047\n",
      "\n",
      "Epoch 108/300\n",
      "1/19, train_loss: 0.5047\n",
      "2/19, train_loss: 0.5046\n",
      "3/19, train_loss: 0.5046\n",
      "4/19, train_loss: 0.5046\n",
      "5/19, train_loss: 0.5046\n",
      "6/19, train_loss: 0.5046\n",
      "7/19, train_loss: 0.5046\n",
      "8/19, train_loss: 0.5046\n",
      "9/19, train_loss: 0.5046\n",
      "10/19, train_loss: 0.5046\n",
      "11/19, train_loss: 0.5046\n",
      "12/19, train_loss: 0.5046\n",
      "13/19, train_loss: 0.5046\n",
      "14/19, train_loss: 0.5046\n",
      "15/19, train_loss: 0.5046\n",
      "16/19, train_loss: 0.5046\n",
      "17/19, train_loss: 0.5046\n",
      "18/19, train_loss: 0.5046\n",
      "19/19, train_loss: 0.5046\n",
      "Epoch 108 average loss: 0.5046\n",
      "\n",
      "Epoch 109/300\n",
      "1/19, train_loss: 0.5046\n",
      "2/19, train_loss: 0.5046\n",
      "3/19, train_loss: 0.5046\n",
      "4/19, train_loss: 0.5046\n",
      "5/19, train_loss: 0.5046\n",
      "6/19, train_loss: 0.5046\n",
      "7/19, train_loss: 0.5046\n",
      "8/19, train_loss: 0.5045\n",
      "9/19, train_loss: 0.5045\n",
      "10/19, train_loss: 0.5045\n",
      "11/19, train_loss: 0.5045\n",
      "12/19, train_loss: 0.5045\n",
      "13/19, train_loss: 0.5045\n",
      "14/19, train_loss: 0.5045\n",
      "15/19, train_loss: 0.5045\n",
      "16/19, train_loss: 0.5045\n",
      "17/19, train_loss: 0.5045\n",
      "18/19, train_loss: 0.5045\n",
      "19/19, train_loss: 0.5045\n",
      "Epoch 109 average loss: 0.5045\n",
      "\n",
      "Epoch 110/300\n",
      "1/19, train_loss: 0.5045\n",
      "2/19, train_loss: 0.5045\n",
      "3/19, train_loss: 0.5045\n",
      "4/19, train_loss: 0.5045\n",
      "5/19, train_loss: 0.5045\n",
      "6/19, train_loss: 0.5045\n",
      "7/19, train_loss: 0.5045\n",
      "8/19, train_loss: 0.5045\n",
      "9/19, train_loss: 0.5045\n",
      "10/19, train_loss: 0.5045\n",
      "11/19, train_loss: 0.5045\n",
      "12/19, train_loss: 0.5045\n",
      "13/19, train_loss: 0.5045\n",
      "14/19, train_loss: 0.5044\n",
      "15/19, train_loss: 0.5044\n",
      "16/19, train_loss: 0.5044\n",
      "17/19, train_loss: 0.5044\n",
      "18/19, train_loss: 0.5044\n",
      "19/19, train_loss: 0.5044\n",
      "Epoch 110 average loss: 0.5045\n",
      "Current epoch: 110, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 111/300\n",
      "1/19, train_loss: 0.5044\n",
      "2/19, train_loss: 0.5044\n",
      "3/19, train_loss: 0.5044\n",
      "4/19, train_loss: 0.5044\n",
      "5/19, train_loss: 0.5044\n",
      "6/19, train_loss: 0.5044\n",
      "7/19, train_loss: 0.5044\n",
      "8/19, train_loss: 0.5044\n",
      "9/19, train_loss: 0.5044\n",
      "10/19, train_loss: 0.5044\n",
      "11/19, train_loss: 0.5044\n",
      "12/19, train_loss: 0.5044\n",
      "13/19, train_loss: 0.5044\n",
      "14/19, train_loss: 0.5044\n",
      "15/19, train_loss: 0.5044\n",
      "16/19, train_loss: 0.5044\n",
      "17/19, train_loss: 0.5044\n",
      "18/19, train_loss: 0.5044\n",
      "19/19, train_loss: 0.5044\n",
      "Epoch 111 average loss: 0.5044\n",
      "\n",
      "Epoch 112/300\n",
      "1/19, train_loss: 0.5043\n",
      "2/19, train_loss: 0.5043\n",
      "3/19, train_loss: 0.5043\n",
      "4/19, train_loss: 0.5043\n",
      "5/19, train_loss: 0.5043\n",
      "6/19, train_loss: 0.5043\n",
      "7/19, train_loss: 0.5043\n",
      "8/19, train_loss: 0.5043\n",
      "9/19, train_loss: 0.5043\n",
      "10/19, train_loss: 0.5043\n",
      "11/19, train_loss: 0.5043\n",
      "12/19, train_loss: 0.5043\n",
      "13/19, train_loss: 0.5043\n",
      "14/19, train_loss: 0.5043\n",
      "15/19, train_loss: 0.5043\n",
      "16/19, train_loss: 0.5043\n",
      "17/19, train_loss: 0.5043\n",
      "18/19, train_loss: 0.5043\n",
      "19/19, train_loss: 0.5043\n",
      "Epoch 112 average loss: 0.5043\n",
      "\n",
      "Epoch 113/300\n",
      "1/19, train_loss: 0.5043\n",
      "2/19, train_loss: 0.5043\n",
      "3/19, train_loss: 0.5043\n",
      "4/19, train_loss: 0.5043\n",
      "5/19, train_loss: 0.5043\n",
      "6/19, train_loss: 0.5043\n",
      "7/19, train_loss: 0.5043\n",
      "8/19, train_loss: 0.5043\n",
      "9/19, train_loss: 0.5043\n",
      "10/19, train_loss: 0.5042\n",
      "11/19, train_loss: 0.5042\n",
      "12/19, train_loss: 0.5042\n",
      "13/19, train_loss: 0.5042\n",
      "14/19, train_loss: 0.5042\n",
      "15/19, train_loss: 0.5042\n",
      "16/19, train_loss: 0.5042\n",
      "17/19, train_loss: 0.5042\n",
      "18/19, train_loss: 0.5042\n",
      "19/19, train_loss: 0.5042\n",
      "Epoch 113 average loss: 0.5042\n",
      "\n",
      "Epoch 114/300\n",
      "1/19, train_loss: 0.5042\n",
      "2/19, train_loss: 0.5042\n",
      "3/19, train_loss: 0.5042\n",
      "4/19, train_loss: 0.5042\n",
      "5/19, train_loss: 0.5042\n",
      "6/19, train_loss: 0.5042\n",
      "7/19, train_loss: 0.5042\n",
      "8/19, train_loss: 0.5042\n",
      "9/19, train_loss: 0.5042\n",
      "10/19, train_loss: 0.5042\n",
      "11/19, train_loss: 0.5042\n",
      "12/19, train_loss: 0.5042\n",
      "13/19, train_loss: 0.5042\n",
      "14/19, train_loss: 0.5042\n",
      "15/19, train_loss: 0.5042\n",
      "16/19, train_loss: 0.5042\n",
      "17/19, train_loss: 0.5042\n",
      "18/19, train_loss: 0.5042\n",
      "19/19, train_loss: 0.5041\n",
      "Epoch 114 average loss: 0.5042\n",
      "\n",
      "Epoch 115/300\n",
      "1/19, train_loss: 0.5041\n",
      "2/19, train_loss: 0.5041\n",
      "3/19, train_loss: 0.5041\n",
      "4/19, train_loss: 0.5041\n",
      "5/19, train_loss: 0.5041\n",
      "6/19, train_loss: 0.5041\n",
      "7/19, train_loss: 0.5041\n",
      "8/19, train_loss: 0.5041\n",
      "9/19, train_loss: 0.5041\n",
      "10/19, train_loss: 0.5041\n",
      "11/19, train_loss: 0.5041\n",
      "12/19, train_loss: 0.5041\n",
      "13/19, train_loss: 0.5041\n",
      "14/19, train_loss: 0.5041\n",
      "15/19, train_loss: 0.5041\n",
      "16/19, train_loss: 0.5041\n",
      "17/19, train_loss: 0.5041\n",
      "18/19, train_loss: 0.5041\n",
      "19/19, train_loss: 0.5041\n",
      "Epoch 115 average loss: 0.5041\n",
      "Current epoch: 115, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 116/300\n",
      "1/19, train_loss: 0.5041\n",
      "2/19, train_loss: 0.5041\n",
      "3/19, train_loss: 0.5041\n",
      "4/19, train_loss: 0.5041\n",
      "5/19, train_loss: 0.5041\n",
      "6/19, train_loss: 0.5041\n",
      "7/19, train_loss: 0.5041\n",
      "8/19, train_loss: 0.5040\n",
      "9/19, train_loss: 0.5040\n",
      "10/19, train_loss: 0.5040\n",
      "11/19, train_loss: 0.5040\n",
      "12/19, train_loss: 0.5040\n",
      "13/19, train_loss: 0.5040\n",
      "14/19, train_loss: 0.5040\n",
      "15/19, train_loss: 0.5040\n",
      "16/19, train_loss: 0.5040\n",
      "17/19, train_loss: 0.5040\n",
      "18/19, train_loss: 0.5040\n",
      "19/19, train_loss: 0.5040\n",
      "Epoch 116 average loss: 0.5040\n",
      "\n",
      "Epoch 117/300\n",
      "1/19, train_loss: 0.5040\n",
      "2/19, train_loss: 0.5040\n",
      "3/19, train_loss: 0.5040\n",
      "4/19, train_loss: 0.5040\n",
      "5/19, train_loss: 0.5040\n",
      "6/19, train_loss: 0.5040\n",
      "7/19, train_loss: 0.5040\n",
      "8/19, train_loss: 0.5040\n",
      "9/19, train_loss: 0.5040\n",
      "10/19, train_loss: 0.5040\n",
      "11/19, train_loss: 0.5040\n",
      "12/19, train_loss: 0.5040\n",
      "13/19, train_loss: 0.5040\n",
      "14/19, train_loss: 0.5040\n",
      "15/19, train_loss: 0.5040\n",
      "16/19, train_loss: 0.5040\n",
      "17/19, train_loss: 0.5040\n",
      "18/19, train_loss: 0.5040\n",
      "19/19, train_loss: 0.5039\n",
      "Epoch 117 average loss: 0.5040\n",
      "\n",
      "Epoch 118/300\n",
      "1/19, train_loss: 0.5039\n",
      "2/19, train_loss: 0.5039\n",
      "3/19, train_loss: 0.5039\n",
      "4/19, train_loss: 0.5039\n",
      "5/19, train_loss: 0.5039\n",
      "6/19, train_loss: 0.5039\n",
      "7/19, train_loss: 0.5039\n",
      "8/19, train_loss: 0.5039\n",
      "9/19, train_loss: 0.5039\n",
      "10/19, train_loss: 0.5039\n",
      "11/19, train_loss: 0.5039\n",
      "12/19, train_loss: 0.5039\n",
      "13/19, train_loss: 0.5039\n",
      "14/19, train_loss: 0.5039\n",
      "15/19, train_loss: 0.5039\n",
      "16/19, train_loss: 0.5039\n",
      "17/19, train_loss: 0.5039\n",
      "18/19, train_loss: 0.5039\n",
      "19/19, train_loss: 0.5039\n",
      "Epoch 118 average loss: 0.5039\n",
      "\n",
      "Epoch 119/300\n",
      "1/19, train_loss: 0.5039\n",
      "2/19, train_loss: 0.5039\n",
      "3/19, train_loss: 0.5039\n",
      "4/19, train_loss: 0.5039\n",
      "5/19, train_loss: 0.5039\n",
      "6/19, train_loss: 0.5039\n",
      "7/19, train_loss: 0.5039\n",
      "8/19, train_loss: 0.5039\n",
      "9/19, train_loss: 0.5039\n",
      "10/19, train_loss: 0.5039\n",
      "11/19, train_loss: 0.5039\n",
      "12/19, train_loss: 0.5039\n",
      "13/19, train_loss: 0.5038\n",
      "14/19, train_loss: 0.5038\n",
      "15/19, train_loss: 0.5038\n",
      "16/19, train_loss: 0.5038\n",
      "17/19, train_loss: 0.5038\n",
      "18/19, train_loss: 0.5038\n",
      "19/19, train_loss: 0.5038\n",
      "Epoch 119 average loss: 0.5039\n",
      "\n",
      "Epoch 120/300\n",
      "1/19, train_loss: 0.5038\n",
      "2/19, train_loss: 0.5038\n",
      "3/19, train_loss: 0.5038\n",
      "4/19, train_loss: 0.5038\n",
      "5/19, train_loss: 0.5038\n",
      "6/19, train_loss: 0.5038\n",
      "7/19, train_loss: 0.5038\n",
      "8/19, train_loss: 0.5038\n",
      "9/19, train_loss: 0.5038\n",
      "10/19, train_loss: 0.5038\n",
      "11/19, train_loss: 0.5038\n",
      "12/19, train_loss: 0.5038\n",
      "13/19, train_loss: 0.5038\n",
      "14/19, train_loss: 0.5038\n",
      "15/19, train_loss: 0.5038\n",
      "16/19, train_loss: 0.5038\n",
      "17/19, train_loss: 0.5038\n",
      "18/19, train_loss: 0.5038\n",
      "19/19, train_loss: 0.5038\n",
      "Epoch 120 average loss: 0.5038\n",
      "Current epoch: 120, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 121/300\n",
      "1/19, train_loss: 0.5038\n",
      "2/19, train_loss: 0.5038\n",
      "3/19, train_loss: 0.5038\n",
      "4/19, train_loss: 0.5038\n",
      "5/19, train_loss: 0.5037\n",
      "6/19, train_loss: 0.5037\n",
      "7/19, train_loss: 0.5037\n",
      "8/19, train_loss: 0.5037\n",
      "9/19, train_loss: 0.5037\n",
      "10/19, train_loss: 0.5037\n",
      "11/19, train_loss: 0.5037\n",
      "12/19, train_loss: 0.5037\n",
      "13/19, train_loss: 0.5037\n",
      "14/19, train_loss: 0.5037\n",
      "15/19, train_loss: 0.5037\n",
      "16/19, train_loss: 0.5037\n",
      "17/19, train_loss: 0.5037\n",
      "18/19, train_loss: 0.5037\n",
      "19/19, train_loss: 0.5037\n",
      "Epoch 121 average loss: 0.5037\n",
      "\n",
      "Epoch 122/300\n",
      "1/19, train_loss: 0.5037\n",
      "2/19, train_loss: 0.5037\n",
      "3/19, train_loss: 0.5037\n",
      "4/19, train_loss: 0.5037\n",
      "5/19, train_loss: 0.5037\n",
      "6/19, train_loss: 0.5037\n",
      "7/19, train_loss: 0.5037\n",
      "8/19, train_loss: 0.5037\n",
      "9/19, train_loss: 0.5037\n",
      "10/19, train_loss: 0.5037\n",
      "11/19, train_loss: 0.5037\n",
      "12/19, train_loss: 0.5037\n",
      "13/19, train_loss: 0.5037\n",
      "14/19, train_loss: 0.5037\n",
      "15/19, train_loss: 0.5037\n",
      "16/19, train_loss: 0.5037\n",
      "17/19, train_loss: 0.5036\n",
      "18/19, train_loss: 0.5036\n",
      "19/19, train_loss: 0.5036\n",
      "Epoch 122 average loss: 0.5037\n",
      "\n",
      "Epoch 123/300\n",
      "1/19, train_loss: 0.5036\n",
      "2/19, train_loss: 0.5036\n",
      "3/19, train_loss: 0.5036\n",
      "4/19, train_loss: 0.5036\n",
      "5/19, train_loss: 0.5036\n",
      "6/19, train_loss: 0.5036\n",
      "7/19, train_loss: 0.5036\n",
      "8/19, train_loss: 0.5036\n",
      "9/19, train_loss: 0.5036\n",
      "10/19, train_loss: 0.5036\n",
      "11/19, train_loss: 0.5036\n",
      "12/19, train_loss: 0.5036\n",
      "13/19, train_loss: 0.5036\n",
      "14/19, train_loss: 0.5036\n",
      "15/19, train_loss: 0.5036\n",
      "16/19, train_loss: 0.5036\n",
      "17/19, train_loss: 0.5036\n",
      "18/19, train_loss: 0.5036\n",
      "19/19, train_loss: 0.5036\n",
      "Epoch 123 average loss: 0.5036\n",
      "\n",
      "Epoch 124/300\n",
      "1/19, train_loss: 0.5036\n",
      "2/19, train_loss: 0.5036\n",
      "3/19, train_loss: 0.5036\n",
      "4/19, train_loss: 0.5036\n",
      "5/19, train_loss: 0.5036\n",
      "6/19, train_loss: 0.5036\n",
      "7/19, train_loss: 0.5036\n",
      "8/19, train_loss: 0.5036\n",
      "9/19, train_loss: 0.5036\n",
      "10/19, train_loss: 0.5036\n",
      "11/19, train_loss: 0.5035\n",
      "12/19, train_loss: 0.5035\n",
      "13/19, train_loss: 0.5035\n",
      "14/19, train_loss: 0.5035\n",
      "15/19, train_loss: 0.5035\n",
      "16/19, train_loss: 0.5035\n",
      "17/19, train_loss: 0.5035\n",
      "18/19, train_loss: 0.5035\n",
      "19/19, train_loss: 0.5035\n",
      "Epoch 124 average loss: 0.5036\n",
      "\n",
      "Epoch 125/300\n",
      "1/19, train_loss: 0.5035\n",
      "2/19, train_loss: 0.5035\n",
      "3/19, train_loss: 0.5035\n",
      "4/19, train_loss: 0.5035\n",
      "5/19, train_loss: 0.5035\n",
      "6/19, train_loss: 0.5035\n",
      "7/19, train_loss: 0.5035\n",
      "8/19, train_loss: 0.5035\n",
      "9/19, train_loss: 0.5035\n",
      "10/19, train_loss: 0.5035\n",
      "11/19, train_loss: 0.5035\n",
      "12/19, train_loss: 0.5035\n",
      "13/19, train_loss: 0.5035\n",
      "14/19, train_loss: 0.5035\n",
      "15/19, train_loss: 0.5035\n",
      "16/19, train_loss: 0.5035\n",
      "17/19, train_loss: 0.5035\n",
      "18/19, train_loss: 0.5035\n",
      "19/19, train_loss: 0.5035\n",
      "Epoch 125 average loss: 0.5035\n",
      "Current epoch: 125, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 126/300\n",
      "1/19, train_loss: 0.5035\n",
      "2/19, train_loss: 0.5035\n",
      "3/19, train_loss: 0.5035\n",
      "4/19, train_loss: 0.5035\n",
      "5/19, train_loss: 0.5035\n",
      "6/19, train_loss: 0.5035\n",
      "7/19, train_loss: 0.5035\n",
      "8/19, train_loss: 0.5034\n",
      "9/19, train_loss: 0.5034\n",
      "10/19, train_loss: 0.5034\n",
      "11/19, train_loss: 0.5034\n",
      "12/19, train_loss: 0.5034\n",
      "13/19, train_loss: 0.5034\n",
      "14/19, train_loss: 0.5034\n",
      "15/19, train_loss: 0.5034\n",
      "16/19, train_loss: 0.5034\n",
      "17/19, train_loss: 0.5034\n",
      "18/19, train_loss: 0.5034\n",
      "19/19, train_loss: 0.5034\n",
      "Epoch 126 average loss: 0.5034\n",
      "\n",
      "Epoch 127/300\n",
      "1/19, train_loss: 0.5034\n",
      "2/19, train_loss: 0.5034\n",
      "3/19, train_loss: 0.5034\n",
      "4/19, train_loss: 0.5034\n",
      "5/19, train_loss: 0.5034\n",
      "6/19, train_loss: 0.5034\n",
      "7/19, train_loss: 0.5034\n",
      "8/19, train_loss: 0.5034\n",
      "9/19, train_loss: 0.5034\n",
      "10/19, train_loss: 0.5034\n",
      "11/19, train_loss: 0.5034\n",
      "12/19, train_loss: 0.5034\n",
      "13/19, train_loss: 0.5034\n",
      "14/19, train_loss: 0.5034\n",
      "15/19, train_loss: 0.5034\n",
      "16/19, train_loss: 0.5034\n",
      "17/19, train_loss: 0.5034\n",
      "18/19, train_loss: 0.5034\n",
      "19/19, train_loss: 0.5034\n",
      "Epoch 127 average loss: 0.5034\n",
      "\n",
      "Epoch 128/300\n",
      "1/19, train_loss: 0.5034\n",
      "2/19, train_loss: 0.5034\n",
      "3/19, train_loss: 0.5034\n",
      "4/19, train_loss: 0.5034\n",
      "5/19, train_loss: 0.5034\n",
      "6/19, train_loss: 0.5033\n",
      "7/19, train_loss: 0.5033\n",
      "8/19, train_loss: 0.5033\n",
      "9/19, train_loss: 0.5033\n",
      "10/19, train_loss: 0.5033\n",
      "11/19, train_loss: 0.5033\n",
      "12/19, train_loss: 0.5033\n",
      "13/19, train_loss: 0.5033\n",
      "14/19, train_loss: 0.5033\n",
      "15/19, train_loss: 0.5033\n",
      "16/19, train_loss: 0.5033\n",
      "17/19, train_loss: 0.5033\n",
      "18/19, train_loss: 0.5033\n",
      "19/19, train_loss: 0.5033\n",
      "Epoch 128 average loss: 0.5033\n",
      "\n",
      "Epoch 129/300\n",
      "1/19, train_loss: 0.5033\n",
      "2/19, train_loss: 0.5033\n",
      "3/19, train_loss: 0.5033\n",
      "4/19, train_loss: 0.5033\n",
      "5/19, train_loss: 0.5033\n",
      "6/19, train_loss: 0.5033\n",
      "7/19, train_loss: 0.5033\n",
      "8/19, train_loss: 0.5033\n",
      "9/19, train_loss: 0.5033\n",
      "10/19, train_loss: 0.5033\n",
      "11/19, train_loss: 0.5033\n",
      "12/19, train_loss: 0.5033\n",
      "13/19, train_loss: 0.5033\n",
      "14/19, train_loss: 0.5033\n",
      "15/19, train_loss: 0.5033\n",
      "16/19, train_loss: 0.5033\n",
      "17/19, train_loss: 0.5033\n",
      "18/19, train_loss: 0.5033\n",
      "19/19, train_loss: 0.5033\n",
      "Epoch 129 average loss: 0.5033\n",
      "\n",
      "Epoch 130/300\n",
      "1/19, train_loss: 0.5033\n",
      "2/19, train_loss: 0.5033\n",
      "3/19, train_loss: 0.5033\n",
      "4/19, train_loss: 0.5033\n",
      "5/19, train_loss: 0.5032\n",
      "6/19, train_loss: 0.5032\n",
      "7/19, train_loss: 0.5033\n",
      "8/19, train_loss: 0.5032\n",
      "9/19, train_loss: 0.5032\n",
      "10/19, train_loss: 0.5032\n",
      "11/19, train_loss: 0.5032\n",
      "12/19, train_loss: 0.5032\n",
      "13/19, train_loss: 0.5032\n",
      "14/19, train_loss: 0.5032\n",
      "15/19, train_loss: 0.5032\n",
      "16/19, train_loss: 0.5032\n",
      "17/19, train_loss: 0.5032\n",
      "18/19, train_loss: 0.5032\n",
      "19/19, train_loss: 0.5032\n",
      "Epoch 130 average loss: 0.5032\n",
      "Current epoch: 130, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 131/300\n",
      "1/19, train_loss: 0.5032\n",
      "2/19, train_loss: 0.5032\n",
      "3/19, train_loss: 0.5032\n",
      "4/19, train_loss: 0.5032\n",
      "5/19, train_loss: 0.5032\n",
      "6/19, train_loss: 0.5032\n",
      "7/19, train_loss: 0.5032\n",
      "8/19, train_loss: 0.5032\n",
      "9/19, train_loss: 0.5032\n",
      "10/19, train_loss: 0.5032\n",
      "11/19, train_loss: 0.5032\n",
      "12/19, train_loss: 0.5032\n",
      "13/19, train_loss: 0.5032\n",
      "14/19, train_loss: 0.5032\n",
      "15/19, train_loss: 0.5032\n",
      "16/19, train_loss: 0.5032\n",
      "17/19, train_loss: 0.5032\n",
      "18/19, train_loss: 0.5032\n",
      "19/19, train_loss: 0.5032\n",
      "Epoch 131 average loss: 0.5032\n",
      "\n",
      "Epoch 132/300\n",
      "1/19, train_loss: 0.5032\n",
      "2/19, train_loss: 0.5032\n",
      "3/19, train_loss: 0.5032\n",
      "4/19, train_loss: 0.5032\n",
      "5/19, train_loss: 0.5032\n",
      "6/19, train_loss: 0.5031\n",
      "7/19, train_loss: 0.5031\n",
      "8/19, train_loss: 0.5031\n",
      "9/19, train_loss: 0.5031\n",
      "10/19, train_loss: 0.5031\n",
      "11/19, train_loss: 0.5031\n",
      "12/19, train_loss: 0.5031\n",
      "13/19, train_loss: 0.5031\n",
      "14/19, train_loss: 0.5031\n",
      "15/19, train_loss: 0.5031\n",
      "16/19, train_loss: 0.5031\n",
      "17/19, train_loss: 0.5031\n",
      "18/19, train_loss: 0.5031\n",
      "19/19, train_loss: 0.5031\n",
      "Epoch 132 average loss: 0.5031\n",
      "\n",
      "Epoch 133/300\n",
      "1/19, train_loss: 0.5031\n",
      "2/19, train_loss: 0.5031\n",
      "3/19, train_loss: 0.5031\n",
      "4/19, train_loss: 0.5031\n",
      "5/19, train_loss: 0.5031\n",
      "6/19, train_loss: 0.5031\n",
      "7/19, train_loss: 0.5031\n",
      "8/19, train_loss: 0.5031\n",
      "9/19, train_loss: 0.5031\n",
      "10/19, train_loss: 0.5031\n",
      "11/19, train_loss: 0.5031\n",
      "12/19, train_loss: 0.5031\n",
      "13/19, train_loss: 0.5031\n",
      "14/19, train_loss: 0.5031\n",
      "15/19, train_loss: 0.5031\n",
      "16/19, train_loss: 0.5031\n",
      "17/19, train_loss: 0.5031\n",
      "18/19, train_loss: 0.5031\n",
      "19/19, train_loss: 0.5031\n",
      "Epoch 133 average loss: 0.5031\n",
      "\n",
      "Epoch 134/300\n",
      "1/19, train_loss: 0.5031\n",
      "2/19, train_loss: 0.5031\n",
      "3/19, train_loss: 0.5031\n",
      "4/19, train_loss: 0.5031\n",
      "5/19, train_loss: 0.5031\n",
      "6/19, train_loss: 0.5031\n",
      "7/19, train_loss: 0.5031\n",
      "8/19, train_loss: 0.5031\n",
      "9/19, train_loss: 0.5031\n",
      "10/19, train_loss: 0.5030\n",
      "11/19, train_loss: 0.5030\n",
      "12/19, train_loss: 0.5031\n",
      "13/19, train_loss: 0.5030\n",
      "14/19, train_loss: 0.5030\n",
      "15/19, train_loss: 0.5030\n",
      "16/19, train_loss: 0.5030\n",
      "17/19, train_loss: 0.5030\n",
      "18/19, train_loss: 0.5030\n",
      "19/19, train_loss: 0.5030\n",
      "Epoch 134 average loss: 0.5030\n",
      "\n",
      "Epoch 135/300\n",
      "1/19, train_loss: 0.5030\n",
      "2/19, train_loss: 0.5030\n",
      "3/19, train_loss: 0.5030\n",
      "4/19, train_loss: 0.5030\n",
      "5/19, train_loss: 0.5030\n",
      "6/19, train_loss: 0.5030\n",
      "7/19, train_loss: 0.5030\n",
      "8/19, train_loss: 0.5030\n",
      "9/19, train_loss: 0.5030\n",
      "10/19, train_loss: 0.5030\n",
      "11/19, train_loss: 0.5030\n",
      "12/19, train_loss: 0.5030\n",
      "13/19, train_loss: 0.5030\n",
      "14/19, train_loss: 0.5030\n",
      "15/19, train_loss: 0.5030\n",
      "16/19, train_loss: 0.5030\n",
      "17/19, train_loss: 0.5030\n",
      "18/19, train_loss: 0.5030\n",
      "19/19, train_loss: 0.5030\n",
      "Epoch 135 average loss: 0.5030\n",
      "Current epoch: 135, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 136/300\n",
      "1/19, train_loss: 0.5030\n",
      "2/19, train_loss: 0.5030\n",
      "3/19, train_loss: 0.5030\n",
      "4/19, train_loss: 0.5030\n",
      "5/19, train_loss: 0.5030\n",
      "6/19, train_loss: 0.5030\n",
      "7/19, train_loss: 0.5030\n",
      "8/19, train_loss: 0.5030\n",
      "9/19, train_loss: 0.5030\n",
      "10/19, train_loss: 0.5030\n",
      "11/19, train_loss: 0.5030\n",
      "12/19, train_loss: 0.5029\n",
      "13/19, train_loss: 0.5029\n",
      "14/19, train_loss: 0.5029\n",
      "15/19, train_loss: 0.5029\n",
      "16/19, train_loss: 0.5029\n",
      "17/19, train_loss: 0.5029\n",
      "18/19, train_loss: 0.5029\n",
      "19/19, train_loss: 0.5029\n",
      "Epoch 136 average loss: 0.5030\n",
      "\n",
      "Epoch 137/300\n",
      "1/19, train_loss: 0.5029\n",
      "2/19, train_loss: 0.5029\n",
      "3/19, train_loss: 0.5029\n",
      "4/19, train_loss: 0.5029\n",
      "5/19, train_loss: 0.5029\n",
      "6/19, train_loss: 0.5029\n",
      "7/19, train_loss: 0.5029\n",
      "8/19, train_loss: 0.5029\n",
      "9/19, train_loss: 0.5029\n",
      "10/19, train_loss: 0.5029\n",
      "11/19, train_loss: 0.5029\n",
      "12/19, train_loss: 0.5029\n",
      "13/19, train_loss: 0.5029\n",
      "14/19, train_loss: 0.5029\n",
      "15/19, train_loss: 0.5029\n",
      "16/19, train_loss: 0.5029\n",
      "17/19, train_loss: 0.5028\n",
      "18/19, train_loss: 0.5028\n",
      "19/19, train_loss: 0.5028\n",
      "Epoch 137 average loss: 0.5029\n",
      "\n",
      "Epoch 138/300\n",
      "1/19, train_loss: 0.5028\n",
      "2/19, train_loss: 0.5028\n",
      "3/19, train_loss: 0.5028\n",
      "4/19, train_loss: 0.5028\n",
      "5/19, train_loss: 0.5027\n",
      "6/19, train_loss: 0.5027\n",
      "7/19, train_loss: 0.5027\n",
      "8/19, train_loss: 0.5027\n",
      "9/19, train_loss: 0.5027\n",
      "10/19, train_loss: 0.5027\n",
      "11/19, train_loss: 0.5027\n",
      "12/19, train_loss: 0.5027\n",
      "13/19, train_loss: 0.5027\n",
      "14/19, train_loss: 0.5027\n",
      "15/19, train_loss: 0.5027\n",
      "16/19, train_loss: 0.5027\n",
      "17/19, train_loss: 0.5027\n",
      "18/19, train_loss: 0.5027\n",
      "19/19, train_loss: 0.5027\n",
      "Epoch 138 average loss: 0.5027\n",
      "\n",
      "Epoch 139/300\n",
      "1/19, train_loss: 0.5027\n",
      "2/19, train_loss: 0.5027\n",
      "3/19, train_loss: 0.5027\n",
      "4/19, train_loss: 0.5027\n",
      "5/19, train_loss: 0.5027\n",
      "6/19, train_loss: 0.5027\n",
      "7/19, train_loss: 0.5027\n",
      "8/19, train_loss: 0.5026\n",
      "9/19, train_loss: 0.5026\n",
      "10/19, train_loss: 0.5026\n",
      "11/19, train_loss: 0.5026\n",
      "12/19, train_loss: 0.5026\n",
      "13/19, train_loss: 0.5026\n",
      "14/19, train_loss: 0.5026\n",
      "15/19, train_loss: 0.5026\n",
      "16/19, train_loss: 0.5026\n",
      "17/19, train_loss: 0.5026\n",
      "18/19, train_loss: 0.5026\n",
      "19/19, train_loss: 0.5026\n",
      "Epoch 139 average loss: 0.5026\n",
      "\n",
      "Epoch 140/300\n",
      "1/19, train_loss: 0.5026\n",
      "2/19, train_loss: 0.5026\n",
      "3/19, train_loss: 0.5026\n",
      "4/19, train_loss: 0.5026\n",
      "5/19, train_loss: 0.5026\n",
      "6/19, train_loss: 0.5026\n",
      "7/19, train_loss: 0.5026\n",
      "8/19, train_loss: 0.5026\n",
      "9/19, train_loss: 0.5026\n",
      "10/19, train_loss: 0.5026\n",
      "11/19, train_loss: 0.5026\n",
      "12/19, train_loss: 0.5026\n",
      "13/19, train_loss: 0.5026\n",
      "14/19, train_loss: 0.5026\n",
      "15/19, train_loss: 0.5026\n",
      "16/19, train_loss: 0.5026\n",
      "17/19, train_loss: 0.5026\n",
      "18/19, train_loss: 0.5026\n",
      "19/19, train_loss: 0.5026\n",
      "Epoch 140 average loss: 0.5026\n",
      "Current epoch: 140, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 141/300\n",
      "1/19, train_loss: 0.5026\n",
      "2/19, train_loss: 0.5026\n",
      "3/19, train_loss: 0.5026\n",
      "4/19, train_loss: 0.5026\n",
      "5/19, train_loss: 0.5026\n",
      "6/19, train_loss: 0.5026\n",
      "7/19, train_loss: 0.5026\n",
      "8/19, train_loss: 0.5026\n",
      "9/19, train_loss: 0.5026\n",
      "10/19, train_loss: 0.5026\n",
      "11/19, train_loss: 0.5026\n",
      "12/19, train_loss: 0.5025\n",
      "13/19, train_loss: 0.5025\n",
      "14/19, train_loss: 0.5025\n",
      "15/19, train_loss: 0.5025\n",
      "16/19, train_loss: 0.5025\n",
      "17/19, train_loss: 0.5025\n",
      "18/19, train_loss: 0.5025\n",
      "19/19, train_loss: 0.5025\n",
      "Epoch 141 average loss: 0.5026\n",
      "\n",
      "Epoch 142/300\n",
      "1/19, train_loss: 0.5025\n",
      "2/19, train_loss: 0.5025\n",
      "3/19, train_loss: 0.5025\n",
      "4/19, train_loss: 0.5025\n",
      "5/19, train_loss: 0.5025\n",
      "6/19, train_loss: 0.5025\n",
      "7/19, train_loss: 0.5025\n",
      "8/19, train_loss: 0.5025\n",
      "9/19, train_loss: 0.5025\n",
      "10/19, train_loss: 0.5025\n",
      "11/19, train_loss: 0.5025\n",
      "12/19, train_loss: 0.5025\n",
      "13/19, train_loss: 0.5025\n",
      "14/19, train_loss: 0.5025\n",
      "15/19, train_loss: 0.5025\n",
      "16/19, train_loss: 0.5025\n",
      "17/19, train_loss: 0.5025\n",
      "18/19, train_loss: 0.5025\n",
      "19/19, train_loss: 0.5025\n",
      "Epoch 142 average loss: 0.5025\n",
      "\n",
      "Epoch 143/300\n",
      "1/19, train_loss: 0.5025\n",
      "2/19, train_loss: 0.5025\n",
      "3/19, train_loss: 0.5025\n",
      "4/19, train_loss: 0.5025\n",
      "5/19, train_loss: 0.5025\n",
      "6/19, train_loss: 0.5025\n",
      "7/19, train_loss: 0.5025\n",
      "8/19, train_loss: 0.5025\n",
      "9/19, train_loss: 0.5025\n",
      "10/19, train_loss: 0.5025\n",
      "11/19, train_loss: 0.5025\n",
      "12/19, train_loss: 0.5025\n",
      "13/19, train_loss: 0.5025\n",
      "14/19, train_loss: 0.5025\n",
      "15/19, train_loss: 0.5025\n",
      "16/19, train_loss: 0.5025\n",
      "17/19, train_loss: 0.5025\n",
      "18/19, train_loss: 0.5024\n",
      "19/19, train_loss: 0.5024\n",
      "Epoch 143 average loss: 0.5025\n",
      "\n",
      "Epoch 144/300\n",
      "1/19, train_loss: 0.5024\n",
      "2/19, train_loss: 0.5024\n",
      "3/19, train_loss: 0.5024\n",
      "4/19, train_loss: 0.5024\n",
      "5/19, train_loss: 0.5024\n",
      "6/19, train_loss: 0.5024\n",
      "7/19, train_loss: 0.5024\n",
      "8/19, train_loss: 0.5024\n",
      "9/19, train_loss: 0.5024\n",
      "10/19, train_loss: 0.5024\n",
      "11/19, train_loss: 0.5024\n",
      "12/19, train_loss: 0.5024\n",
      "13/19, train_loss: 0.5024\n",
      "14/19, train_loss: 0.5024\n",
      "15/19, train_loss: 0.5024\n",
      "16/19, train_loss: 0.5024\n",
      "17/19, train_loss: 0.5024\n",
      "18/19, train_loss: 0.5024\n",
      "19/19, train_loss: 0.5024\n",
      "Epoch 144 average loss: 0.5024\n",
      "\n",
      "Epoch 145/300\n",
      "1/19, train_loss: 0.5024\n",
      "2/19, train_loss: 0.5024\n",
      "3/19, train_loss: 0.5024\n",
      "4/19, train_loss: 0.5024\n",
      "5/19, train_loss: 0.5024\n",
      "6/19, train_loss: 0.5024\n",
      "7/19, train_loss: 0.5024\n",
      "8/19, train_loss: 0.5024\n",
      "9/19, train_loss: 0.5024\n",
      "10/19, train_loss: 0.5024\n",
      "11/19, train_loss: 0.5024\n",
      "12/19, train_loss: 0.5024\n",
      "13/19, train_loss: 0.5024\n",
      "14/19, train_loss: 0.5024\n",
      "15/19, train_loss: 0.5024\n",
      "16/19, train_loss: 0.5024\n",
      "17/19, train_loss: 0.5024\n",
      "18/19, train_loss: 0.5024\n",
      "19/19, train_loss: 0.5024\n",
      "Epoch 145 average loss: 0.5024\n",
      "Current epoch: 145, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 146/300\n",
      "1/19, train_loss: 0.5024\n",
      "2/19, train_loss: 0.5024\n",
      "3/19, train_loss: 0.5024\n",
      "4/19, train_loss: 0.5024\n",
      "5/19, train_loss: 0.5024\n",
      "6/19, train_loss: 0.5024\n",
      "7/19, train_loss: 0.5024\n",
      "8/19, train_loss: 0.5024\n",
      "9/19, train_loss: 0.5024\n",
      "10/19, train_loss: 0.5024\n",
      "11/19, train_loss: 0.5024\n",
      "12/19, train_loss: 0.5024\n",
      "13/19, train_loss: 0.5024\n",
      "14/19, train_loss: 0.5024\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5023\n",
      "17/19, train_loss: 0.5023\n",
      "18/19, train_loss: 0.5023\n",
      "19/19, train_loss: 0.5023\n",
      "Epoch 146 average loss: 0.5024\n",
      "\n",
      "Epoch 147/300\n",
      "1/19, train_loss: 0.5023\n",
      "2/19, train_loss: 0.5023\n",
      "3/19, train_loss: 0.5023\n",
      "4/19, train_loss: 0.5023\n",
      "5/19, train_loss: 0.5023\n",
      "6/19, train_loss: 0.5023\n",
      "7/19, train_loss: 0.5023\n",
      "8/19, train_loss: 0.5023\n",
      "9/19, train_loss: 0.5023\n",
      "10/19, train_loss: 0.5023\n",
      "11/19, train_loss: 0.5023\n",
      "12/19, train_loss: 0.5023\n",
      "13/19, train_loss: 0.5023\n",
      "14/19, train_loss: 0.5023\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5023\n",
      "17/19, train_loss: 0.5023\n",
      "18/19, train_loss: 0.5023\n",
      "19/19, train_loss: 0.5023\n",
      "Epoch 147 average loss: 0.5023\n",
      "\n",
      "Epoch 148/300\n",
      "1/19, train_loss: 0.5023\n",
      "2/19, train_loss: 0.5023\n",
      "3/19, train_loss: 0.5023\n",
      "4/19, train_loss: 0.5023\n",
      "5/19, train_loss: 0.5023\n",
      "6/19, train_loss: 0.5023\n",
      "7/19, train_loss: 0.5023\n",
      "8/19, train_loss: 0.5023\n",
      "9/19, train_loss: 0.5023\n",
      "10/19, train_loss: 0.5023\n",
      "11/19, train_loss: 0.5023\n",
      "12/19, train_loss: 0.5023\n",
      "13/19, train_loss: 0.5023\n",
      "14/19, train_loss: 0.5023\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5023\n",
      "17/19, train_loss: 0.5023\n",
      "18/19, train_loss: 0.5023\n",
      "19/19, train_loss: 0.5023\n",
      "Epoch 148 average loss: 0.5023\n",
      "\n",
      "Epoch 149/300\n",
      "1/19, train_loss: 0.5023\n",
      "2/19, train_loss: 0.5023\n",
      "3/19, train_loss: 0.5023\n",
      "4/19, train_loss: 0.5023\n",
      "5/19, train_loss: 0.5023\n",
      "6/19, train_loss: 0.5023\n",
      "7/19, train_loss: 0.5023\n",
      "8/19, train_loss: 0.5023\n",
      "9/19, train_loss: 0.5023\n",
      "10/19, train_loss: 0.5023\n",
      "11/19, train_loss: 0.5023\n",
      "12/19, train_loss: 0.5023\n",
      "13/19, train_loss: 0.5023\n",
      "14/19, train_loss: 0.5023\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5022\n",
      "17/19, train_loss: 0.5022\n",
      "18/19, train_loss: 0.5022\n",
      "19/19, train_loss: 0.5022\n",
      "Epoch 149 average loss: 0.5023\n",
      "\n",
      "Epoch 150/300\n",
      "1/19, train_loss: 0.5022\n",
      "2/19, train_loss: 0.5022\n",
      "3/19, train_loss: 0.5022\n",
      "4/19, train_loss: 0.5022\n",
      "5/19, train_loss: 0.5022\n",
      "6/19, train_loss: 0.5022\n",
      "7/19, train_loss: 0.5022\n",
      "8/19, train_loss: 0.5022\n",
      "9/19, train_loss: 0.5022\n",
      "10/19, train_loss: 0.5022\n",
      "11/19, train_loss: 0.5022\n",
      "12/19, train_loss: 0.5022\n",
      "13/19, train_loss: 0.5022\n",
      "14/19, train_loss: 0.5022\n",
      "15/19, train_loss: 0.5022\n",
      "16/19, train_loss: 0.5022\n",
      "17/19, train_loss: 0.5022\n",
      "18/19, train_loss: 0.5022\n",
      "19/19, train_loss: 0.5022\n",
      "Epoch 150 average loss: 0.5022\n",
      "Current epoch: 150, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 151/300\n",
      "1/19, train_loss: 0.5022\n",
      "2/19, train_loss: 0.5022\n",
      "3/19, train_loss: 0.5022\n",
      "4/19, train_loss: 0.5022\n",
      "5/19, train_loss: 0.5022\n",
      "6/19, train_loss: 0.5022\n",
      "7/19, train_loss: 0.5022\n",
      "8/19, train_loss: 0.5022\n",
      "9/19, train_loss: 0.5022\n",
      "10/19, train_loss: 0.5022\n",
      "11/19, train_loss: 0.5022\n",
      "12/19, train_loss: 0.5022\n",
      "13/19, train_loss: 0.5022\n",
      "14/19, train_loss: 0.5022\n",
      "15/19, train_loss: 0.5022\n",
      "16/19, train_loss: 0.5022\n",
      "17/19, train_loss: 0.5022\n",
      "18/19, train_loss: 0.5022\n",
      "19/19, train_loss: 0.5022\n",
      "Epoch 151 average loss: 0.5022\n",
      "\n",
      "Epoch 152/300\n",
      "1/19, train_loss: 0.5022\n",
      "2/19, train_loss: 0.5022\n",
      "3/19, train_loss: 0.5022\n",
      "4/19, train_loss: 0.5022\n",
      "5/19, train_loss: 0.5022\n",
      "6/19, train_loss: 0.5022\n",
      "7/19, train_loss: 0.5022\n",
      "8/19, train_loss: 0.5022\n",
      "9/19, train_loss: 0.5022\n",
      "10/19, train_loss: 0.5022\n",
      "11/19, train_loss: 0.5022\n",
      "12/19, train_loss: 0.5022\n",
      "13/19, train_loss: 0.5022\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 152 average loss: 0.5022\n",
      "\n",
      "Epoch 153/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5021\n",
      "3/19, train_loss: 0.5021\n",
      "4/19, train_loss: 0.5021\n",
      "5/19, train_loss: 0.5021\n",
      "6/19, train_loss: 0.5021\n",
      "7/19, train_loss: 0.5021\n",
      "8/19, train_loss: 0.5021\n",
      "9/19, train_loss: 0.5021\n",
      "10/19, train_loss: 0.5021\n",
      "11/19, train_loss: 0.5021\n",
      "12/19, train_loss: 0.5021\n",
      "13/19, train_loss: 0.5021\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 153 average loss: 0.5021\n",
      "\n",
      "Epoch 154/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5021\n",
      "3/19, train_loss: 0.5021\n",
      "4/19, train_loss: 0.5021\n",
      "5/19, train_loss: 0.5021\n",
      "6/19, train_loss: 0.5021\n",
      "7/19, train_loss: 0.5021\n",
      "8/19, train_loss: 0.5021\n",
      "9/19, train_loss: 0.5021\n",
      "10/19, train_loss: 0.5021\n",
      "11/19, train_loss: 0.5021\n",
      "12/19, train_loss: 0.5021\n",
      "13/19, train_loss: 0.5021\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 154 average loss: 0.5021\n",
      "\n",
      "Epoch 155/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5021\n",
      "3/19, train_loss: 0.5021\n",
      "4/19, train_loss: 0.5021\n",
      "5/19, train_loss: 0.5021\n",
      "6/19, train_loss: 0.5021\n",
      "7/19, train_loss: 0.5021\n",
      "8/19, train_loss: 0.5021\n",
      "9/19, train_loss: 0.5021\n",
      "10/19, train_loss: 0.5021\n",
      "11/19, train_loss: 0.5021\n",
      "12/19, train_loss: 0.5021\n",
      "13/19, train_loss: 0.5021\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 155 average loss: 0.5021\n",
      "Current epoch: 155, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 156/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5020\n",
      "19/19, train_loss: 0.5020\n",
      "Epoch 156 average loss: 0.5020\n",
      "\n",
      "Epoch 157/300\n",
      "1/19, train_loss: 0.5020\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5020\n",
      "19/19, train_loss: 0.5020\n",
      "Epoch 157 average loss: 0.5020\n",
      "\n",
      "Epoch 158/300\n",
      "1/19, train_loss: 0.5020\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5020\n",
      "19/19, train_loss: 0.5020\n",
      "Epoch 158 average loss: 0.5020\n",
      "\n",
      "Epoch 159/300\n",
      "1/19, train_loss: 0.5020\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 159 average loss: 0.5020\n",
      "\n",
      "Epoch 160/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 160 average loss: 0.5019\n",
      "Current epoch: 160, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 161/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 161 average loss: 0.5019\n",
      "\n",
      "Epoch 162/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 162 average loss: 0.5019\n",
      "\n",
      "Epoch 163/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 163 average loss: 0.5019\n",
      "\n",
      "Epoch 164/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 164 average loss: 0.5018\n",
      "\n",
      "Epoch 165/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 165 average loss: 0.5018\n",
      "Current epoch: 165, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 166/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 166 average loss: 0.5018\n",
      "\n",
      "Epoch 167/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 167 average loss: 0.5018\n",
      "\n",
      "Epoch 168/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 168 average loss: 0.5017\n",
      "\n",
      "Epoch 169/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 169 average loss: 0.5017\n",
      "\n",
      "Epoch 170/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 170 average loss: 0.5017\n",
      "Current epoch: 170, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 171/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 171 average loss: 0.5017\n",
      "\n",
      "Epoch 172/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 172 average loss: 0.5017\n",
      "\n",
      "Epoch 173/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 173 average loss: 0.5016\n",
      "\n",
      "Epoch 174/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 174 average loss: 0.5016\n",
      "\n",
      "Epoch 175/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 175 average loss: 0.5016\n",
      "Current epoch: 175, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 176/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 176 average loss: 0.5016\n",
      "\n",
      "Epoch 177/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 177 average loss: 0.5016\n",
      "\n",
      "Epoch 178/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 178 average loss: 0.5015\n",
      "\n",
      "Epoch 179/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 179 average loss: 0.5015\n",
      "\n",
      "Epoch 180/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 180 average loss: 0.5015\n",
      "Current epoch: 180, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 181/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 181 average loss: 0.5015\n",
      "\n",
      "Epoch 182/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 182 average loss: 0.5015\n",
      "\n",
      "Epoch 183/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 183 average loss: 0.5015\n",
      "\n",
      "Epoch 184/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 184 average loss: 0.5014\n",
      "\n",
      "Epoch 185/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 185 average loss: 0.5014\n",
      "Current epoch: 185, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 186/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 186 average loss: 0.5014\n",
      "\n",
      "Epoch 187/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 187 average loss: 0.5014\n",
      "\n",
      "Epoch 188/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 188 average loss: 0.5014\n",
      "\n",
      "Epoch 189/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 189 average loss: 0.5014\n",
      "\n",
      "Epoch 190/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 190 average loss: 0.5013\n",
      "Current epoch: 190, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 191/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 191 average loss: 0.5013\n",
      "\n",
      "Epoch 192/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 192 average loss: 0.5013\n",
      "\n",
      "Epoch 193/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 193 average loss: 0.5013\n",
      "\n",
      "Epoch 194/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 194 average loss: 0.5013\n",
      "\n",
      "Epoch 195/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 195 average loss: 0.5013\n",
      "Current epoch: 195, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 196/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 196 average loss: 0.5012\n",
      "\n",
      "Epoch 197/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 197 average loss: 0.5012\n",
      "\n",
      "Epoch 198/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 198 average loss: 0.5012\n",
      "\n",
      "Epoch 199/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 199 average loss: 0.5012\n",
      "\n",
      "Epoch 200/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 200 average loss: 0.5012\n",
      "Current epoch: 200, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 201/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 201 average loss: 0.5012\n",
      "\n",
      "Epoch 202/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 202 average loss: 0.5012\n",
      "\n",
      "Epoch 203/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 203 average loss: 0.5011\n",
      "\n",
      "Epoch 204/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 204 average loss: 0.5011\n",
      "\n",
      "Epoch 205/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 205 average loss: 0.5011\n",
      "Current epoch: 205, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 206/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 206 average loss: 0.5011\n",
      "\n",
      "Epoch 207/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 207 average loss: 0.5011\n",
      "\n",
      "Epoch 208/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 208 average loss: 0.5011\n",
      "\n",
      "Epoch 209/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 209 average loss: 0.5011\n",
      "\n",
      "Epoch 210/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 210 average loss: 0.5011\n",
      "Current epoch: 210, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 211/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 211 average loss: 0.5011\n",
      "\n",
      "Epoch 212/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 212 average loss: 0.5010\n",
      "\n",
      "Epoch 213/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 213 average loss: 0.5010\n",
      "\n",
      "Epoch 214/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 214 average loss: 0.5010\n",
      "\n",
      "Epoch 215/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 215 average loss: 0.5010\n",
      "Current epoch: 215, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 216/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 216 average loss: 0.5010\n",
      "\n",
      "Epoch 217/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 217 average loss: 0.5010\n",
      "\n",
      "Epoch 218/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 218 average loss: 0.5010\n",
      "\n",
      "Epoch 219/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 219 average loss: 0.5010\n",
      "\n",
      "Epoch 220/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 220 average loss: 0.5010\n",
      "Current epoch: 220, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 221/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 221 average loss: 0.5009\n",
      "\n",
      "Epoch 222/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 222 average loss: 0.5009\n",
      "\n",
      "Epoch 223/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 223 average loss: 0.5009\n",
      "\n",
      "Epoch 224/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 224 average loss: 0.5009\n",
      "\n",
      "Epoch 225/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 225 average loss: 0.5009\n",
      "Current epoch: 225, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 226/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 226 average loss: 0.5009\n",
      "\n",
      "Epoch 227/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 227 average loss: 0.5009\n",
      "\n",
      "Epoch 228/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 228 average loss: 0.5009\n",
      "\n",
      "Epoch 229/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 229 average loss: 0.5009\n",
      "\n",
      "Epoch 230/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 230 average loss: 0.5009\n",
      "Current epoch: 230, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 231/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 231 average loss: 0.5008\n",
      "\n",
      "Epoch 232/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 232 average loss: 0.5008\n",
      "\n",
      "Epoch 233/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 233 average loss: 0.5008\n",
      "\n",
      "Epoch 234/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 234 average loss: 0.5008\n",
      "\n",
      "Epoch 235/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 235 average loss: 0.5008\n",
      "Current epoch: 235, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 236/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 236 average loss: 0.5008\n",
      "\n",
      "Epoch 237/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 237 average loss: 0.5008\n",
      "\n",
      "Epoch 238/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 238 average loss: 0.5008\n",
      "\n",
      "Epoch 239/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 239 average loss: 0.5008\n",
      "\n",
      "Epoch 240/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 240 average loss: 0.5008\n",
      "Current epoch: 240, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 241/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 241 average loss: 0.5008\n",
      "\n",
      "Epoch 242/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 242 average loss: 0.5008\n",
      "\n",
      "Epoch 243/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 243 average loss: 0.5007\n",
      "\n",
      "Epoch 244/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 244 average loss: 0.5007\n",
      "\n",
      "Epoch 245/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 245 average loss: 0.5007\n",
      "Current epoch: 245, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 246/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 246 average loss: 0.5007\n",
      "\n",
      "Epoch 247/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 247 average loss: 0.5007\n",
      "\n",
      "Epoch 248/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 248 average loss: 0.5007\n",
      "\n",
      "Epoch 249/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 249 average loss: 0.5007\n",
      "\n",
      "Epoch 250/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 250 average loss: 0.5007\n",
      "Current epoch: 250, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 251/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 251 average loss: 0.5007\n",
      "\n",
      "Epoch 252/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 252 average loss: 0.5007\n",
      "\n",
      "Epoch 253/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 253 average loss: 0.5007\n",
      "\n",
      "Epoch 254/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 254 average loss: 0.5007\n",
      "\n",
      "Epoch 255/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5007\n",
      "11/19, train_loss: 0.5007\n",
      "12/19, train_loss: 0.5007\n",
      "13/19, train_loss: 0.5007\n",
      "14/19, train_loss: 0.5007\n",
      "15/19, train_loss: 0.5007\n",
      "16/19, train_loss: 0.5007\n",
      "17/19, train_loss: 0.5007\n",
      "18/19, train_loss: 0.5007\n",
      "19/19, train_loss: 0.5007\n",
      "Epoch 255 average loss: 0.5007\n",
      "Current epoch: 255, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 256/300\n",
      "1/19, train_loss: 0.5007\n",
      "2/19, train_loss: 0.5007\n",
      "3/19, train_loss: 0.5007\n",
      "4/19, train_loss: 0.5007\n",
      "5/19, train_loss: 0.5007\n",
      "6/19, train_loss: 0.5007\n",
      "7/19, train_loss: 0.5007\n",
      "8/19, train_loss: 0.5007\n",
      "9/19, train_loss: 0.5007\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 256 average loss: 0.5007\n",
      "\n",
      "Epoch 257/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 257 average loss: 0.5006\n",
      "\n",
      "Epoch 258/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 258 average loss: 0.5006\n",
      "\n",
      "Epoch 259/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 259 average loss: 0.5006\n",
      "\n",
      "Epoch 260/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 260 average loss: 0.5006\n",
      "Current epoch: 260, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 261/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 261 average loss: 0.5006\n",
      "\n",
      "Epoch 262/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 262 average loss: 0.5006\n",
      "\n",
      "Epoch 263/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 263 average loss: 0.5006\n",
      "\n",
      "Epoch 264/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 264 average loss: 0.5006\n",
      "\n",
      "Epoch 265/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 265 average loss: 0.5006\n",
      "Current epoch: 265, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 266/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 266 average loss: 0.5006\n",
      "\n",
      "Epoch 267/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 267 average loss: 0.5006\n",
      "\n",
      "Epoch 268/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 268 average loss: 0.5006\n",
      "\n",
      "Epoch 269/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 269 average loss: 0.5006\n",
      "\n",
      "Epoch 270/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 270 average loss: 0.5006\n",
      "Current epoch: 270, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 271/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5006\n",
      "16/19, train_loss: 0.5006\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5006\n",
      "19/19, train_loss: 0.5006\n",
      "Epoch 271 average loss: 0.5006\n",
      "\n",
      "Epoch 272/300\n",
      "1/19, train_loss: 0.5006\n",
      "2/19, train_loss: 0.5006\n",
      "3/19, train_loss: 0.5006\n",
      "4/19, train_loss: 0.5006\n",
      "5/19, train_loss: 0.5006\n",
      "6/19, train_loss: 0.5006\n",
      "7/19, train_loss: 0.5006\n",
      "8/19, train_loss: 0.5006\n",
      "9/19, train_loss: 0.5006\n",
      "10/19, train_loss: 0.5006\n",
      "11/19, train_loss: 0.5006\n",
      "12/19, train_loss: 0.5006\n",
      "13/19, train_loss: 0.5006\n",
      "14/19, train_loss: 0.5006\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5006\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 272 average loss: 0.5006\n",
      "\n",
      "Epoch 273/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 273 average loss: 0.5005\n",
      "\n",
      "Epoch 274/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 274 average loss: 0.5005\n",
      "\n",
      "Epoch 275/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 275 average loss: 0.5005\n",
      "Current epoch: 275, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 276/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 276 average loss: 0.5005\n",
      "\n",
      "Epoch 277/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 277 average loss: 0.5005\n",
      "\n",
      "Epoch 278/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 278 average loss: 0.5005\n",
      "\n",
      "Epoch 279/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 279 average loss: 0.5005\n",
      "\n",
      "Epoch 280/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 280 average loss: 0.5005\n",
      "Current epoch: 280, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 281/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n",
      "16/19, train_loss: 0.5005\n",
      "17/19, train_loss: 0.5005\n",
      "18/19, train_loss: 0.5005\n",
      "19/19, train_loss: 0.5005\n",
      "Epoch 281 average loss: 0.5005\n",
      "\n",
      "Epoch 282/300\n",
      "1/19, train_loss: 0.5005\n",
      "2/19, train_loss: 0.5005\n",
      "3/19, train_loss: 0.5005\n",
      "4/19, train_loss: 0.5005\n",
      "5/19, train_loss: 0.5005\n",
      "6/19, train_loss: 0.5005\n",
      "7/19, train_loss: 0.5005\n",
      "8/19, train_loss: 0.5005\n",
      "9/19, train_loss: 0.5005\n",
      "10/19, train_loss: 0.5005\n",
      "11/19, train_loss: 0.5005\n",
      "12/19, train_loss: 0.5005\n",
      "13/19, train_loss: 0.5005\n",
      "14/19, train_loss: 0.5005\n",
      "15/19, train_loss: 0.5005\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import monai\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.data import (\n",
    "    decollate_batch,\n",
    "    CacheDataset,\n",
    "    list_data_collate,\n",
    ")\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ConcatItemsd,\n",
    "    SaveImaged,\n",
    "    Invertd,\n",
    "    SpatialPadd\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set paths for your data\n",
    "# Set paths for your data\n",
    "t1_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/t1_masks\"\n",
    "t2_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/t2_masks\"\n",
    "labels_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/label_masks\"\n",
    "output_dir = \"./output\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create data dictionaries for MONAI\n",
    "def create_data_dicts(t1_dir, t2_dir, labels_dir):\n",
    "    t1_files = sorted([os.path.join(t1_dir, f) for f in os.listdir(t1_dir) if f.endswith('.nii.gz')])\n",
    "    t2_files = sorted([os.path.join(t2_dir, f) for f in os.listdir(t2_dir) if f.endswith('.nii.gz')])\n",
    "    label_files = sorted([os.path.join(labels_dir, f) for f in os.listdir(labels_dir) if f.endswith('.nii.gz')])\n",
    "    \n",
    "    # Ensure consistent file counts\n",
    "    assert len(t1_files) == len(t2_files) == len(label_files), \"Mismatch in file counts\"\n",
    "    \n",
    "    data_dicts = [\n",
    "        {\n",
    "            \"t1\": t1_file,\n",
    "            \"t2\": t2_file,\n",
    "            \"label\": label_file\n",
    "        }\n",
    "        for t1_file, t2_file, label_file in zip(t1_files, t2_files, label_files)\n",
    "    ]\n",
    "    return data_dicts\n",
    "\n",
    "# Define transforms for NIfTI files\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t1\", \"t2\", \"label\"]),  # Load NIfTI files\n",
    "    EnsureChannelFirstd(keys=[\"t1\", \"t2\", \"label\"]),\n",
    "    Orientationd(keys=[\"t1\", \"t2\", \"label\"], axcodes=\"RAS\"),  # Standardize orientation\n",
    "    Spacingd(keys=[\"t1\", \"t2\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\", \"nearest\")),\n",
    "    SpatialPadd(keys=[\"t1\", \"t2\", \"label\"], spatial_size=(96, 96, 96), mode=(\"constant\", \"constant\", \"constant\")),\n",
    "    ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "    ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),  # Concatenate T1 and T2 along channel dimension\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t1\", \"t2\", \"label\"]),  # Load NIfTI files\n",
    "    EnsureChannelFirstd(keys=[\"t1\", \"t2\", \"label\"]),\n",
    "    Orientationd(keys=[\"t1\", \"t2\", \"label\"], axcodes=\"RAS\"),  # Standardize orientation\n",
    "    Spacingd(keys=[\"t1\", \"t2\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\", \"nearest\")),\n",
    "    SpatialPadd(keys=[\"t1\", \"t2\", \"label\"], spatial_size=(96, 96, 96), mode=(\"constant\", \"constant\", \"constant\")),\n",
    "    ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "    ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),  # Concatenate T1 and T2 along channel dimension\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Split dataset into training and validation\n",
    "def train_val_split(data_dicts, val_ratio=0.2):\n",
    "    n_val = int(len(data_dicts) * val_ratio)\n",
    "    n_train = len(data_dicts) - n_val\n",
    "    \n",
    "    train_files = data_dicts[:n_train]\n",
    "    val_files = data_dicts[n_train:]\n",
    "    \n",
    "    return train_files, val_files\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "data_dicts = create_data_dicts(t1_dir, t2_dir, labels_dir)\n",
    "train_files, val_files = train_val_split(data_dicts)\n",
    "\n",
    "# Using CacheDataset for better performance\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=list_data_collate)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4, collate_fn=list_data_collate)\n",
    "\n",
    "# Define model\n",
    "# Assuming you have n classes (including background)\n",
    "num_classes = 2  # Change to the number of segmentation classes + background\n",
    "\n",
    "# Create Swin-UNETR model (with 2 input channels for T1 and T2 concatenated)\n",
    "model = SwinUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=2,  # 2 channels for T1 and T2\n",
    "    out_channels=num_classes,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function, optimizer, and metrics\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "# Post-processing transforms\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=num_classes)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=num_classes)])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "val_interval = 5\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "writer = SummaryWriter(os.path.join(output_dir, 'logs'))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_loader)}, train_loss: {loss.item():.4f}\")\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    print(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    writer.add_scalar(\"train_loss\", epoch_loss, epoch + 1)\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            metric_sum = 0.0\n",
    "            metric_count = 0\n",
    "            \n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                \n",
    "                # Compute metric\n",
    "                val_outputs_list = decollate_batch(val_outputs)\n",
    "                val_labels_list = decollate_batch(val_labels)\n",
    "                \n",
    "                val_outputs_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "                val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "                \n",
    "                dice_metric(y_pred=val_outputs_convert, y=val_labels_convert)\n",
    "                \n",
    "            metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "            \n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n",
    "                print(\"Saved new best model\")\n",
    "                \n",
    "            print(f\"Current epoch: {epoch + 1}, current mean dice: {metric:.4f}, best mean dice: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "            writer.add_scalar(\"val_mean_dice\", metric, epoch + 1)\n",
    "            \n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()\n",
    "\n",
    "# Function to infer and save predictions as NIfTI files\n",
    "def infer_and_save(model, data_dicts, output_dir):\n",
    "    # Define inference transforms\n",
    "    infer_transforms = Compose([\n",
    "        LoadImaged(keys=[\"t1\", \"t2\"]),\n",
    "        EnsureChannelFirstd(keys=[\"t1\", \"t2\"]),\n",
    "        Orientationd(keys=[\"t1\", \"t2\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"t1\", \"t2\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\")),\n",
    "        ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "        ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),\n",
    "        EnsureTyped(keys=[\"image\"]),\n",
    "    ])\n",
    "    \n",
    "    # Create a new dataset for inference (without labels)\n",
    "    infer_ds = CacheDataset(data=data_dicts, transform=infer_transforms, cache_rate=1.0, num_workers=4)\n",
    "    infer_loader = DataLoader(infer_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate)\n",
    "    \n",
    "    # Create output directory for predictions\n",
    "    pred_dir = os.path.join(output_dir, \"predictions\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    \n",
    "    # Post-transform to convert predictions back to NIfTI\n",
    "    post_transforms = Compose([\n",
    "        EnsureTyped(keys=\"pred\"),\n",
    "        AsDiscrete(argmax=True, to_onehot=num_classes),\n",
    "        # Add transforms to map predictions back to original space if needed\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"t1_meta_dict\", output_dir=pred_dir, output_postfix=\"seg\", resample=False, output_dtype=None),\n",
    "    ])\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(infer_loader):\n",
    "            print(f\"Processing case {i+1}/{len(infer_loader)}\")\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            \n",
    "            # Get the filename for saving\n",
    "            t1_path = batch_data[\"t1_meta_dict\"][\"filename_or_obj\"][0]\n",
    "            base_name = os.path.basename(t1_path)\n",
    "            \n",
    "            # Run inference\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Process outputs and save as NIfTI\n",
    "            outputs = outputs.argmax(dim=1, keepdim=True)\n",
    "            batch_data[\"pred\"] = outputs\n",
    "            \n",
    "            # Save the prediction as NIfTI\n",
    "            post_transforms(batch_data)\n",
    "            \n",
    "            print(f\"Saved prediction for {base_name}\")\n",
    "\n",
    "# Load the best model and run inference\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n",
    "infer_and_save(model, val_files, output_dir)\n",
    "print(f\"Inference completed. Predictions saved to {os.path.join(output_dir, 'predictions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63fc28-087c-4135-876b-492b82ead37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|| 37/37 [00:02<00:00, 16.47it/s]\n",
      "Loading dataset: 100%|| 9/9 [00:00<00:00, 15.42it/s]\n",
      "/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/300\n",
      "1/19, train_loss: 1.4254\n",
      "2/19, train_loss: 1.2029\n",
      "3/19, train_loss: 1.0576\n",
      "4/19, train_loss: 0.9663\n",
      "5/19, train_loss: 0.9142\n",
      "6/19, train_loss: 0.8803\n",
      "7/19, train_loss: 0.8563\n",
      "8/19, train_loss: 0.8355\n",
      "9/19, train_loss: 0.8183\n",
      "10/19, train_loss: 0.8035\n",
      "11/19, train_loss: 0.7959\n",
      "12/19, train_loss: 0.7838\n",
      "13/19, train_loss: 0.7773\n",
      "14/19, train_loss: 0.7704\n",
      "15/19, train_loss: 0.7613\n",
      "16/19, train_loss: 0.7576\n",
      "17/19, train_loss: 0.7521\n",
      "18/19, train_loss: 0.7455\n",
      "19/19, train_loss: 0.7407\n",
      "Epoch 1 average loss: 0.8760\n",
      "\n",
      "Epoch 2/300\n",
      "1/19, train_loss: 0.7394\n",
      "2/19, train_loss: 0.7349\n",
      "3/19, train_loss: 0.7296\n",
      "4/19, train_loss: 0.7267\n",
      "5/19, train_loss: 0.7250\n",
      "6/19, train_loss: 0.7221\n",
      "7/19, train_loss: 0.7191\n",
      "8/19, train_loss: 0.7169\n",
      "9/19, train_loss: 0.7132\n",
      "10/19, train_loss: 0.7111\n",
      "11/19, train_loss: 0.7099\n",
      "12/19, train_loss: 0.7079\n",
      "13/19, train_loss: 0.7055\n",
      "14/19, train_loss: 0.7039\n",
      "15/19, train_loss: 0.7010\n",
      "16/19, train_loss: 0.7005\n",
      "17/19, train_loss: 0.6993\n",
      "18/19, train_loss: 0.6976\n",
      "19/19, train_loss: 0.6957\n",
      "Epoch 2 average loss: 0.7136\n",
      "\n",
      "Epoch 3/300\n",
      "1/19, train_loss: 0.6953\n",
      "2/19, train_loss: 0.6944\n",
      "3/19, train_loss: 0.6917\n",
      "4/19, train_loss: 0.6905\n",
      "5/19, train_loss: 0.6896\n",
      "6/19, train_loss: 0.6881\n",
      "7/19, train_loss: 0.6862\n",
      "8/19, train_loss: 0.6860\n",
      "9/19, train_loss: 0.6849\n",
      "10/19, train_loss: 0.6835\n",
      "11/19, train_loss: 0.6819\n",
      "12/19, train_loss: 0.6812\n",
      "13/19, train_loss: 0.6801\n",
      "14/19, train_loss: 0.6797\n",
      "15/19, train_loss: 0.6790\n",
      "16/19, train_loss: 0.6775\n",
      "17/19, train_loss: 0.6757\n",
      "18/19, train_loss: 0.6761\n",
      "19/19, train_loss: 0.6746\n",
      "Epoch 3 average loss: 0.6840\n",
      "\n",
      "Epoch 4/300\n",
      "1/19, train_loss: 0.6743\n",
      "2/19, train_loss: 0.6732\n",
      "3/19, train_loss: 0.6733\n",
      "4/19, train_loss: 0.6711\n",
      "5/19, train_loss: 0.6705\n",
      "6/19, train_loss: 0.6701\n",
      "7/19, train_loss: 0.6687\n",
      "8/19, train_loss: 0.6681\n",
      "9/19, train_loss: 0.6673\n",
      "10/19, train_loss: 0.6665\n",
      "11/19, train_loss: 0.6654\n",
      "12/19, train_loss: 0.6648\n",
      "13/19, train_loss: 0.6637\n",
      "14/19, train_loss: 0.6634\n",
      "15/19, train_loss: 0.6624\n",
      "16/19, train_loss: 0.6618\n",
      "17/19, train_loss: 0.6609\n",
      "18/19, train_loss: 0.6595\n",
      "19/19, train_loss: 0.6603\n",
      "Epoch 4 average loss: 0.6666\n",
      "\n",
      "Epoch 5/300\n",
      "1/19, train_loss: 0.6590\n",
      "2/19, train_loss: 0.6588\n",
      "3/19, train_loss: 0.6576\n",
      "4/19, train_loss: 0.6574\n",
      "5/19, train_loss: 0.6565\n",
      "6/19, train_loss: 0.6553\n",
      "7/19, train_loss: 0.6549\n",
      "8/19, train_loss: 0.6541\n",
      "9/19, train_loss: 0.6543\n",
      "10/19, train_loss: 0.6530\n",
      "11/19, train_loss: 0.6520\n",
      "12/19, train_loss: 0.6514\n",
      "13/19, train_loss: 0.6508\n",
      "14/19, train_loss: 0.6503\n",
      "15/19, train_loss: 0.6497\n",
      "16/19, train_loss: 0.6489\n",
      "17/19, train_loss: 0.6489\n",
      "18/19, train_loss: 0.6473\n",
      "19/19, train_loss: 0.6471\n",
      "Epoch 5 average loss: 0.6530\n",
      "Saved new best model\n",
      "Current epoch: 5, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 6/300\n",
      "1/19, train_loss: 0.6464\n",
      "2/19, train_loss: 0.6457\n",
      "3/19, train_loss: 0.6446\n",
      "4/19, train_loss: 0.6444\n",
      "5/19, train_loss: 0.6439\n",
      "6/19, train_loss: 0.6433\n",
      "7/19, train_loss: 0.6429\n",
      "8/19, train_loss: 0.6422\n",
      "9/19, train_loss: 0.6417\n",
      "10/19, train_loss: 0.6413\n",
      "11/19, train_loss: 0.6402\n",
      "12/19, train_loss: 0.6400\n",
      "13/19, train_loss: 0.6390\n",
      "14/19, train_loss: 0.6389\n",
      "15/19, train_loss: 0.6378\n",
      "16/19, train_loss: 0.6375\n",
      "17/19, train_loss: 0.6371\n",
      "18/19, train_loss: 0.6359\n",
      "19/19, train_loss: 0.6367\n",
      "Epoch 6 average loss: 0.6410\n",
      "\n",
      "Epoch 7/300\n",
      "1/19, train_loss: 0.6351\n",
      "2/19, train_loss: 0.6343\n",
      "3/19, train_loss: 0.6337\n",
      "4/19, train_loss: 0.6329\n",
      "5/19, train_loss: 0.6330\n",
      "6/19, train_loss: 0.6322\n",
      "7/19, train_loss: 0.6320\n",
      "8/19, train_loss: 0.6308\n",
      "9/19, train_loss: 0.6298\n",
      "10/19, train_loss: 0.6296\n",
      "11/19, train_loss: 0.6293\n",
      "12/19, train_loss: 0.6286\n",
      "13/19, train_loss: 0.6287\n",
      "14/19, train_loss: 0.6273\n",
      "15/19, train_loss: 0.6270\n",
      "16/19, train_loss: 0.6265\n",
      "17/19, train_loss: 0.6263\n",
      "18/19, train_loss: 0.6253\n",
      "19/19, train_loss: 0.6248\n",
      "Epoch 7 average loss: 0.6299\n",
      "\n",
      "Epoch 8/300\n",
      "1/19, train_loss: 0.6241\n",
      "2/19, train_loss: 0.6236\n",
      "3/19, train_loss: 0.6235\n",
      "4/19, train_loss: 0.6229\n",
      "5/19, train_loss: 0.6220\n",
      "6/19, train_loss: 0.6216\n",
      "7/19, train_loss: 0.6216\n",
      "8/19, train_loss: 0.6206\n",
      "9/19, train_loss: 0.6197\n",
      "10/19, train_loss: 0.6196\n",
      "11/19, train_loss: 0.6193\n",
      "12/19, train_loss: 0.6186\n",
      "13/19, train_loss: 0.6186\n",
      "14/19, train_loss: 0.6178\n",
      "15/19, train_loss: 0.6174\n",
      "16/19, train_loss: 0.6170\n",
      "17/19, train_loss: 0.6161\n",
      "18/19, train_loss: 0.6157\n",
      "19/19, train_loss: 0.6153\n",
      "Epoch 8 average loss: 0.6197\n",
      "\n",
      "Epoch 9/300\n",
      "1/19, train_loss: 0.6146\n",
      "2/19, train_loss: 0.6141\n",
      "3/19, train_loss: 0.6141\n",
      "4/19, train_loss: 0.6134\n",
      "5/19, train_loss: 0.6127\n",
      "6/19, train_loss: 0.6122\n",
      "7/19, train_loss: 0.6123\n",
      "8/19, train_loss: 0.6117\n",
      "9/19, train_loss: 0.6109\n",
      "10/19, train_loss: 0.6107\n",
      "11/19, train_loss: 0.6100\n",
      "12/19, train_loss: 0.6100\n",
      "13/19, train_loss: 0.6092\n",
      "14/19, train_loss: 0.6085\n",
      "15/19, train_loss: 0.6085\n",
      "16/19, train_loss: 0.6079\n",
      "17/19, train_loss: 0.6075\n",
      "18/19, train_loss: 0.6069\n",
      "19/19, train_loss: 0.6069\n",
      "Epoch 9 average loss: 0.6106\n",
      "\n",
      "Epoch 10/300\n",
      "1/19, train_loss: 0.6062\n",
      "2/19, train_loss: 0.6057\n",
      "3/19, train_loss: 0.6053\n",
      "4/19, train_loss: 0.6051\n",
      "5/19, train_loss: 0.6045\n",
      "6/19, train_loss: 0.6041\n",
      "7/19, train_loss: 0.6040\n",
      "8/19, train_loss: 0.6035\n",
      "9/19, train_loss: 0.6032\n",
      "10/19, train_loss: 0.6026\n",
      "11/19, train_loss: 0.6026\n",
      "12/19, train_loss: 0.6019\n",
      "13/19, train_loss: 0.6015\n",
      "14/19, train_loss: 0.6011\n",
      "15/19, train_loss: 0.6007\n",
      "16/19, train_loss: 0.6003\n",
      "17/19, train_loss: 0.5997\n",
      "18/19, train_loss: 0.5997\n",
      "19/19, train_loss: 0.6003\n",
      "Epoch 10 average loss: 0.6027\n",
      "Current epoch: 10, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 11/300\n",
      "1/19, train_loss: 0.5990\n",
      "2/19, train_loss: 0.5985\n",
      "3/19, train_loss: 0.5982\n",
      "4/19, train_loss: 0.5980\n",
      "5/19, train_loss: 0.5975\n",
      "6/19, train_loss: 0.5975\n",
      "7/19, train_loss: 0.5969\n",
      "8/19, train_loss: 0.5964\n",
      "9/19, train_loss: 0.5965\n",
      "10/19, train_loss: 0.5959\n",
      "11/19, train_loss: 0.5956\n",
      "12/19, train_loss: 0.5952\n",
      "13/19, train_loss: 0.5949\n",
      "14/19, train_loss: 0.5945\n",
      "15/19, train_loss: 0.5945\n",
      "16/19, train_loss: 0.5940\n",
      "17/19, train_loss: 0.5932\n",
      "18/19, train_loss: 0.5927\n",
      "19/19, train_loss: 0.5927\n",
      "Epoch 11 average loss: 0.5959\n",
      "\n",
      "Epoch 12/300\n",
      "1/19, train_loss: 0.5916\n",
      "2/19, train_loss: 0.5912\n",
      "3/19, train_loss: 0.5907\n",
      "4/19, train_loss: 0.5902\n",
      "5/19, train_loss: 0.5901\n",
      "6/19, train_loss: 0.5890\n",
      "7/19, train_loss: 0.5885\n",
      "8/19, train_loss: 0.5878\n",
      "9/19, train_loss: 0.5871\n",
      "10/19, train_loss: 0.5869\n",
      "11/19, train_loss: 0.5867\n",
      "12/19, train_loss: 0.5855\n",
      "13/19, train_loss: 0.5852\n",
      "14/19, train_loss: 0.5843\n",
      "15/19, train_loss: 0.5837\n",
      "16/19, train_loss: 0.5828\n",
      "17/19, train_loss: 0.5821\n",
      "18/19, train_loss: 0.5814\n",
      "19/19, train_loss: 0.5801\n",
      "Epoch 12 average loss: 0.5866\n",
      "\n",
      "Epoch 13/300\n",
      "1/19, train_loss: 0.5795\n",
      "2/19, train_loss: 0.5789\n",
      "3/19, train_loss: 0.5778\n",
      "4/19, train_loss: 0.5774\n",
      "5/19, train_loss: 0.5767\n",
      "6/19, train_loss: 0.5756\n",
      "7/19, train_loss: 0.5749\n",
      "8/19, train_loss: 0.5746\n",
      "9/19, train_loss: 0.5737\n",
      "10/19, train_loss: 0.5739\n",
      "11/19, train_loss: 0.5725\n",
      "12/19, train_loss: 0.5726\n",
      "13/19, train_loss: 0.5718\n",
      "14/19, train_loss: 0.5717\n",
      "15/19, train_loss: 0.5711\n",
      "16/19, train_loss: 0.5706\n",
      "17/19, train_loss: 0.5706\n",
      "18/19, train_loss: 0.5702\n",
      "19/19, train_loss: 0.5698\n",
      "Epoch 13 average loss: 0.5739\n",
      "\n",
      "Epoch 14/300\n",
      "1/19, train_loss: 0.5693\n",
      "2/19, train_loss: 0.5689\n",
      "3/19, train_loss: 0.5687\n",
      "4/19, train_loss: 0.5684\n",
      "5/19, train_loss: 0.5681\n",
      "6/19, train_loss: 0.5680\n",
      "7/19, train_loss: 0.5676\n",
      "8/19, train_loss: 0.5672\n",
      "9/19, train_loss: 0.5670\n",
      "10/19, train_loss: 0.5666\n",
      "11/19, train_loss: 0.5666\n",
      "12/19, train_loss: 0.5661\n",
      "13/19, train_loss: 0.5659\n",
      "14/19, train_loss: 0.5663\n",
      "15/19, train_loss: 0.5654\n",
      "16/19, train_loss: 0.5655\n",
      "17/19, train_loss: 0.5649\n",
      "18/19, train_loss: 0.5649\n",
      "19/19, train_loss: 0.5644\n",
      "Epoch 14 average loss: 0.5668\n",
      "\n",
      "Epoch 15/300\n",
      "1/19, train_loss: 0.5643\n",
      "2/19, train_loss: 0.5641\n",
      "3/19, train_loss: 0.5638\n",
      "4/19, train_loss: 0.5638\n",
      "5/19, train_loss: 0.5635\n",
      "6/19, train_loss: 0.5634\n",
      "7/19, train_loss: 0.5631\n",
      "8/19, train_loss: 0.5627\n",
      "9/19, train_loss: 0.5626\n",
      "10/19, train_loss: 0.5627\n",
      "11/19, train_loss: 0.5622\n",
      "12/19, train_loss: 0.5624\n",
      "13/19, train_loss: 0.5619\n",
      "14/19, train_loss: 0.5616\n",
      "15/19, train_loss: 0.5614\n",
      "16/19, train_loss: 0.5612\n",
      "17/19, train_loss: 0.5613\n",
      "18/19, train_loss: 0.5608\n",
      "19/19, train_loss: 0.5613\n",
      "Epoch 15 average loss: 0.5625\n",
      "Current epoch: 15, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 16/300\n",
      "1/19, train_loss: 0.5605\n",
      "2/19, train_loss: 0.5603\n",
      "3/19, train_loss: 0.5602\n",
      "4/19, train_loss: 0.5600\n",
      "5/19, train_loss: 0.5600\n",
      "6/19, train_loss: 0.5598\n",
      "7/19, train_loss: 0.5593\n",
      "8/19, train_loss: 0.5595\n",
      "9/19, train_loss: 0.5591\n",
      "10/19, train_loss: 0.5590\n",
      "11/19, train_loss: 0.5587\n",
      "12/19, train_loss: 0.5588\n",
      "13/19, train_loss: 0.5583\n",
      "14/19, train_loss: 0.5583\n",
      "15/19, train_loss: 0.5581\n",
      "16/19, train_loss: 0.5579\n",
      "17/19, train_loss: 0.5578\n",
      "18/19, train_loss: 0.5576\n",
      "19/19, train_loss: 0.5581\n",
      "Epoch 16 average loss: 0.5590\n",
      "\n",
      "Epoch 17/300\n",
      "1/19, train_loss: 0.5573\n",
      "2/19, train_loss: 0.5575\n",
      "3/19, train_loss: 0.5568\n",
      "4/19, train_loss: 0.5568\n",
      "5/19, train_loss: 0.5566\n",
      "6/19, train_loss: 0.5565\n",
      "7/19, train_loss: 0.5563\n",
      "8/19, train_loss: 0.5561\n",
      "9/19, train_loss: 0.5561\n",
      "10/19, train_loss: 0.5559\n",
      "11/19, train_loss: 0.5557\n",
      "12/19, train_loss: 0.5555\n",
      "13/19, train_loss: 0.5557\n",
      "14/19, train_loss: 0.5554\n",
      "15/19, train_loss: 0.5553\n",
      "16/19, train_loss: 0.5550\n",
      "17/19, train_loss: 0.5548\n",
      "18/19, train_loss: 0.5547\n",
      "19/19, train_loss: 0.5545\n",
      "Epoch 17 average loss: 0.5559\n",
      "\n",
      "Epoch 18/300\n",
      "1/19, train_loss: 0.5544\n",
      "2/19, train_loss: 0.5543\n",
      "3/19, train_loss: 0.5540\n",
      "4/19, train_loss: 0.5539\n",
      "5/19, train_loss: 0.5539\n",
      "6/19, train_loss: 0.5538\n",
      "7/19, train_loss: 0.5535\n",
      "8/19, train_loss: 0.5533\n",
      "9/19, train_loss: 0.5532\n",
      "10/19, train_loss: 0.5531\n",
      "11/19, train_loss: 0.5529\n",
      "12/19, train_loss: 0.5528\n",
      "13/19, train_loss: 0.5526\n",
      "14/19, train_loss: 0.5528\n",
      "15/19, train_loss: 0.5524\n",
      "16/19, train_loss: 0.5521\n",
      "17/19, train_loss: 0.5521\n",
      "18/19, train_loss: 0.5519\n",
      "19/19, train_loss: 0.5522\n",
      "Epoch 18 average loss: 0.5531\n",
      "\n",
      "Epoch 19/300\n",
      "1/19, train_loss: 0.5517\n",
      "2/19, train_loss: 0.5517\n",
      "3/19, train_loss: 0.5515\n",
      "4/19, train_loss: 0.5512\n",
      "5/19, train_loss: 0.5511\n",
      "6/19, train_loss: 0.5510\n",
      "7/19, train_loss: 0.5508\n",
      "8/19, train_loss: 0.5507\n",
      "9/19, train_loss: 0.5507\n",
      "10/19, train_loss: 0.5505\n",
      "11/19, train_loss: 0.5503\n",
      "12/19, train_loss: 0.5501\n",
      "13/19, train_loss: 0.5505\n",
      "14/19, train_loss: 0.5500\n",
      "15/19, train_loss: 0.5498\n",
      "16/19, train_loss: 0.5498\n",
      "17/19, train_loss: 0.5495\n",
      "18/19, train_loss: 0.5494\n",
      "19/19, train_loss: 0.5493\n",
      "Epoch 19 average loss: 0.5505\n",
      "\n",
      "Epoch 20/300\n",
      "1/19, train_loss: 0.5492\n",
      "2/19, train_loss: 0.5491\n",
      "3/19, train_loss: 0.5488\n",
      "4/19, train_loss: 0.5487\n",
      "5/19, train_loss: 0.5485\n",
      "6/19, train_loss: 0.5484\n",
      "7/19, train_loss: 0.5486\n",
      "8/19, train_loss: 0.5481\n",
      "9/19, train_loss: 0.5481\n",
      "10/19, train_loss: 0.5480\n",
      "11/19, train_loss: 0.5477\n",
      "12/19, train_loss: 0.5478\n",
      "13/19, train_loss: 0.5475\n",
      "14/19, train_loss: 0.5474\n",
      "15/19, train_loss: 0.5473\n",
      "16/19, train_loss: 0.5471\n",
      "17/19, train_loss: 0.5470\n",
      "18/19, train_loss: 0.5468\n",
      "19/19, train_loss: 0.5467\n",
      "Epoch 20 average loss: 0.5479\n",
      "Current epoch: 20, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 21/300\n",
      "1/19, train_loss: 0.5466\n",
      "2/19, train_loss: 0.5464\n",
      "3/19, train_loss: 0.5463\n",
      "4/19, train_loss: 0.5461\n",
      "5/19, train_loss: 0.5462\n",
      "6/19, train_loss: 0.5462\n",
      "7/19, train_loss: 0.5459\n",
      "8/19, train_loss: 0.5457\n",
      "9/19, train_loss: 0.5455\n",
      "10/19, train_loss: 0.5454\n",
      "11/19, train_loss: 0.5452\n",
      "12/19, train_loss: 0.5454\n",
      "13/19, train_loss: 0.5451\n",
      "14/19, train_loss: 0.5451\n",
      "15/19, train_loss: 0.5448\n",
      "16/19, train_loss: 0.5447\n",
      "17/19, train_loss: 0.5448\n",
      "18/19, train_loss: 0.5445\n",
      "19/19, train_loss: 0.5443\n",
      "Epoch 21 average loss: 0.5455\n",
      "\n",
      "Epoch 22/300\n",
      "1/19, train_loss: 0.5443\n",
      "2/19, train_loss: 0.5443\n",
      "3/19, train_loss: 0.5440\n",
      "4/19, train_loss: 0.5440\n",
      "5/19, train_loss: 0.5437\n",
      "6/19, train_loss: 0.5436\n",
      "7/19, train_loss: 0.5435\n",
      "8/19, train_loss: 0.5435\n",
      "9/19, train_loss: 0.5435\n",
      "10/19, train_loss: 0.5433\n",
      "11/19, train_loss: 0.5430\n",
      "12/19, train_loss: 0.5430\n",
      "13/19, train_loss: 0.5428\n",
      "14/19, train_loss: 0.5428\n",
      "15/19, train_loss: 0.5426\n",
      "16/19, train_loss: 0.5424\n",
      "17/19, train_loss: 0.5424\n",
      "18/19, train_loss: 0.5425\n",
      "19/19, train_loss: 0.5422\n",
      "Epoch 22 average loss: 0.5432\n",
      "\n",
      "Epoch 23/300\n",
      "1/19, train_loss: 0.5420\n",
      "2/19, train_loss: 0.5419\n",
      "3/19, train_loss: 0.5418\n",
      "4/19, train_loss: 0.5418\n",
      "5/19, train_loss: 0.5416\n",
      "6/19, train_loss: 0.5417\n",
      "7/19, train_loss: 0.5415\n",
      "8/19, train_loss: 0.5412\n",
      "9/19, train_loss: 0.5412\n",
      "10/19, train_loss: 0.5411\n",
      "11/19, train_loss: 0.5410\n",
      "12/19, train_loss: 0.5409\n",
      "13/19, train_loss: 0.5409\n",
      "14/19, train_loss: 0.5408\n",
      "15/19, train_loss: 0.5407\n",
      "16/19, train_loss: 0.5405\n",
      "17/19, train_loss: 0.5404\n",
      "18/19, train_loss: 0.5402\n",
      "19/19, train_loss: 0.5402\n",
      "Epoch 23 average loss: 0.5411\n",
      "\n",
      "Epoch 24/300\n",
      "1/19, train_loss: 0.5401\n",
      "2/19, train_loss: 0.5400\n",
      "3/19, train_loss: 0.5399\n",
      "4/19, train_loss: 0.5398\n",
      "5/19, train_loss: 0.5397\n",
      "6/19, train_loss: 0.5396\n",
      "7/19, train_loss: 0.5395\n",
      "8/19, train_loss: 0.5395\n",
      "9/19, train_loss: 0.5393\n",
      "10/19, train_loss: 0.5393\n",
      "11/19, train_loss: 0.5391\n",
      "12/19, train_loss: 0.5390\n",
      "13/19, train_loss: 0.5391\n",
      "14/19, train_loss: 0.5388\n",
      "15/19, train_loss: 0.5387\n",
      "16/19, train_loss: 0.5387\n",
      "17/19, train_loss: 0.5386\n",
      "18/19, train_loss: 0.5384\n",
      "19/19, train_loss: 0.5383\n",
      "Epoch 24 average loss: 0.5392\n",
      "\n",
      "Epoch 25/300\n",
      "1/19, train_loss: 0.5382\n",
      "2/19, train_loss: 0.5382\n",
      "3/19, train_loss: 0.5381\n",
      "4/19, train_loss: 0.5380\n",
      "5/19, train_loss: 0.5378\n",
      "6/19, train_loss: 0.5377\n",
      "7/19, train_loss: 0.5378\n",
      "8/19, train_loss: 0.5376\n",
      "9/19, train_loss: 0.5376\n",
      "10/19, train_loss: 0.5374\n",
      "11/19, train_loss: 0.5374\n",
      "12/19, train_loss: 0.5373\n",
      "13/19, train_loss: 0.5374\n",
      "14/19, train_loss: 0.5371\n",
      "15/19, train_loss: 0.5370\n",
      "16/19, train_loss: 0.5369\n",
      "17/19, train_loss: 0.5368\n",
      "18/19, train_loss: 0.5368\n",
      "19/19, train_loss: 0.5366\n",
      "Epoch 25 average loss: 0.5375\n",
      "Current epoch: 25, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 26/300\n",
      "1/19, train_loss: 0.5365\n",
      "2/19, train_loss: 0.5366\n",
      "3/19, train_loss: 0.5365\n",
      "4/19, train_loss: 0.5363\n",
      "5/19, train_loss: 0.5362\n",
      "6/19, train_loss: 0.5361\n",
      "7/19, train_loss: 0.5361\n",
      "8/19, train_loss: 0.5360\n",
      "9/19, train_loss: 0.5358\n",
      "10/19, train_loss: 0.5359\n",
      "11/19, train_loss: 0.5357\n",
      "12/19, train_loss: 0.5356\n",
      "13/19, train_loss: 0.5355\n",
      "14/19, train_loss: 0.5354\n",
      "15/19, train_loss: 0.5354\n",
      "16/19, train_loss: 0.5353\n",
      "17/19, train_loss: 0.5353\n",
      "18/19, train_loss: 0.5351\n",
      "19/19, train_loss: 0.5350\n",
      "Epoch 26 average loss: 0.5358\n",
      "\n",
      "Epoch 27/300\n",
      "1/19, train_loss: 0.5350\n",
      "2/19, train_loss: 0.5349\n",
      "3/19, train_loss: 0.5348\n",
      "4/19, train_loss: 0.5348\n",
      "5/19, train_loss: 0.5346\n",
      "6/19, train_loss: 0.5346\n",
      "7/19, train_loss: 0.5345\n",
      "8/19, train_loss: 0.5344\n",
      "9/19, train_loss: 0.5343\n",
      "10/19, train_loss: 0.5343\n",
      "11/19, train_loss: 0.5343\n",
      "12/19, train_loss: 0.5341\n",
      "13/19, train_loss: 0.5340\n",
      "14/19, train_loss: 0.5339\n",
      "15/19, train_loss: 0.5339\n",
      "16/19, train_loss: 0.5339\n",
      "17/19, train_loss: 0.5338\n",
      "18/19, train_loss: 0.5336\n",
      "19/19, train_loss: 0.5337\n",
      "Epoch 27 average loss: 0.5343\n",
      "\n",
      "Epoch 28/300\n",
      "1/19, train_loss: 0.5335\n",
      "2/19, train_loss: 0.5334\n",
      "3/19, train_loss: 0.5334\n",
      "4/19, train_loss: 0.5333\n",
      "5/19, train_loss: 0.5332\n",
      "6/19, train_loss: 0.5333\n",
      "7/19, train_loss: 0.5331\n",
      "8/19, train_loss: 0.5331\n",
      "9/19, train_loss: 0.5330\n",
      "10/19, train_loss: 0.5329\n",
      "11/19, train_loss: 0.5328\n",
      "12/19, train_loss: 0.5327\n",
      "13/19, train_loss: 0.5326\n",
      "14/19, train_loss: 0.5325\n",
      "15/19, train_loss: 0.5325\n",
      "16/19, train_loss: 0.5324\n",
      "17/19, train_loss: 0.5322\n",
      "18/19, train_loss: 0.5322\n",
      "19/19, train_loss: 0.5322\n",
      "Epoch 28 average loss: 0.5329\n",
      "\n",
      "Epoch 29/300\n",
      "1/19, train_loss: 0.5322\n",
      "2/19, train_loss: 0.5320\n",
      "3/19, train_loss: 0.5320\n",
      "4/19, train_loss: 0.5319\n",
      "5/19, train_loss: 0.5318\n",
      "6/19, train_loss: 0.5318\n",
      "7/19, train_loss: 0.5317\n",
      "8/19, train_loss: 0.5318\n",
      "9/19, train_loss: 0.5317\n",
      "10/19, train_loss: 0.5316\n",
      "11/19, train_loss: 0.5314\n",
      "12/19, train_loss: 0.5314\n",
      "13/19, train_loss: 0.5313\n",
      "14/19, train_loss: 0.5312\n",
      "15/19, train_loss: 0.5313\n",
      "16/19, train_loss: 0.5311\n",
      "17/19, train_loss: 0.5311\n",
      "18/19, train_loss: 0.5310\n",
      "19/19, train_loss: 0.5309\n",
      "Epoch 29 average loss: 0.5315\n",
      "\n",
      "Epoch 30/300\n",
      "1/19, train_loss: 0.5310\n",
      "2/19, train_loss: 0.5308\n",
      "3/19, train_loss: 0.5308\n",
      "4/19, train_loss: 0.5307\n",
      "5/19, train_loss: 0.5306\n",
      "6/19, train_loss: 0.5305\n",
      "7/19, train_loss: 0.5304\n",
      "8/19, train_loss: 0.5304\n",
      "9/19, train_loss: 0.5303\n",
      "10/19, train_loss: 0.5303\n",
      "11/19, train_loss: 0.5302\n",
      "12/19, train_loss: 0.5302\n",
      "13/19, train_loss: 0.5301\n",
      "14/19, train_loss: 0.5301\n",
      "15/19, train_loss: 0.5299\n",
      "16/19, train_loss: 0.5299\n",
      "17/19, train_loss: 0.5298\n",
      "18/19, train_loss: 0.5298\n",
      "19/19, train_loss: 0.5297\n",
      "Epoch 30 average loss: 0.5303\n",
      "Current epoch: 30, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 31/300\n",
      "1/19, train_loss: 0.5296\n",
      "2/19, train_loss: 0.5297\n",
      "3/19, train_loss: 0.5295\n",
      "4/19, train_loss: 0.5296\n",
      "5/19, train_loss: 0.5294\n",
      "6/19, train_loss: 0.5293\n",
      "7/19, train_loss: 0.5293\n",
      "8/19, train_loss: 0.5293\n",
      "9/19, train_loss: 0.5292\n",
      "10/19, train_loss: 0.5292\n",
      "11/19, train_loss: 0.5290\n",
      "12/19, train_loss: 0.5290\n",
      "13/19, train_loss: 0.5290\n",
      "14/19, train_loss: 0.5289\n",
      "15/19, train_loss: 0.5288\n",
      "16/19, train_loss: 0.5289\n",
      "17/19, train_loss: 0.5287\n",
      "18/19, train_loss: 0.5286\n",
      "19/19, train_loss: 0.5286\n",
      "Epoch 31 average loss: 0.5291\n",
      "\n",
      "Epoch 32/300\n",
      "1/19, train_loss: 0.5286\n",
      "2/19, train_loss: 0.5285\n",
      "3/19, train_loss: 0.5284\n",
      "4/19, train_loss: 0.5283\n",
      "5/19, train_loss: 0.5284\n",
      "6/19, train_loss: 0.5282\n",
      "7/19, train_loss: 0.5282\n",
      "8/19, train_loss: 0.5281\n",
      "9/19, train_loss: 0.5281\n",
      "10/19, train_loss: 0.5280\n",
      "11/19, train_loss: 0.5279\n",
      "12/19, train_loss: 0.5279\n",
      "13/19, train_loss: 0.5278\n",
      "14/19, train_loss: 0.5278\n",
      "15/19, train_loss: 0.5278\n",
      "16/19, train_loss: 0.5277\n",
      "17/19, train_loss: 0.5276\n",
      "18/19, train_loss: 0.5276\n",
      "19/19, train_loss: 0.5276\n",
      "Epoch 32 average loss: 0.5280\n",
      "\n",
      "Epoch 33/300\n",
      "1/19, train_loss: 0.5274\n",
      "2/19, train_loss: 0.5274\n",
      "3/19, train_loss: 0.5274\n",
      "4/19, train_loss: 0.5274\n",
      "5/19, train_loss: 0.5273\n",
      "6/19, train_loss: 0.5272\n",
      "7/19, train_loss: 0.5272\n",
      "8/19, train_loss: 0.5272\n",
      "9/19, train_loss: 0.5270\n",
      "10/19, train_loss: 0.5270\n",
      "11/19, train_loss: 0.5269\n",
      "12/19, train_loss: 0.5269\n",
      "13/19, train_loss: 0.5268\n",
      "14/19, train_loss: 0.5268\n",
      "15/19, train_loss: 0.5267\n",
      "16/19, train_loss: 0.5266\n",
      "17/19, train_loss: 0.5266\n",
      "18/19, train_loss: 0.5266\n",
      "19/19, train_loss: 0.5267\n",
      "Epoch 33 average loss: 0.5270\n",
      "\n",
      "Epoch 34/300\n",
      "1/19, train_loss: 0.5265\n",
      "2/19, train_loss: 0.5265\n",
      "3/19, train_loss: 0.5264\n",
      "4/19, train_loss: 0.5263\n",
      "5/19, train_loss: 0.5262\n",
      "6/19, train_loss: 0.5262\n",
      "7/19, train_loss: 0.5262\n",
      "8/19, train_loss: 0.5261\n",
      "9/19, train_loss: 0.5261\n",
      "10/19, train_loss: 0.5260\n",
      "11/19, train_loss: 0.5261\n",
      "12/19, train_loss: 0.5260\n",
      "13/19, train_loss: 0.5260\n",
      "14/19, train_loss: 0.5258\n",
      "15/19, train_loss: 0.5258\n",
      "16/19, train_loss: 0.5257\n",
      "17/19, train_loss: 0.5257\n",
      "18/19, train_loss: 0.5257\n",
      "19/19, train_loss: 0.5256\n",
      "Epoch 34 average loss: 0.5260\n",
      "\n",
      "Epoch 35/300\n",
      "1/19, train_loss: 0.5255\n",
      "2/19, train_loss: 0.5256\n",
      "3/19, train_loss: 0.5255\n",
      "4/19, train_loss: 0.5254\n",
      "5/19, train_loss: 0.5253\n",
      "6/19, train_loss: 0.5253\n",
      "7/19, train_loss: 0.5253\n",
      "8/19, train_loss: 0.5252\n",
      "9/19, train_loss: 0.5251\n",
      "10/19, train_loss: 0.5252\n",
      "11/19, train_loss: 0.5251\n",
      "12/19, train_loss: 0.5250\n",
      "13/19, train_loss: 0.5249\n",
      "14/19, train_loss: 0.5249\n",
      "15/19, train_loss: 0.5249\n",
      "16/19, train_loss: 0.5248\n",
      "17/19, train_loss: 0.5248\n",
      "18/19, train_loss: 0.5248\n",
      "19/19, train_loss: 0.5247\n",
      "Epoch 35 average loss: 0.5251\n",
      "Current epoch: 35, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 36/300\n",
      "1/19, train_loss: 0.5246\n",
      "2/19, train_loss: 0.5246\n",
      "3/19, train_loss: 0.5246\n",
      "4/19, train_loss: 0.5246\n",
      "5/19, train_loss: 0.5245\n",
      "6/19, train_loss: 0.5244\n",
      "7/19, train_loss: 0.5244\n",
      "8/19, train_loss: 0.5244\n",
      "9/19, train_loss: 0.5243\n",
      "10/19, train_loss: 0.5242\n",
      "11/19, train_loss: 0.5242\n",
      "12/19, train_loss: 0.5241\n",
      "13/19, train_loss: 0.5242\n",
      "14/19, train_loss: 0.5241\n",
      "15/19, train_loss: 0.5240\n",
      "16/19, train_loss: 0.5239\n",
      "17/19, train_loss: 0.5239\n",
      "18/19, train_loss: 0.5239\n",
      "19/19, train_loss: 0.5239\n",
      "Epoch 36 average loss: 0.5242\n",
      "\n",
      "Epoch 37/300\n",
      "1/19, train_loss: 0.5238\n",
      "2/19, train_loss: 0.5238\n",
      "3/19, train_loss: 0.5237\n",
      "4/19, train_loss: 0.5236\n",
      "5/19, train_loss: 0.5236\n",
      "6/19, train_loss: 0.5236\n",
      "7/19, train_loss: 0.5235\n",
      "8/19, train_loss: 0.5236\n",
      "9/19, train_loss: 0.5235\n",
      "10/19, train_loss: 0.5234\n",
      "11/19, train_loss: 0.5234\n",
      "12/19, train_loss: 0.5233\n",
      "13/19, train_loss: 0.5233\n",
      "14/19, train_loss: 0.5232\n",
      "15/19, train_loss: 0.5232\n",
      "16/19, train_loss: 0.5232\n",
      "17/19, train_loss: 0.5231\n",
      "18/19, train_loss: 0.5231\n",
      "19/19, train_loss: 0.5231\n",
      "Epoch 37 average loss: 0.5234\n",
      "\n",
      "Epoch 38/300\n",
      "1/19, train_loss: 0.5231\n",
      "2/19, train_loss: 0.5230\n",
      "3/19, train_loss: 0.5230\n",
      "4/19, train_loss: 0.5229\n",
      "5/19, train_loss: 0.5228\n",
      "6/19, train_loss: 0.5228\n",
      "7/19, train_loss: 0.5228\n",
      "8/19, train_loss: 0.5227\n",
      "9/19, train_loss: 0.5227\n",
      "10/19, train_loss: 0.5226\n",
      "11/19, train_loss: 0.5226\n",
      "12/19, train_loss: 0.5225\n",
      "13/19, train_loss: 0.5225\n",
      "14/19, train_loss: 0.5225\n",
      "15/19, train_loss: 0.5224\n",
      "16/19, train_loss: 0.5224\n",
      "17/19, train_loss: 0.5224\n",
      "18/19, train_loss: 0.5224\n",
      "19/19, train_loss: 0.5223\n",
      "Epoch 38 average loss: 0.5227\n",
      "\n",
      "Epoch 39/300\n",
      "1/19, train_loss: 0.5223\n",
      "2/19, train_loss: 0.5222\n",
      "3/19, train_loss: 0.5222\n",
      "4/19, train_loss: 0.5221\n",
      "5/19, train_loss: 0.5221\n",
      "6/19, train_loss: 0.5221\n",
      "7/19, train_loss: 0.5220\n",
      "8/19, train_loss: 0.5220\n",
      "9/19, train_loss: 0.5219\n",
      "10/19, train_loss: 0.5219\n",
      "11/19, train_loss: 0.5219\n",
      "12/19, train_loss: 0.5219\n",
      "13/19, train_loss: 0.5218\n",
      "14/19, train_loss: 0.5217\n",
      "15/19, train_loss: 0.5217\n",
      "16/19, train_loss: 0.5217\n",
      "17/19, train_loss: 0.5217\n",
      "18/19, train_loss: 0.5216\n",
      "19/19, train_loss: 0.5216\n",
      "Epoch 39 average loss: 0.5219\n",
      "\n",
      "Epoch 40/300\n",
      "1/19, train_loss: 0.5215\n",
      "2/19, train_loss: 0.5215\n",
      "3/19, train_loss: 0.5215\n",
      "4/19, train_loss: 0.5214\n",
      "5/19, train_loss: 0.5214\n",
      "6/19, train_loss: 0.5214\n",
      "7/19, train_loss: 0.5213\n",
      "8/19, train_loss: 0.5213\n",
      "9/19, train_loss: 0.5213\n",
      "10/19, train_loss: 0.5212\n",
      "11/19, train_loss: 0.5212\n",
      "12/19, train_loss: 0.5212\n",
      "13/19, train_loss: 0.5211\n",
      "14/19, train_loss: 0.5211\n",
      "15/19, train_loss: 0.5210\n",
      "16/19, train_loss: 0.5210\n",
      "17/19, train_loss: 0.5210\n",
      "18/19, train_loss: 0.5209\n",
      "19/19, train_loss: 0.5210\n",
      "Epoch 40 average loss: 0.5212\n",
      "Current epoch: 40, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 41/300\n",
      "1/19, train_loss: 0.5208\n",
      "2/19, train_loss: 0.5209\n",
      "3/19, train_loss: 0.5208\n",
      "4/19, train_loss: 0.5207\n",
      "5/19, train_loss: 0.5207\n",
      "6/19, train_loss: 0.5207\n",
      "7/19, train_loss: 0.5206\n",
      "8/19, train_loss: 0.5206\n",
      "9/19, train_loss: 0.5205\n",
      "10/19, train_loss: 0.5205\n",
      "11/19, train_loss: 0.5205\n",
      "12/19, train_loss: 0.5204\n",
      "13/19, train_loss: 0.5205\n",
      "14/19, train_loss: 0.5204\n",
      "15/19, train_loss: 0.5204\n",
      "16/19, train_loss: 0.5203\n",
      "17/19, train_loss: 0.5203\n",
      "18/19, train_loss: 0.5203\n",
      "19/19, train_loss: 0.5203\n",
      "Epoch 41 average loss: 0.5205\n",
      "\n",
      "Epoch 42/300\n",
      "1/19, train_loss: 0.5202\n",
      "2/19, train_loss: 0.5202\n",
      "3/19, train_loss: 0.5201\n",
      "4/19, train_loss: 0.5201\n",
      "5/19, train_loss: 0.5201\n",
      "6/19, train_loss: 0.5201\n",
      "7/19, train_loss: 0.5200\n",
      "8/19, train_loss: 0.5200\n",
      "9/19, train_loss: 0.5199\n",
      "10/19, train_loss: 0.5199\n",
      "11/19, train_loss: 0.5199\n",
      "12/19, train_loss: 0.5198\n",
      "13/19, train_loss: 0.5199\n",
      "14/19, train_loss: 0.5198\n",
      "15/19, train_loss: 0.5197\n",
      "16/19, train_loss: 0.5197\n",
      "17/19, train_loss: 0.5197\n",
      "18/19, train_loss: 0.5196\n",
      "19/19, train_loss: 0.5196\n",
      "Epoch 42 average loss: 0.5199\n",
      "\n",
      "Epoch 43/300\n",
      "1/19, train_loss: 0.5196\n",
      "2/19, train_loss: 0.5195\n",
      "3/19, train_loss: 0.5195\n",
      "4/19, train_loss: 0.5195\n",
      "5/19, train_loss: 0.5195\n",
      "6/19, train_loss: 0.5194\n",
      "7/19, train_loss: 0.5194\n",
      "8/19, train_loss: 0.5193\n",
      "9/19, train_loss: 0.5193\n",
      "10/19, train_loss: 0.5193\n",
      "11/19, train_loss: 0.5193\n",
      "12/19, train_loss: 0.5193\n",
      "13/19, train_loss: 0.5192\n",
      "14/19, train_loss: 0.5192\n",
      "15/19, train_loss: 0.5191\n",
      "16/19, train_loss: 0.5191\n",
      "17/19, train_loss: 0.5191\n",
      "18/19, train_loss: 0.5191\n",
      "19/19, train_loss: 0.5190\n",
      "Epoch 43 average loss: 0.5193\n",
      "\n",
      "Epoch 44/300\n",
      "1/19, train_loss: 0.5190\n",
      "2/19, train_loss: 0.5190\n",
      "3/19, train_loss: 0.5189\n",
      "4/19, train_loss: 0.5189\n",
      "5/19, train_loss: 0.5189\n",
      "6/19, train_loss: 0.5189\n",
      "7/19, train_loss: 0.5188\n",
      "8/19, train_loss: 0.5188\n",
      "9/19, train_loss: 0.5188\n",
      "10/19, train_loss: 0.5188\n",
      "11/19, train_loss: 0.5187\n",
      "12/19, train_loss: 0.5187\n",
      "13/19, train_loss: 0.5187\n",
      "14/19, train_loss: 0.5186\n",
      "15/19, train_loss: 0.5186\n",
      "16/19, train_loss: 0.5185\n",
      "17/19, train_loss: 0.5185\n",
      "18/19, train_loss: 0.5185\n",
      "19/19, train_loss: 0.5184\n",
      "Epoch 44 average loss: 0.5187\n",
      "\n",
      "Epoch 45/300\n",
      "1/19, train_loss: 0.5185\n",
      "2/19, train_loss: 0.5184\n",
      "3/19, train_loss: 0.5184\n",
      "4/19, train_loss: 0.5183\n",
      "5/19, train_loss: 0.5183\n",
      "6/19, train_loss: 0.5183\n",
      "7/19, train_loss: 0.5182\n",
      "8/19, train_loss: 0.5182\n",
      "9/19, train_loss: 0.5182\n",
      "10/19, train_loss: 0.5182\n",
      "11/19, train_loss: 0.5182\n",
      "12/19, train_loss: 0.5181\n",
      "13/19, train_loss: 0.5181\n",
      "14/19, train_loss: 0.5180\n",
      "15/19, train_loss: 0.5180\n",
      "16/19, train_loss: 0.5180\n",
      "17/19, train_loss: 0.5180\n",
      "18/19, train_loss: 0.5179\n",
      "19/19, train_loss: 0.5179\n",
      "Epoch 45 average loss: 0.5182\n",
      "Current epoch: 45, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 46/300\n",
      "1/19, train_loss: 0.5179\n",
      "2/19, train_loss: 0.5179\n",
      "3/19, train_loss: 0.5178\n",
      "4/19, train_loss: 0.5178\n",
      "5/19, train_loss: 0.5178\n",
      "6/19, train_loss: 0.5178\n",
      "7/19, train_loss: 0.5177\n",
      "8/19, train_loss: 0.5177\n",
      "9/19, train_loss: 0.5177\n",
      "10/19, train_loss: 0.5177\n",
      "11/19, train_loss: 0.5176\n",
      "12/19, train_loss: 0.5175\n",
      "13/19, train_loss: 0.5175\n",
      "14/19, train_loss: 0.5175\n",
      "15/19, train_loss: 0.5175\n",
      "16/19, train_loss: 0.5174\n",
      "17/19, train_loss: 0.5174\n",
      "18/19, train_loss: 0.5174\n",
      "19/19, train_loss: 0.5174\n",
      "Epoch 46 average loss: 0.5176\n",
      "\n",
      "Epoch 47/300\n",
      "1/19, train_loss: 0.5173\n",
      "2/19, train_loss: 0.5173\n",
      "3/19, train_loss: 0.5173\n",
      "4/19, train_loss: 0.5173\n",
      "5/19, train_loss: 0.5173\n",
      "6/19, train_loss: 0.5172\n",
      "7/19, train_loss: 0.5172\n",
      "8/19, train_loss: 0.5172\n",
      "9/19, train_loss: 0.5171\n",
      "10/19, train_loss: 0.5171\n",
      "11/19, train_loss: 0.5171\n",
      "12/19, train_loss: 0.5171\n",
      "13/19, train_loss: 0.5170\n",
      "14/19, train_loss: 0.5170\n",
      "15/19, train_loss: 0.5170\n",
      "16/19, train_loss: 0.5169\n",
      "17/19, train_loss: 0.5170\n",
      "18/19, train_loss: 0.5169\n",
      "19/19, train_loss: 0.5169\n",
      "Epoch 47 average loss: 0.5171\n",
      "\n",
      "Epoch 48/300\n",
      "1/19, train_loss: 0.5168\n",
      "2/19, train_loss: 0.5168\n",
      "3/19, train_loss: 0.5168\n",
      "4/19, train_loss: 0.5168\n",
      "5/19, train_loss: 0.5167\n",
      "6/19, train_loss: 0.5167\n",
      "7/19, train_loss: 0.5167\n",
      "8/19, train_loss: 0.5167\n",
      "9/19, train_loss: 0.5166\n",
      "10/19, train_loss: 0.5166\n",
      "11/19, train_loss: 0.5166\n",
      "12/19, train_loss: 0.5166\n",
      "13/19, train_loss: 0.5165\n",
      "14/19, train_loss: 0.5165\n",
      "15/19, train_loss: 0.5165\n",
      "16/19, train_loss: 0.5165\n",
      "17/19, train_loss: 0.5165\n",
      "18/19, train_loss: 0.5165\n",
      "19/19, train_loss: 0.5164\n",
      "Epoch 48 average loss: 0.5166\n",
      "\n",
      "Epoch 49/300\n",
      "1/19, train_loss: 0.5164\n",
      "2/19, train_loss: 0.5163\n",
      "3/19, train_loss: 0.5163\n",
      "4/19, train_loss: 0.5163\n",
      "5/19, train_loss: 0.5163\n",
      "6/19, train_loss: 0.5163\n",
      "7/19, train_loss: 0.5162\n",
      "8/19, train_loss: 0.5162\n",
      "9/19, train_loss: 0.5162\n",
      "10/19, train_loss: 0.5161\n",
      "11/19, train_loss: 0.5161\n",
      "12/19, train_loss: 0.5161\n",
      "13/19, train_loss: 0.5161\n",
      "14/19, train_loss: 0.5160\n",
      "15/19, train_loss: 0.5161\n",
      "16/19, train_loss: 0.5160\n",
      "17/19, train_loss: 0.5160\n",
      "18/19, train_loss: 0.5159\n",
      "19/19, train_loss: 0.5159\n",
      "Epoch 49 average loss: 0.5161\n",
      "\n",
      "Epoch 50/300\n",
      "1/19, train_loss: 0.5159\n",
      "2/19, train_loss: 0.5159\n",
      "3/19, train_loss: 0.5158\n",
      "4/19, train_loss: 0.5158\n",
      "5/19, train_loss: 0.5159\n",
      "6/19, train_loss: 0.5158\n",
      "7/19, train_loss: 0.5157\n",
      "8/19, train_loss: 0.5158\n",
      "9/19, train_loss: 0.5157\n",
      "10/19, train_loss: 0.5157\n",
      "11/19, train_loss: 0.5157\n",
      "12/19, train_loss: 0.5156\n",
      "13/19, train_loss: 0.5156\n",
      "14/19, train_loss: 0.5156\n",
      "15/19, train_loss: 0.5156\n",
      "16/19, train_loss: 0.5156\n",
      "17/19, train_loss: 0.5155\n",
      "18/19, train_loss: 0.5155\n",
      "19/19, train_loss: 0.5155\n",
      "Epoch 50 average loss: 0.5157\n",
      "Current epoch: 50, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 51/300\n",
      "1/19, train_loss: 0.5155\n",
      "2/19, train_loss: 0.5154\n",
      "3/19, train_loss: 0.5154\n",
      "4/19, train_loss: 0.5154\n",
      "5/19, train_loss: 0.5154\n",
      "6/19, train_loss: 0.5153\n",
      "7/19, train_loss: 0.5153\n",
      "8/19, train_loss: 0.5153\n",
      "9/19, train_loss: 0.5153\n",
      "10/19, train_loss: 0.5152\n",
      "11/19, train_loss: 0.5152\n",
      "12/19, train_loss: 0.5152\n",
      "13/19, train_loss: 0.5152\n",
      "14/19, train_loss: 0.5151\n",
      "15/19, train_loss: 0.5152\n",
      "16/19, train_loss: 0.5151\n",
      "17/19, train_loss: 0.5151\n",
      "18/19, train_loss: 0.5151\n",
      "19/19, train_loss: 0.5150\n",
      "Epoch 51 average loss: 0.5152\n",
      "\n",
      "Epoch 52/300\n",
      "1/19, train_loss: 0.5150\n",
      "2/19, train_loss: 0.5150\n",
      "3/19, train_loss: 0.5150\n",
      "4/19, train_loss: 0.5149\n",
      "5/19, train_loss: 0.5149\n",
      "6/19, train_loss: 0.5149\n",
      "7/19, train_loss: 0.5149\n",
      "8/19, train_loss: 0.5149\n",
      "9/19, train_loss: 0.5148\n",
      "10/19, train_loss: 0.5148\n",
      "11/19, train_loss: 0.5148\n",
      "12/19, train_loss: 0.5148\n",
      "13/19, train_loss: 0.5147\n",
      "14/19, train_loss: 0.5147\n",
      "15/19, train_loss: 0.5147\n",
      "16/19, train_loss: 0.5147\n",
      "17/19, train_loss: 0.5147\n",
      "18/19, train_loss: 0.5147\n",
      "19/19, train_loss: 0.5146\n",
      "Epoch 52 average loss: 0.5148\n",
      "\n",
      "Epoch 53/300\n",
      "1/19, train_loss: 0.5146\n",
      "2/19, train_loss: 0.5146\n",
      "3/19, train_loss: 0.5146\n",
      "4/19, train_loss: 0.5146\n",
      "5/19, train_loss: 0.5145\n",
      "6/19, train_loss: 0.5145\n",
      "7/19, train_loss: 0.5145\n",
      "8/19, train_loss: 0.5144\n",
      "9/19, train_loss: 0.5144\n",
      "10/19, train_loss: 0.5144\n",
      "11/19, train_loss: 0.5144\n",
      "12/19, train_loss: 0.5144\n",
      "13/19, train_loss: 0.5144\n",
      "14/19, train_loss: 0.5144\n",
      "15/19, train_loss: 0.5143\n",
      "16/19, train_loss: 0.5143\n",
      "17/19, train_loss: 0.5143\n",
      "18/19, train_loss: 0.5143\n",
      "19/19, train_loss: 0.5142\n",
      "Epoch 53 average loss: 0.5144\n",
      "\n",
      "Epoch 54/300\n",
      "1/19, train_loss: 0.5142\n",
      "2/19, train_loss: 0.5142\n",
      "3/19, train_loss: 0.5142\n",
      "4/19, train_loss: 0.5142\n",
      "5/19, train_loss: 0.5141\n",
      "6/19, train_loss: 0.5141\n",
      "7/19, train_loss: 0.5141\n",
      "8/19, train_loss: 0.5141\n",
      "9/19, train_loss: 0.5140\n",
      "10/19, train_loss: 0.5140\n",
      "11/19, train_loss: 0.5140\n",
      "12/19, train_loss: 0.5140\n",
      "13/19, train_loss: 0.5140\n",
      "14/19, train_loss: 0.5140\n",
      "15/19, train_loss: 0.5139\n",
      "16/19, train_loss: 0.5139\n",
      "17/19, train_loss: 0.5139\n",
      "18/19, train_loss: 0.5139\n",
      "19/19, train_loss: 0.5138\n",
      "Epoch 54 average loss: 0.5140\n",
      "\n",
      "Epoch 55/300\n",
      "1/19, train_loss: 0.5138\n",
      "2/19, train_loss: 0.5138\n",
      "3/19, train_loss: 0.5138\n",
      "4/19, train_loss: 0.5138\n",
      "5/19, train_loss: 0.5137\n",
      "6/19, train_loss: 0.5138\n",
      "7/19, train_loss: 0.5137\n",
      "8/19, train_loss: 0.5137\n",
      "9/19, train_loss: 0.5137\n",
      "10/19, train_loss: 0.5137\n",
      "11/19, train_loss: 0.5137\n",
      "12/19, train_loss: 0.5136\n",
      "13/19, train_loss: 0.5136\n",
      "14/19, train_loss: 0.5136\n",
      "15/19, train_loss: 0.5136\n",
      "16/19, train_loss: 0.5136\n",
      "17/19, train_loss: 0.5135\n",
      "18/19, train_loss: 0.5135\n",
      "19/19, train_loss: 0.5135\n",
      "Epoch 55 average loss: 0.5137\n",
      "Current epoch: 55, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 56/300\n",
      "1/19, train_loss: 0.5135\n",
      "2/19, train_loss: 0.5134\n",
      "3/19, train_loss: 0.5134\n",
      "4/19, train_loss: 0.5134\n",
      "5/19, train_loss: 0.5134\n",
      "6/19, train_loss: 0.5134\n",
      "7/19, train_loss: 0.5134\n",
      "8/19, train_loss: 0.5133\n",
      "9/19, train_loss: 0.5133\n",
      "10/19, train_loss: 0.5133\n",
      "11/19, train_loss: 0.5133\n",
      "12/19, train_loss: 0.5133\n",
      "13/19, train_loss: 0.5133\n",
      "14/19, train_loss: 0.5133\n",
      "15/19, train_loss: 0.5132\n",
      "16/19, train_loss: 0.5132\n",
      "17/19, train_loss: 0.5132\n",
      "18/19, train_loss: 0.5132\n",
      "19/19, train_loss: 0.5132\n",
      "Epoch 56 average loss: 0.5133\n",
      "\n",
      "Epoch 57/300\n",
      "1/19, train_loss: 0.5131\n",
      "2/19, train_loss: 0.5131\n",
      "3/19, train_loss: 0.5131\n",
      "4/19, train_loss: 0.5131\n",
      "5/19, train_loss: 0.5130\n",
      "6/19, train_loss: 0.5130\n",
      "7/19, train_loss: 0.5130\n",
      "8/19, train_loss: 0.5130\n",
      "9/19, train_loss: 0.5130\n",
      "10/19, train_loss: 0.5130\n",
      "11/19, train_loss: 0.5129\n",
      "12/19, train_loss: 0.5130\n",
      "13/19, train_loss: 0.5129\n",
      "14/19, train_loss: 0.5129\n",
      "15/19, train_loss: 0.5129\n",
      "16/19, train_loss: 0.5129\n",
      "17/19, train_loss: 0.5128\n",
      "18/19, train_loss: 0.5128\n",
      "19/19, train_loss: 0.5128\n",
      "Epoch 57 average loss: 0.5130\n",
      "\n",
      "Epoch 58/300\n",
      "1/19, train_loss: 0.5128\n",
      "2/19, train_loss: 0.5128\n",
      "3/19, train_loss: 0.5127\n",
      "4/19, train_loss: 0.5127\n",
      "5/19, train_loss: 0.5127\n",
      "6/19, train_loss: 0.5127\n",
      "7/19, train_loss: 0.5127\n",
      "8/19, train_loss: 0.5127\n",
      "9/19, train_loss: 0.5126\n",
      "10/19, train_loss: 0.5126\n",
      "11/19, train_loss: 0.5126\n",
      "12/19, train_loss: 0.5126\n",
      "13/19, train_loss: 0.5126\n",
      "14/19, train_loss: 0.5126\n",
      "15/19, train_loss: 0.5125\n",
      "16/19, train_loss: 0.5125\n",
      "17/19, train_loss: 0.5125\n",
      "18/19, train_loss: 0.5125\n",
      "19/19, train_loss: 0.5125\n",
      "Epoch 58 average loss: 0.5126\n",
      "\n",
      "Epoch 59/300\n",
      "1/19, train_loss: 0.5125\n",
      "2/19, train_loss: 0.5124\n",
      "3/19, train_loss: 0.5124\n",
      "4/19, train_loss: 0.5124\n",
      "5/19, train_loss: 0.5124\n",
      "6/19, train_loss: 0.5124\n",
      "7/19, train_loss: 0.5124\n",
      "8/19, train_loss: 0.5123\n",
      "9/19, train_loss: 0.5123\n",
      "10/19, train_loss: 0.5123\n",
      "11/19, train_loss: 0.5123\n",
      "12/19, train_loss: 0.5123\n",
      "13/19, train_loss: 0.5123\n",
      "14/19, train_loss: 0.5122\n",
      "15/19, train_loss: 0.5122\n",
      "16/19, train_loss: 0.5122\n",
      "17/19, train_loss: 0.5122\n",
      "18/19, train_loss: 0.5122\n",
      "19/19, train_loss: 0.5122\n",
      "Epoch 59 average loss: 0.5123\n",
      "\n",
      "Epoch 60/300\n",
      "1/19, train_loss: 0.5122\n",
      "2/19, train_loss: 0.5121\n",
      "3/19, train_loss: 0.5121\n",
      "4/19, train_loss: 0.5121\n",
      "5/19, train_loss: 0.5121\n",
      "6/19, train_loss: 0.5121\n",
      "7/19, train_loss: 0.5121\n",
      "8/19, train_loss: 0.5120\n",
      "9/19, train_loss: 0.5120\n",
      "10/19, train_loss: 0.5120\n",
      "11/19, train_loss: 0.5120\n",
      "12/19, train_loss: 0.5120\n",
      "13/19, train_loss: 0.5120\n",
      "14/19, train_loss: 0.5119\n",
      "15/19, train_loss: 0.5119\n",
      "16/19, train_loss: 0.5119\n",
      "17/19, train_loss: 0.5119\n",
      "18/19, train_loss: 0.5119\n",
      "19/19, train_loss: 0.5119\n",
      "Epoch 60 average loss: 0.5120\n",
      "Current epoch: 60, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 61/300\n",
      "1/19, train_loss: 0.5118\n",
      "2/19, train_loss: 0.5118\n",
      "3/19, train_loss: 0.5118\n",
      "4/19, train_loss: 0.5118\n",
      "5/19, train_loss: 0.5118\n",
      "6/19, train_loss: 0.5118\n",
      "7/19, train_loss: 0.5118\n",
      "8/19, train_loss: 0.5117\n",
      "9/19, train_loss: 0.5117\n",
      "10/19, train_loss: 0.5117\n",
      "11/19, train_loss: 0.5117\n",
      "12/19, train_loss: 0.5117\n",
      "13/19, train_loss: 0.5117\n",
      "14/19, train_loss: 0.5117\n",
      "15/19, train_loss: 0.5116\n",
      "16/19, train_loss: 0.5116\n",
      "17/19, train_loss: 0.5116\n",
      "18/19, train_loss: 0.5116\n",
      "19/19, train_loss: 0.5116\n",
      "Epoch 61 average loss: 0.5117\n",
      "\n",
      "Epoch 62/300\n",
      "1/19, train_loss: 0.5116\n",
      "2/19, train_loss: 0.5115\n",
      "3/19, train_loss: 0.5115\n",
      "4/19, train_loss: 0.5115\n",
      "5/19, train_loss: 0.5115\n",
      "6/19, train_loss: 0.5115\n",
      "7/19, train_loss: 0.5115\n",
      "8/19, train_loss: 0.5115\n",
      "9/19, train_loss: 0.5115\n",
      "10/19, train_loss: 0.5114\n",
      "11/19, train_loss: 0.5114\n",
      "12/19, train_loss: 0.5114\n",
      "13/19, train_loss: 0.5114\n",
      "14/19, train_loss: 0.5114\n",
      "15/19, train_loss: 0.5114\n",
      "16/19, train_loss: 0.5113\n",
      "17/19, train_loss: 0.5113\n",
      "18/19, train_loss: 0.5113\n",
      "19/19, train_loss: 0.5113\n",
      "Epoch 62 average loss: 0.5114\n",
      "\n",
      "Epoch 63/300\n",
      "1/19, train_loss: 0.5113\n",
      "2/19, train_loss: 0.5113\n",
      "3/19, train_loss: 0.5113\n",
      "4/19, train_loss: 0.5112\n",
      "5/19, train_loss: 0.5112\n",
      "6/19, train_loss: 0.5112\n",
      "7/19, train_loss: 0.5112\n",
      "8/19, train_loss: 0.5112\n",
      "9/19, train_loss: 0.5112\n",
      "10/19, train_loss: 0.5112\n",
      "11/19, train_loss: 0.5111\n",
      "12/19, train_loss: 0.5111\n",
      "13/19, train_loss: 0.5111\n",
      "14/19, train_loss: 0.5111\n",
      "15/19, train_loss: 0.5111\n",
      "16/19, train_loss: 0.5111\n",
      "17/19, train_loss: 0.5111\n",
      "18/19, train_loss: 0.5110\n",
      "19/19, train_loss: 0.5111\n",
      "Epoch 63 average loss: 0.5112\n",
      "\n",
      "Epoch 64/300\n",
      "1/19, train_loss: 0.5110\n",
      "2/19, train_loss: 0.5110\n",
      "3/19, train_loss: 0.5110\n",
      "4/19, train_loss: 0.5110\n",
      "5/19, train_loss: 0.5110\n",
      "6/19, train_loss: 0.5109\n",
      "7/19, train_loss: 0.5109\n",
      "8/19, train_loss: 0.5109\n",
      "9/19, train_loss: 0.5109\n",
      "10/19, train_loss: 0.5109\n",
      "11/19, train_loss: 0.5109\n",
      "12/19, train_loss: 0.5109\n",
      "13/19, train_loss: 0.5108\n",
      "14/19, train_loss: 0.5108\n",
      "15/19, train_loss: 0.5108\n",
      "16/19, train_loss: 0.5108\n",
      "17/19, train_loss: 0.5108\n",
      "18/19, train_loss: 0.5108\n",
      "19/19, train_loss: 0.5108\n",
      "Epoch 64 average loss: 0.5109\n",
      "\n",
      "Epoch 65/300\n",
      "1/19, train_loss: 0.5108\n",
      "2/19, train_loss: 0.5107\n",
      "3/19, train_loss: 0.5107\n",
      "4/19, train_loss: 0.5107\n",
      "5/19, train_loss: 0.5107\n",
      "6/19, train_loss: 0.5107\n",
      "7/19, train_loss: 0.5107\n",
      "8/19, train_loss: 0.5107\n",
      "9/19, train_loss: 0.5107\n",
      "10/19, train_loss: 0.5106\n",
      "11/19, train_loss: 0.5106\n",
      "12/19, train_loss: 0.5106\n",
      "13/19, train_loss: 0.5106\n",
      "14/19, train_loss: 0.5106\n",
      "15/19, train_loss: 0.5106\n",
      "16/19, train_loss: 0.5106\n",
      "17/19, train_loss: 0.5106\n",
      "18/19, train_loss: 0.5105\n",
      "19/19, train_loss: 0.5105\n",
      "Epoch 65 average loss: 0.5106\n",
      "Current epoch: 65, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 66/300\n",
      "1/19, train_loss: 0.5105\n",
      "2/19, train_loss: 0.5105\n",
      "3/19, train_loss: 0.5105\n",
      "4/19, train_loss: 0.5105\n",
      "5/19, train_loss: 0.5105\n",
      "6/19, train_loss: 0.5104\n",
      "7/19, train_loss: 0.5105\n",
      "8/19, train_loss: 0.5105\n",
      "9/19, train_loss: 0.5104\n",
      "10/19, train_loss: 0.5104\n",
      "11/19, train_loss: 0.5104\n",
      "12/19, train_loss: 0.5104\n",
      "13/19, train_loss: 0.5103\n",
      "14/19, train_loss: 0.5103\n",
      "15/19, train_loss: 0.5103\n",
      "16/19, train_loss: 0.5103\n",
      "17/19, train_loss: 0.5103\n",
      "18/19, train_loss: 0.5103\n",
      "19/19, train_loss: 0.5103\n",
      "Epoch 66 average loss: 0.5104\n",
      "\n",
      "Epoch 67/300\n",
      "1/19, train_loss: 0.5103\n",
      "2/19, train_loss: 0.5103\n",
      "3/19, train_loss: 0.5102\n",
      "4/19, train_loss: 0.5103\n",
      "5/19, train_loss: 0.5102\n",
      "6/19, train_loss: 0.5102\n",
      "7/19, train_loss: 0.5102\n",
      "8/19, train_loss: 0.5102\n",
      "9/19, train_loss: 0.5102\n",
      "10/19, train_loss: 0.5102\n",
      "11/19, train_loss: 0.5102\n",
      "12/19, train_loss: 0.5101\n",
      "13/19, train_loss: 0.5101\n",
      "14/19, train_loss: 0.5101\n",
      "15/19, train_loss: 0.5101\n",
      "16/19, train_loss: 0.5101\n",
      "17/19, train_loss: 0.5101\n",
      "18/19, train_loss: 0.5101\n",
      "19/19, train_loss: 0.5101\n",
      "Epoch 67 average loss: 0.5102\n",
      "\n",
      "Epoch 68/300\n",
      "1/19, train_loss: 0.5101\n",
      "2/19, train_loss: 0.5101\n",
      "3/19, train_loss: 0.5100\n",
      "4/19, train_loss: 0.5100\n",
      "5/19, train_loss: 0.5100\n",
      "6/19, train_loss: 0.5100\n",
      "7/19, train_loss: 0.5100\n",
      "8/19, train_loss: 0.5100\n",
      "9/19, train_loss: 0.5099\n",
      "10/19, train_loss: 0.5099\n",
      "11/19, train_loss: 0.5099\n",
      "12/19, train_loss: 0.5099\n",
      "13/19, train_loss: 0.5099\n",
      "14/19, train_loss: 0.5099\n",
      "15/19, train_loss: 0.5099\n",
      "16/19, train_loss: 0.5099\n",
      "17/19, train_loss: 0.5099\n",
      "18/19, train_loss: 0.5099\n",
      "19/19, train_loss: 0.5098\n",
      "Epoch 68 average loss: 0.5099\n",
      "\n",
      "Epoch 69/300\n",
      "1/19, train_loss: 0.5098\n",
      "2/19, train_loss: 0.5098\n",
      "3/19, train_loss: 0.5098\n",
      "4/19, train_loss: 0.5098\n",
      "5/19, train_loss: 0.5098\n",
      "6/19, train_loss: 0.5098\n",
      "7/19, train_loss: 0.5097\n",
      "8/19, train_loss: 0.5097\n",
      "9/19, train_loss: 0.5097\n",
      "10/19, train_loss: 0.5097\n",
      "11/19, train_loss: 0.5097\n",
      "12/19, train_loss: 0.5097\n",
      "13/19, train_loss: 0.5097\n",
      "14/19, train_loss: 0.5097\n",
      "15/19, train_loss: 0.5096\n",
      "16/19, train_loss: 0.5097\n",
      "17/19, train_loss: 0.5096\n",
      "18/19, train_loss: 0.5096\n",
      "19/19, train_loss: 0.5096\n",
      "Epoch 69 average loss: 0.5097\n",
      "\n",
      "Epoch 70/300\n",
      "1/19, train_loss: 0.5096\n",
      "2/19, train_loss: 0.5096\n",
      "3/19, train_loss: 0.5096\n",
      "4/19, train_loss: 0.5096\n",
      "5/19, train_loss: 0.5096\n",
      "6/19, train_loss: 0.5096\n",
      "7/19, train_loss: 0.5095\n",
      "8/19, train_loss: 0.5095\n",
      "9/19, train_loss: 0.5095\n",
      "10/19, train_loss: 0.5095\n",
      "11/19, train_loss: 0.5095\n",
      "12/19, train_loss: 0.5095\n",
      "13/19, train_loss: 0.5095\n",
      "14/19, train_loss: 0.5094\n",
      "15/19, train_loss: 0.5094\n",
      "16/19, train_loss: 0.5094\n",
      "17/19, train_loss: 0.5094\n",
      "18/19, train_loss: 0.5094\n",
      "19/19, train_loss: 0.5094\n",
      "Epoch 70 average loss: 0.5095\n",
      "Current epoch: 70, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 71/300\n",
      "1/19, train_loss: 0.5094\n",
      "2/19, train_loss: 0.5094\n",
      "3/19, train_loss: 0.5094\n",
      "4/19, train_loss: 0.5094\n",
      "5/19, train_loss: 0.5093\n",
      "6/19, train_loss: 0.5094\n",
      "7/19, train_loss: 0.5093\n",
      "8/19, train_loss: 0.5093\n",
      "9/19, train_loss: 0.5093\n",
      "10/19, train_loss: 0.5093\n",
      "11/19, train_loss: 0.5093\n",
      "12/19, train_loss: 0.5093\n",
      "13/19, train_loss: 0.5093\n",
      "14/19, train_loss: 0.5092\n",
      "15/19, train_loss: 0.5092\n",
      "16/19, train_loss: 0.5092\n",
      "17/19, train_loss: 0.5092\n",
      "18/19, train_loss: 0.5092\n",
      "19/19, train_loss: 0.5092\n",
      "Epoch 71 average loss: 0.5093\n",
      "\n",
      "Epoch 72/300\n",
      "1/19, train_loss: 0.5092\n",
      "2/19, train_loss: 0.5092\n",
      "3/19, train_loss: 0.5092\n",
      "4/19, train_loss: 0.5091\n",
      "5/19, train_loss: 0.5091\n",
      "6/19, train_loss: 0.5091\n",
      "7/19, train_loss: 0.5091\n",
      "8/19, train_loss: 0.5091\n",
      "9/19, train_loss: 0.5091\n",
      "10/19, train_loss: 0.5091\n",
      "11/19, train_loss: 0.5091\n",
      "12/19, train_loss: 0.5091\n",
      "13/19, train_loss: 0.5091\n",
      "14/19, train_loss: 0.5090\n",
      "15/19, train_loss: 0.5091\n",
      "16/19, train_loss: 0.5090\n",
      "17/19, train_loss: 0.5090\n",
      "18/19, train_loss: 0.5090\n",
      "19/19, train_loss: 0.5090\n",
      "Epoch 72 average loss: 0.5091\n",
      "\n",
      "Epoch 73/300\n",
      "1/19, train_loss: 0.5090\n",
      "2/19, train_loss: 0.5090\n",
      "3/19, train_loss: 0.5090\n",
      "4/19, train_loss: 0.5090\n",
      "5/19, train_loss: 0.5089\n",
      "6/19, train_loss: 0.5089\n",
      "7/19, train_loss: 0.5089\n",
      "8/19, train_loss: 0.5090\n",
      "9/19, train_loss: 0.5089\n",
      "10/19, train_loss: 0.5089\n",
      "11/19, train_loss: 0.5089\n",
      "12/19, train_loss: 0.5089\n",
      "13/19, train_loss: 0.5089\n",
      "14/19, train_loss: 0.5088\n",
      "15/19, train_loss: 0.5088\n",
      "16/19, train_loss: 0.5088\n",
      "17/19, train_loss: 0.5088\n",
      "18/19, train_loss: 0.5088\n",
      "19/19, train_loss: 0.5088\n",
      "Epoch 73 average loss: 0.5089\n",
      "\n",
      "Epoch 74/300\n",
      "1/19, train_loss: 0.5088\n",
      "2/19, train_loss: 0.5088\n",
      "3/19, train_loss: 0.5088\n",
      "4/19, train_loss: 0.5088\n",
      "5/19, train_loss: 0.5088\n",
      "6/19, train_loss: 0.5087\n",
      "7/19, train_loss: 0.5087\n",
      "8/19, train_loss: 0.5087\n",
      "9/19, train_loss: 0.5087\n",
      "10/19, train_loss: 0.5087\n",
      "11/19, train_loss: 0.5087\n",
      "12/19, train_loss: 0.5087\n",
      "13/19, train_loss: 0.5087\n",
      "14/19, train_loss: 0.5087\n",
      "15/19, train_loss: 0.5087\n",
      "16/19, train_loss: 0.5086\n",
      "17/19, train_loss: 0.5086\n",
      "18/19, train_loss: 0.5087\n",
      "19/19, train_loss: 0.5086\n",
      "Epoch 74 average loss: 0.5087\n",
      "\n",
      "Epoch 75/300\n",
      "1/19, train_loss: 0.5086\n",
      "2/19, train_loss: 0.5086\n",
      "3/19, train_loss: 0.5086\n",
      "4/19, train_loss: 0.5086\n",
      "5/19, train_loss: 0.5086\n",
      "6/19, train_loss: 0.5086\n",
      "7/19, train_loss: 0.5085\n",
      "8/19, train_loss: 0.5085\n",
      "9/19, train_loss: 0.5085\n",
      "10/19, train_loss: 0.5085\n",
      "11/19, train_loss: 0.5085\n",
      "12/19, train_loss: 0.5085\n",
      "13/19, train_loss: 0.5085\n",
      "14/19, train_loss: 0.5085\n",
      "15/19, train_loss: 0.5085\n",
      "16/19, train_loss: 0.5085\n",
      "17/19, train_loss: 0.5085\n",
      "18/19, train_loss: 0.5084\n",
      "19/19, train_loss: 0.5085\n",
      "Epoch 75 average loss: 0.5085\n",
      "Current epoch: 75, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 76/300\n",
      "1/19, train_loss: 0.5084\n",
      "2/19, train_loss: 0.5084\n",
      "3/19, train_loss: 0.5084\n",
      "4/19, train_loss: 0.5084\n",
      "5/19, train_loss: 0.5084\n",
      "6/19, train_loss: 0.5084\n",
      "7/19, train_loss: 0.5084\n",
      "8/19, train_loss: 0.5084\n",
      "9/19, train_loss: 0.5084\n",
      "10/19, train_loss: 0.5083\n",
      "11/19, train_loss: 0.5083\n",
      "12/19, train_loss: 0.5083\n",
      "13/19, train_loss: 0.5083\n",
      "14/19, train_loss: 0.5083\n",
      "15/19, train_loss: 0.5083\n",
      "16/19, train_loss: 0.5083\n",
      "17/19, train_loss: 0.5083\n",
      "18/19, train_loss: 0.5083\n",
      "19/19, train_loss: 0.5083\n",
      "Epoch 76 average loss: 0.5083\n",
      "\n",
      "Epoch 77/300\n",
      "1/19, train_loss: 0.5082\n",
      "2/19, train_loss: 0.5082\n",
      "3/19, train_loss: 0.5082\n",
      "4/19, train_loss: 0.5082\n",
      "5/19, train_loss: 0.5082\n",
      "6/19, train_loss: 0.5082\n",
      "7/19, train_loss: 0.5082\n",
      "8/19, train_loss: 0.5082\n",
      "9/19, train_loss: 0.5082\n",
      "10/19, train_loss: 0.5082\n",
      "11/19, train_loss: 0.5082\n",
      "12/19, train_loss: 0.5082\n",
      "13/19, train_loss: 0.5081\n",
      "14/19, train_loss: 0.5081\n",
      "15/19, train_loss: 0.5081\n",
      "16/19, train_loss: 0.5081\n",
      "17/19, train_loss: 0.5081\n",
      "18/19, train_loss: 0.5081\n",
      "19/19, train_loss: 0.5081\n",
      "Epoch 77 average loss: 0.5082\n",
      "\n",
      "Epoch 78/300\n",
      "1/19, train_loss: 0.5081\n",
      "2/19, train_loss: 0.5081\n",
      "3/19, train_loss: 0.5081\n",
      "4/19, train_loss: 0.5081\n",
      "5/19, train_loss: 0.5080\n",
      "6/19, train_loss: 0.5080\n",
      "7/19, train_loss: 0.5080\n",
      "8/19, train_loss: 0.5080\n",
      "9/19, train_loss: 0.5080\n",
      "10/19, train_loss: 0.5080\n",
      "11/19, train_loss: 0.5080\n",
      "12/19, train_loss: 0.5080\n",
      "13/19, train_loss: 0.5080\n",
      "14/19, train_loss: 0.5080\n",
      "15/19, train_loss: 0.5080\n",
      "16/19, train_loss: 0.5080\n",
      "17/19, train_loss: 0.5079\n",
      "18/19, train_loss: 0.5079\n",
      "19/19, train_loss: 0.5080\n",
      "Epoch 78 average loss: 0.5080\n",
      "\n",
      "Epoch 79/300\n",
      "1/19, train_loss: 0.5079\n",
      "2/19, train_loss: 0.5079\n",
      "3/19, train_loss: 0.5079\n",
      "4/19, train_loss: 0.5079\n",
      "5/19, train_loss: 0.5079\n",
      "6/19, train_loss: 0.5079\n",
      "7/19, train_loss: 0.5079\n",
      "8/19, train_loss: 0.5079\n",
      "9/19, train_loss: 0.5079\n",
      "10/19, train_loss: 0.5078\n",
      "11/19, train_loss: 0.5078\n",
      "12/19, train_loss: 0.5078\n",
      "13/19, train_loss: 0.5078\n",
      "14/19, train_loss: 0.5078\n",
      "15/19, train_loss: 0.5078\n",
      "16/19, train_loss: 0.5078\n",
      "17/19, train_loss: 0.5078\n",
      "18/19, train_loss: 0.5078\n",
      "19/19, train_loss: 0.5078\n",
      "Epoch 79 average loss: 0.5078\n",
      "\n",
      "Epoch 80/300\n",
      "1/19, train_loss: 0.5078\n",
      "2/19, train_loss: 0.5078\n",
      "3/19, train_loss: 0.5077\n",
      "4/19, train_loss: 0.5078\n",
      "5/19, train_loss: 0.5077\n",
      "6/19, train_loss: 0.5077\n",
      "7/19, train_loss: 0.5077\n",
      "8/19, train_loss: 0.5077\n",
      "9/19, train_loss: 0.5077\n",
      "10/19, train_loss: 0.5077\n",
      "11/19, train_loss: 0.5077\n",
      "12/19, train_loss: 0.5077\n",
      "13/19, train_loss: 0.5077\n",
      "14/19, train_loss: 0.5077\n",
      "15/19, train_loss: 0.5076\n",
      "16/19, train_loss: 0.5076\n",
      "17/19, train_loss: 0.5076\n",
      "18/19, train_loss: 0.5076\n",
      "19/19, train_loss: 0.5076\n",
      "Epoch 80 average loss: 0.5077\n",
      "Current epoch: 80, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 81/300\n",
      "1/19, train_loss: 0.5076\n",
      "2/19, train_loss: 0.5076\n",
      "3/19, train_loss: 0.5076\n",
      "4/19, train_loss: 0.5076\n",
      "5/19, train_loss: 0.5076\n",
      "6/19, train_loss: 0.5076\n",
      "7/19, train_loss: 0.5076\n",
      "8/19, train_loss: 0.5075\n",
      "9/19, train_loss: 0.5075\n",
      "10/19, train_loss: 0.5075\n",
      "11/19, train_loss: 0.5075\n",
      "12/19, train_loss: 0.5075\n",
      "13/19, train_loss: 0.5075\n",
      "14/19, train_loss: 0.5075\n",
      "15/19, train_loss: 0.5075\n",
      "16/19, train_loss: 0.5075\n",
      "17/19, train_loss: 0.5075\n",
      "18/19, train_loss: 0.5075\n",
      "19/19, train_loss: 0.5075\n",
      "Epoch 81 average loss: 0.5075\n",
      "\n",
      "Epoch 82/300\n",
      "1/19, train_loss: 0.5075\n",
      "2/19, train_loss: 0.5074\n",
      "3/19, train_loss: 0.5074\n",
      "4/19, train_loss: 0.5074\n",
      "5/19, train_loss: 0.5074\n",
      "6/19, train_loss: 0.5074\n",
      "7/19, train_loss: 0.5074\n",
      "8/19, train_loss: 0.5074\n",
      "9/19, train_loss: 0.5074\n",
      "10/19, train_loss: 0.5074\n",
      "11/19, train_loss: 0.5074\n",
      "12/19, train_loss: 0.5074\n",
      "13/19, train_loss: 0.5074\n",
      "14/19, train_loss: 0.5074\n",
      "15/19, train_loss: 0.5073\n",
      "16/19, train_loss: 0.5073\n",
      "17/19, train_loss: 0.5073\n",
      "18/19, train_loss: 0.5073\n",
      "19/19, train_loss: 0.5073\n",
      "Epoch 82 average loss: 0.5074\n",
      "\n",
      "Epoch 83/300\n",
      "1/19, train_loss: 0.5073\n",
      "2/19, train_loss: 0.5073\n",
      "3/19, train_loss: 0.5073\n",
      "4/19, train_loss: 0.5073\n",
      "5/19, train_loss: 0.5073\n",
      "6/19, train_loss: 0.5073\n",
      "7/19, train_loss: 0.5073\n",
      "8/19, train_loss: 0.5073\n",
      "9/19, train_loss: 0.5073\n",
      "10/19, train_loss: 0.5072\n",
      "11/19, train_loss: 0.5072\n",
      "12/19, train_loss: 0.5072\n",
      "13/19, train_loss: 0.5072\n",
      "14/19, train_loss: 0.5072\n",
      "15/19, train_loss: 0.5072\n",
      "16/19, train_loss: 0.5072\n",
      "17/19, train_loss: 0.5072\n",
      "18/19, train_loss: 0.5072\n",
      "19/19, train_loss: 0.5072\n",
      "Epoch 83 average loss: 0.5072\n",
      "\n",
      "Epoch 84/300\n",
      "1/19, train_loss: 0.5072\n",
      "2/19, train_loss: 0.5072\n",
      "3/19, train_loss: 0.5072\n",
      "4/19, train_loss: 0.5071\n",
      "5/19, train_loss: 0.5071\n",
      "6/19, train_loss: 0.5071\n",
      "7/19, train_loss: 0.5071\n",
      "8/19, train_loss: 0.5071\n",
      "9/19, train_loss: 0.5071\n",
      "10/19, train_loss: 0.5071\n",
      "11/19, train_loss: 0.5071\n",
      "12/19, train_loss: 0.5071\n",
      "13/19, train_loss: 0.5071\n",
      "14/19, train_loss: 0.5071\n",
      "15/19, train_loss: 0.5071\n",
      "16/19, train_loss: 0.5071\n",
      "17/19, train_loss: 0.5070\n",
      "18/19, train_loss: 0.5071\n",
      "19/19, train_loss: 0.5070\n",
      "Epoch 84 average loss: 0.5071\n",
      "\n",
      "Epoch 85/300\n",
      "1/19, train_loss: 0.5070\n",
      "2/19, train_loss: 0.5070\n",
      "3/19, train_loss: 0.5070\n",
      "4/19, train_loss: 0.5070\n",
      "5/19, train_loss: 0.5070\n",
      "6/19, train_loss: 0.5070\n",
      "7/19, train_loss: 0.5070\n",
      "8/19, train_loss: 0.5070\n",
      "9/19, train_loss: 0.5070\n",
      "10/19, train_loss: 0.5070\n",
      "11/19, train_loss: 0.5070\n",
      "12/19, train_loss: 0.5069\n",
      "13/19, train_loss: 0.5070\n",
      "14/19, train_loss: 0.5069\n",
      "15/19, train_loss: 0.5069\n",
      "16/19, train_loss: 0.5069\n",
      "17/19, train_loss: 0.5069\n",
      "18/19, train_loss: 0.5069\n",
      "19/19, train_loss: 0.5069\n",
      "Epoch 85 average loss: 0.5070\n",
      "Current epoch: 85, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 86/300\n",
      "1/19, train_loss: 0.5069\n",
      "2/19, train_loss: 0.5069\n",
      "3/19, train_loss: 0.5069\n",
      "4/19, train_loss: 0.5069\n",
      "5/19, train_loss: 0.5069\n",
      "6/19, train_loss: 0.5069\n",
      "7/19, train_loss: 0.5068\n",
      "8/19, train_loss: 0.5068\n",
      "9/19, train_loss: 0.5068\n",
      "10/19, train_loss: 0.5068\n",
      "11/19, train_loss: 0.5068\n",
      "12/19, train_loss: 0.5068\n",
      "13/19, train_loss: 0.5068\n",
      "14/19, train_loss: 0.5068\n",
      "15/19, train_loss: 0.5068\n",
      "16/19, train_loss: 0.5068\n",
      "17/19, train_loss: 0.5068\n",
      "18/19, train_loss: 0.5068\n",
      "19/19, train_loss: 0.5068\n",
      "Epoch 86 average loss: 0.5068\n",
      "\n",
      "Epoch 87/300\n",
      "1/19, train_loss: 0.5068\n",
      "2/19, train_loss: 0.5068\n",
      "3/19, train_loss: 0.5068\n",
      "4/19, train_loss: 0.5067\n",
      "5/19, train_loss: 0.5067\n",
      "6/19, train_loss: 0.5067\n",
      "7/19, train_loss: 0.5067\n",
      "8/19, train_loss: 0.5067\n",
      "9/19, train_loss: 0.5067\n",
      "10/19, train_loss: 0.5067\n",
      "11/19, train_loss: 0.5067\n",
      "12/19, train_loss: 0.5067\n",
      "13/19, train_loss: 0.5067\n",
      "14/19, train_loss: 0.5067\n",
      "15/19, train_loss: 0.5067\n",
      "16/19, train_loss: 0.5067\n",
      "17/19, train_loss: 0.5066\n",
      "18/19, train_loss: 0.5067\n",
      "19/19, train_loss: 0.5066\n",
      "Epoch 87 average loss: 0.5067\n",
      "\n",
      "Epoch 88/300\n",
      "1/19, train_loss: 0.5066\n",
      "2/19, train_loss: 0.5066\n",
      "3/19, train_loss: 0.5066\n",
      "4/19, train_loss: 0.5066\n",
      "5/19, train_loss: 0.5066\n",
      "6/19, train_loss: 0.5066\n",
      "7/19, train_loss: 0.5066\n",
      "8/19, train_loss: 0.5066\n",
      "9/19, train_loss: 0.5066\n",
      "10/19, train_loss: 0.5066\n",
      "11/19, train_loss: 0.5066\n",
      "12/19, train_loss: 0.5066\n",
      "13/19, train_loss: 0.5066\n",
      "14/19, train_loss: 0.5065\n",
      "15/19, train_loss: 0.5065\n",
      "16/19, train_loss: 0.5065\n",
      "17/19, train_loss: 0.5065\n",
      "18/19, train_loss: 0.5065\n",
      "19/19, train_loss: 0.5065\n",
      "Epoch 88 average loss: 0.5066\n",
      "\n",
      "Epoch 89/300\n",
      "1/19, train_loss: 0.5065\n",
      "2/19, train_loss: 0.5065\n",
      "3/19, train_loss: 0.5065\n",
      "4/19, train_loss: 0.5065\n",
      "5/19, train_loss: 0.5065\n",
      "6/19, train_loss: 0.5065\n",
      "7/19, train_loss: 0.5065\n",
      "8/19, train_loss: 0.5065\n",
      "9/19, train_loss: 0.5065\n",
      "10/19, train_loss: 0.5064\n",
      "11/19, train_loss: 0.5064\n",
      "12/19, train_loss: 0.5064\n",
      "13/19, train_loss: 0.5064\n",
      "14/19, train_loss: 0.5064\n",
      "15/19, train_loss: 0.5064\n",
      "16/19, train_loss: 0.5064\n",
      "17/19, train_loss: 0.5064\n",
      "18/19, train_loss: 0.5064\n",
      "19/19, train_loss: 0.5064\n",
      "Epoch 89 average loss: 0.5065\n",
      "\n",
      "Epoch 90/300\n",
      "1/19, train_loss: 0.5064\n",
      "2/19, train_loss: 0.5064\n",
      "3/19, train_loss: 0.5064\n",
      "4/19, train_loss: 0.5064\n",
      "5/19, train_loss: 0.5064\n",
      "6/19, train_loss: 0.5064\n",
      "7/19, train_loss: 0.5064\n",
      "8/19, train_loss: 0.5063\n",
      "9/19, train_loss: 0.5063\n",
      "10/19, train_loss: 0.5063\n",
      "11/19, train_loss: 0.5063\n",
      "12/19, train_loss: 0.5063\n",
      "13/19, train_loss: 0.5063\n",
      "14/19, train_loss: 0.5063\n",
      "15/19, train_loss: 0.5063\n",
      "16/19, train_loss: 0.5063\n",
      "17/19, train_loss: 0.5063\n",
      "18/19, train_loss: 0.5063\n",
      "19/19, train_loss: 0.5063\n",
      "Epoch 90 average loss: 0.5063\n",
      "Current epoch: 90, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 91/300\n",
      "1/19, train_loss: 0.5063\n",
      "2/19, train_loss: 0.5063\n",
      "3/19, train_loss: 0.5063\n",
      "4/19, train_loss: 0.5063\n",
      "5/19, train_loss: 0.5063\n",
      "6/19, train_loss: 0.5062\n",
      "7/19, train_loss: 0.5062\n",
      "8/19, train_loss: 0.5062\n",
      "9/19, train_loss: 0.5062\n",
      "10/19, train_loss: 0.5062\n",
      "11/19, train_loss: 0.5062\n",
      "12/19, train_loss: 0.5062\n",
      "13/19, train_loss: 0.5062\n",
      "14/19, train_loss: 0.5062\n",
      "15/19, train_loss: 0.5062\n",
      "16/19, train_loss: 0.5062\n",
      "17/19, train_loss: 0.5062\n",
      "18/19, train_loss: 0.5062\n",
      "19/19, train_loss: 0.5062\n",
      "Epoch 91 average loss: 0.5062\n",
      "\n",
      "Epoch 92/300\n",
      "1/19, train_loss: 0.5062\n",
      "2/19, train_loss: 0.5062\n",
      "3/19, train_loss: 0.5062\n",
      "4/19, train_loss: 0.5061\n",
      "5/19, train_loss: 0.5061\n",
      "6/19, train_loss: 0.5061\n",
      "7/19, train_loss: 0.5061\n",
      "8/19, train_loss: 0.5061\n",
      "9/19, train_loss: 0.5061\n",
      "10/19, train_loss: 0.5061\n",
      "11/19, train_loss: 0.5061\n",
      "12/19, train_loss: 0.5061\n",
      "13/19, train_loss: 0.5061\n",
      "14/19, train_loss: 0.5061\n",
      "15/19, train_loss: 0.5061\n",
      "16/19, train_loss: 0.5061\n",
      "17/19, train_loss: 0.5061\n",
      "18/19, train_loss: 0.5060\n",
      "19/19, train_loss: 0.5060\n",
      "Epoch 92 average loss: 0.5061\n",
      "\n",
      "Epoch 93/300\n",
      "1/19, train_loss: 0.5061\n",
      "2/19, train_loss: 0.5060\n",
      "3/19, train_loss: 0.5060\n",
      "4/19, train_loss: 0.5060\n",
      "5/19, train_loss: 0.5060\n",
      "6/19, train_loss: 0.5060\n",
      "7/19, train_loss: 0.5060\n",
      "8/19, train_loss: 0.5060\n",
      "9/19, train_loss: 0.5060\n",
      "10/19, train_loss: 0.5060\n",
      "11/19, train_loss: 0.5060\n",
      "12/19, train_loss: 0.5060\n",
      "13/19, train_loss: 0.5060\n",
      "14/19, train_loss: 0.5060\n",
      "15/19, train_loss: 0.5060\n",
      "16/19, train_loss: 0.5060\n",
      "17/19, train_loss: 0.5060\n",
      "18/19, train_loss: 0.5059\n",
      "19/19, train_loss: 0.5059\n",
      "Epoch 93 average loss: 0.5060\n",
      "\n",
      "Epoch 94/300\n",
      "1/19, train_loss: 0.5059\n",
      "2/19, train_loss: 0.5059\n",
      "3/19, train_loss: 0.5059\n",
      "4/19, train_loss: 0.5059\n",
      "5/19, train_loss: 0.5059\n",
      "6/19, train_loss: 0.5059\n",
      "7/19, train_loss: 0.5059\n",
      "8/19, train_loss: 0.5059\n",
      "9/19, train_loss: 0.5059\n",
      "10/19, train_loss: 0.5059\n",
      "11/19, train_loss: 0.5059\n",
      "12/19, train_loss: 0.5059\n",
      "13/19, train_loss: 0.5059\n",
      "14/19, train_loss: 0.5059\n",
      "15/19, train_loss: 0.5059\n",
      "16/19, train_loss: 0.5059\n",
      "17/19, train_loss: 0.5058\n",
      "18/19, train_loss: 0.5058\n",
      "19/19, train_loss: 0.5058\n",
      "Epoch 94 average loss: 0.5059\n",
      "\n",
      "Epoch 95/300\n",
      "1/19, train_loss: 0.5058\n",
      "2/19, train_loss: 0.5058\n",
      "3/19, train_loss: 0.5058\n",
      "4/19, train_loss: 0.5058\n",
      "5/19, train_loss: 0.5058\n",
      "6/19, train_loss: 0.5058\n",
      "7/19, train_loss: 0.5058\n",
      "8/19, train_loss: 0.5058\n",
      "9/19, train_loss: 0.5058\n",
      "10/19, train_loss: 0.5058\n",
      "11/19, train_loss: 0.5058\n",
      "12/19, train_loss: 0.5058\n",
      "13/19, train_loss: 0.5058\n",
      "14/19, train_loss: 0.5058\n",
      "15/19, train_loss: 0.5057\n",
      "16/19, train_loss: 0.5057\n",
      "17/19, train_loss: 0.5057\n",
      "18/19, train_loss: 0.5057\n",
      "19/19, train_loss: 0.5057\n",
      "Epoch 95 average loss: 0.5058\n",
      "Current epoch: 95, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 96/300\n",
      "1/19, train_loss: 0.5057\n",
      "2/19, train_loss: 0.5057\n",
      "3/19, train_loss: 0.5057\n",
      "4/19, train_loss: 0.5057\n",
      "5/19, train_loss: 0.5057\n",
      "6/19, train_loss: 0.5057\n",
      "7/19, train_loss: 0.5057\n",
      "8/19, train_loss: 0.5057\n",
      "9/19, train_loss: 0.5057\n",
      "10/19, train_loss: 0.5057\n",
      "11/19, train_loss: 0.5057\n",
      "12/19, train_loss: 0.5057\n",
      "13/19, train_loss: 0.5057\n",
      "14/19, train_loss: 0.5057\n",
      "15/19, train_loss: 0.5056\n",
      "16/19, train_loss: 0.5056\n",
      "17/19, train_loss: 0.5056\n",
      "18/19, train_loss: 0.5056\n",
      "19/19, train_loss: 0.5056\n",
      "Epoch 96 average loss: 0.5057\n",
      "\n",
      "Epoch 97/300\n",
      "1/19, train_loss: 0.5056\n",
      "2/19, train_loss: 0.5056\n",
      "3/19, train_loss: 0.5056\n",
      "4/19, train_loss: 0.5056\n",
      "5/19, train_loss: 0.5056\n",
      "6/19, train_loss: 0.5056\n",
      "7/19, train_loss: 0.5056\n",
      "8/19, train_loss: 0.5056\n",
      "9/19, train_loss: 0.5056\n",
      "10/19, train_loss: 0.5056\n",
      "11/19, train_loss: 0.5056\n",
      "12/19, train_loss: 0.5056\n",
      "13/19, train_loss: 0.5056\n",
      "14/19, train_loss: 0.5056\n",
      "15/19, train_loss: 0.5055\n",
      "16/19, train_loss: 0.5055\n",
      "17/19, train_loss: 0.5055\n",
      "18/19, train_loss: 0.5055\n",
      "19/19, train_loss: 0.5056\n",
      "Epoch 97 average loss: 0.5056\n",
      "\n",
      "Epoch 98/300\n",
      "1/19, train_loss: 0.5055\n",
      "2/19, train_loss: 0.5055\n",
      "3/19, train_loss: 0.5055\n",
      "4/19, train_loss: 0.5055\n",
      "5/19, train_loss: 0.5055\n",
      "6/19, train_loss: 0.5055\n",
      "7/19, train_loss: 0.5055\n",
      "8/19, train_loss: 0.5055\n",
      "9/19, train_loss: 0.5055\n",
      "10/19, train_loss: 0.5055\n",
      "11/19, train_loss: 0.5055\n",
      "12/19, train_loss: 0.5055\n",
      "13/19, train_loss: 0.5055\n",
      "14/19, train_loss: 0.5055\n",
      "15/19, train_loss: 0.5054\n",
      "16/19, train_loss: 0.5054\n",
      "17/19, train_loss: 0.5054\n",
      "18/19, train_loss: 0.5054\n",
      "19/19, train_loss: 0.5054\n",
      "Epoch 98 average loss: 0.5055\n",
      "\n",
      "Epoch 99/300\n",
      "1/19, train_loss: 0.5054\n",
      "2/19, train_loss: 0.5054\n",
      "3/19, train_loss: 0.5054\n",
      "4/19, train_loss: 0.5054\n",
      "5/19, train_loss: 0.5054\n",
      "6/19, train_loss: 0.5054\n",
      "7/19, train_loss: 0.5054\n",
      "8/19, train_loss: 0.5054\n",
      "9/19, train_loss: 0.5054\n",
      "10/19, train_loss: 0.5054\n",
      "11/19, train_loss: 0.5054\n",
      "12/19, train_loss: 0.5054\n",
      "13/19, train_loss: 0.5054\n",
      "14/19, train_loss: 0.5054\n",
      "15/19, train_loss: 0.5054\n",
      "16/19, train_loss: 0.5053\n",
      "17/19, train_loss: 0.5053\n",
      "18/19, train_loss: 0.5053\n",
      "19/19, train_loss: 0.5053\n",
      "Epoch 99 average loss: 0.5054\n",
      "\n",
      "Epoch 100/300\n",
      "1/19, train_loss: 0.5053\n",
      "2/19, train_loss: 0.5053\n",
      "3/19, train_loss: 0.5053\n",
      "4/19, train_loss: 0.5053\n",
      "5/19, train_loss: 0.5053\n",
      "6/19, train_loss: 0.5053\n",
      "7/19, train_loss: 0.5053\n",
      "8/19, train_loss: 0.5053\n",
      "9/19, train_loss: 0.5053\n",
      "10/19, train_loss: 0.5053\n",
      "11/19, train_loss: 0.5053\n",
      "12/19, train_loss: 0.5053\n",
      "13/19, train_loss: 0.5053\n",
      "14/19, train_loss: 0.5053\n",
      "15/19, train_loss: 0.5053\n",
      "16/19, train_loss: 0.5053\n",
      "17/19, train_loss: 0.5053\n",
      "18/19, train_loss: 0.5052\n",
      "19/19, train_loss: 0.5052\n",
      "Epoch 100 average loss: 0.5053\n",
      "Current epoch: 100, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 101/300\n",
      "1/19, train_loss: 0.5052\n",
      "2/19, train_loss: 0.5052\n",
      "3/19, train_loss: 0.5052\n",
      "4/19, train_loss: 0.5052\n",
      "5/19, train_loss: 0.5052\n",
      "6/19, train_loss: 0.5052\n",
      "7/19, train_loss: 0.5052\n",
      "8/19, train_loss: 0.5052\n",
      "9/19, train_loss: 0.5052\n",
      "10/19, train_loss: 0.5052\n",
      "11/19, train_loss: 0.5052\n",
      "12/19, train_loss: 0.5052\n",
      "13/19, train_loss: 0.5052\n",
      "14/19, train_loss: 0.5052\n",
      "15/19, train_loss: 0.5052\n",
      "16/19, train_loss: 0.5052\n",
      "17/19, train_loss: 0.5052\n",
      "18/19, train_loss: 0.5052\n",
      "19/19, train_loss: 0.5052\n",
      "Epoch 101 average loss: 0.5052\n",
      "\n",
      "Epoch 102/300\n",
      "1/19, train_loss: 0.5052\n",
      "2/19, train_loss: 0.5051\n",
      "3/19, train_loss: 0.5051\n",
      "4/19, train_loss: 0.5051\n",
      "5/19, train_loss: 0.5051\n",
      "6/19, train_loss: 0.5051\n",
      "7/19, train_loss: 0.5051\n",
      "8/19, train_loss: 0.5051\n",
      "9/19, train_loss: 0.5051\n",
      "10/19, train_loss: 0.5051\n",
      "11/19, train_loss: 0.5051\n",
      "12/19, train_loss: 0.5051\n",
      "13/19, train_loss: 0.5051\n",
      "14/19, train_loss: 0.5051\n",
      "15/19, train_loss: 0.5051\n",
      "16/19, train_loss: 0.5051\n",
      "17/19, train_loss: 0.5051\n",
      "18/19, train_loss: 0.5051\n",
      "19/19, train_loss: 0.5051\n",
      "Epoch 102 average loss: 0.5051\n",
      "\n",
      "Epoch 103/300\n",
      "1/19, train_loss: 0.5051\n",
      "2/19, train_loss: 0.5051\n",
      "3/19, train_loss: 0.5051\n",
      "4/19, train_loss: 0.5050\n",
      "5/19, train_loss: 0.5051\n",
      "6/19, train_loss: 0.5050\n",
      "7/19, train_loss: 0.5050\n",
      "8/19, train_loss: 0.5050\n",
      "9/19, train_loss: 0.5050\n",
      "10/19, train_loss: 0.5050\n",
      "11/19, train_loss: 0.5050\n",
      "12/19, train_loss: 0.5050\n",
      "13/19, train_loss: 0.5050\n",
      "14/19, train_loss: 0.5050\n",
      "15/19, train_loss: 0.5050\n",
      "16/19, train_loss: 0.5050\n",
      "17/19, train_loss: 0.5050\n",
      "18/19, train_loss: 0.5050\n",
      "19/19, train_loss: 0.5050\n",
      "Epoch 103 average loss: 0.5050\n",
      "\n",
      "Epoch 104/300\n",
      "1/19, train_loss: 0.5050\n",
      "2/19, train_loss: 0.5050\n",
      "3/19, train_loss: 0.5050\n",
      "4/19, train_loss: 0.5050\n",
      "5/19, train_loss: 0.5050\n",
      "6/19, train_loss: 0.5050\n",
      "7/19, train_loss: 0.5049\n",
      "8/19, train_loss: 0.5050\n",
      "9/19, train_loss: 0.5049\n",
      "10/19, train_loss: 0.5049\n",
      "11/19, train_loss: 0.5049\n",
      "12/19, train_loss: 0.5049\n",
      "13/19, train_loss: 0.5049\n",
      "14/19, train_loss: 0.5049\n",
      "15/19, train_loss: 0.5049\n",
      "16/19, train_loss: 0.5049\n",
      "17/19, train_loss: 0.5049\n",
      "18/19, train_loss: 0.5049\n",
      "19/19, train_loss: 0.5049\n",
      "Epoch 104 average loss: 0.5049\n",
      "\n",
      "Epoch 105/300\n",
      "1/19, train_loss: 0.5049\n",
      "2/19, train_loss: 0.5049\n",
      "3/19, train_loss: 0.5049\n",
      "4/19, train_loss: 0.5049\n",
      "5/19, train_loss: 0.5049\n",
      "6/19, train_loss: 0.5049\n",
      "7/19, train_loss: 0.5049\n",
      "8/19, train_loss: 0.5049\n",
      "9/19, train_loss: 0.5049\n",
      "10/19, train_loss: 0.5049\n",
      "11/19, train_loss: 0.5048\n",
      "12/19, train_loss: 0.5049\n",
      "13/19, train_loss: 0.5048\n",
      "14/19, train_loss: 0.5048\n",
      "15/19, train_loss: 0.5048\n",
      "16/19, train_loss: 0.5048\n",
      "17/19, train_loss: 0.5048\n",
      "18/19, train_loss: 0.5048\n",
      "19/19, train_loss: 0.5048\n",
      "Epoch 105 average loss: 0.5049\n",
      "Current epoch: 105, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 106/300\n",
      "1/19, train_loss: 0.5048\n",
      "2/19, train_loss: 0.5048\n",
      "3/19, train_loss: 0.5048\n",
      "4/19, train_loss: 0.5048\n",
      "5/19, train_loss: 0.5048\n",
      "6/19, train_loss: 0.5048\n",
      "7/19, train_loss: 0.5048\n",
      "8/19, train_loss: 0.5048\n",
      "9/19, train_loss: 0.5048\n",
      "10/19, train_loss: 0.5048\n",
      "11/19, train_loss: 0.5048\n",
      "12/19, train_loss: 0.5048\n",
      "13/19, train_loss: 0.5048\n",
      "14/19, train_loss: 0.5048\n",
      "15/19, train_loss: 0.5048\n",
      "16/19, train_loss: 0.5048\n",
      "17/19, train_loss: 0.5047\n",
      "18/19, train_loss: 0.5047\n",
      "19/19, train_loss: 0.5047\n",
      "Epoch 106 average loss: 0.5048\n",
      "\n",
      "Epoch 107/300\n",
      "1/19, train_loss: 0.5047\n",
      "2/19, train_loss: 0.5047\n",
      "3/19, train_loss: 0.5047\n",
      "4/19, train_loss: 0.5047\n",
      "5/19, train_loss: 0.5047\n",
      "6/19, train_loss: 0.5047\n",
      "7/19, train_loss: 0.5047\n",
      "8/19, train_loss: 0.5047\n",
      "9/19, train_loss: 0.5047\n",
      "10/19, train_loss: 0.5047\n",
      "11/19, train_loss: 0.5047\n",
      "12/19, train_loss: 0.5047\n",
      "13/19, train_loss: 0.5047\n",
      "14/19, train_loss: 0.5047\n",
      "15/19, train_loss: 0.5047\n",
      "16/19, train_loss: 0.5047\n",
      "17/19, train_loss: 0.5047\n",
      "18/19, train_loss: 0.5047\n",
      "19/19, train_loss: 0.5047\n",
      "Epoch 107 average loss: 0.5047\n",
      "\n",
      "Epoch 108/300\n",
      "1/19, train_loss: 0.5047\n",
      "2/19, train_loss: 0.5046\n",
      "3/19, train_loss: 0.5046\n",
      "4/19, train_loss: 0.5046\n",
      "5/19, train_loss: 0.5046\n",
      "6/19, train_loss: 0.5046\n",
      "7/19, train_loss: 0.5046\n",
      "8/19, train_loss: 0.5046\n",
      "9/19, train_loss: 0.5046\n",
      "10/19, train_loss: 0.5046\n",
      "11/19, train_loss: 0.5046\n",
      "12/19, train_loss: 0.5046\n",
      "13/19, train_loss: 0.5046\n",
      "14/19, train_loss: 0.5046\n",
      "15/19, train_loss: 0.5046\n",
      "16/19, train_loss: 0.5046\n",
      "17/19, train_loss: 0.5046\n",
      "18/19, train_loss: 0.5046\n",
      "19/19, train_loss: 0.5046\n",
      "Epoch 108 average loss: 0.5046\n",
      "\n",
      "Epoch 109/300\n",
      "1/19, train_loss: 0.5046\n",
      "2/19, train_loss: 0.5046\n",
      "3/19, train_loss: 0.5046\n",
      "4/19, train_loss: 0.5046\n",
      "5/19, train_loss: 0.5046\n",
      "6/19, train_loss: 0.5046\n",
      "7/19, train_loss: 0.5046\n",
      "8/19, train_loss: 0.5045\n",
      "9/19, train_loss: 0.5045\n",
      "10/19, train_loss: 0.5045\n",
      "11/19, train_loss: 0.5045\n",
      "12/19, train_loss: 0.5045\n",
      "13/19, train_loss: 0.5045\n",
      "14/19, train_loss: 0.5045\n",
      "15/19, train_loss: 0.5045\n",
      "16/19, train_loss: 0.5045\n",
      "17/19, train_loss: 0.5045\n",
      "18/19, train_loss: 0.5045\n",
      "19/19, train_loss: 0.5045\n",
      "Epoch 109 average loss: 0.5045\n",
      "\n",
      "Epoch 110/300\n",
      "1/19, train_loss: 0.5045\n",
      "2/19, train_loss: 0.5045\n",
      "3/19, train_loss: 0.5045\n",
      "4/19, train_loss: 0.5045\n",
      "5/19, train_loss: 0.5045\n",
      "6/19, train_loss: 0.5045\n",
      "7/19, train_loss: 0.5045\n",
      "8/19, train_loss: 0.5045\n",
      "9/19, train_loss: 0.5045\n",
      "10/19, train_loss: 0.5045\n",
      "11/19, train_loss: 0.5045\n",
      "12/19, train_loss: 0.5045\n",
      "13/19, train_loss: 0.5045\n",
      "14/19, train_loss: 0.5044\n",
      "15/19, train_loss: 0.5044\n",
      "16/19, train_loss: 0.5044\n",
      "17/19, train_loss: 0.5044\n",
      "18/19, train_loss: 0.5044\n",
      "19/19, train_loss: 0.5044\n",
      "Epoch 110 average loss: 0.5045\n",
      "Current epoch: 110, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 111/300\n",
      "1/19, train_loss: 0.5044\n",
      "2/19, train_loss: 0.5044\n",
      "3/19, train_loss: 0.5044\n",
      "4/19, train_loss: 0.5044\n",
      "5/19, train_loss: 0.5044\n",
      "6/19, train_loss: 0.5044\n",
      "7/19, train_loss: 0.5044\n",
      "8/19, train_loss: 0.5044\n",
      "9/19, train_loss: 0.5044\n",
      "10/19, train_loss: 0.5044\n",
      "11/19, train_loss: 0.5044\n",
      "12/19, train_loss: 0.5044\n",
      "13/19, train_loss: 0.5044\n",
      "14/19, train_loss: 0.5044\n",
      "15/19, train_loss: 0.5044\n",
      "16/19, train_loss: 0.5044\n",
      "17/19, train_loss: 0.5044\n",
      "18/19, train_loss: 0.5044\n",
      "19/19, train_loss: 0.5044\n",
      "Epoch 111 average loss: 0.5044\n",
      "\n",
      "Epoch 112/300\n",
      "1/19, train_loss: 0.5043\n",
      "2/19, train_loss: 0.5043\n",
      "3/19, train_loss: 0.5043\n",
      "4/19, train_loss: 0.5043\n",
      "5/19, train_loss: 0.5043\n",
      "6/19, train_loss: 0.5043\n",
      "7/19, train_loss: 0.5043\n",
      "8/19, train_loss: 0.5043\n",
      "9/19, train_loss: 0.5043\n",
      "10/19, train_loss: 0.5043\n",
      "11/19, train_loss: 0.5043\n",
      "12/19, train_loss: 0.5043\n",
      "13/19, train_loss: 0.5043\n",
      "14/19, train_loss: 0.5043\n",
      "15/19, train_loss: 0.5043\n",
      "16/19, train_loss: 0.5043\n",
      "17/19, train_loss: 0.5043\n",
      "18/19, train_loss: 0.5043\n",
      "19/19, train_loss: 0.5043\n",
      "Epoch 112 average loss: 0.5043\n",
      "\n",
      "Epoch 113/300\n",
      "1/19, train_loss: 0.5043\n",
      "2/19, train_loss: 0.5043\n",
      "3/19, train_loss: 0.5043\n",
      "4/19, train_loss: 0.5043\n",
      "5/19, train_loss: 0.5043\n",
      "6/19, train_loss: 0.5043\n",
      "7/19, train_loss: 0.5043\n",
      "8/19, train_loss: 0.5043\n",
      "9/19, train_loss: 0.5043\n",
      "10/19, train_loss: 0.5042\n",
      "11/19, train_loss: 0.5042\n",
      "12/19, train_loss: 0.5042\n",
      "13/19, train_loss: 0.5042\n",
      "14/19, train_loss: 0.5042\n",
      "15/19, train_loss: 0.5042\n",
      "16/19, train_loss: 0.5042\n",
      "17/19, train_loss: 0.5042\n",
      "18/19, train_loss: 0.5042\n",
      "19/19, train_loss: 0.5042\n",
      "Epoch 113 average loss: 0.5042\n",
      "\n",
      "Epoch 114/300\n",
      "1/19, train_loss: 0.5042\n",
      "2/19, train_loss: 0.5042\n",
      "3/19, train_loss: 0.5042\n",
      "4/19, train_loss: 0.5042\n",
      "5/19, train_loss: 0.5042\n",
      "6/19, train_loss: 0.5042\n",
      "7/19, train_loss: 0.5042\n",
      "8/19, train_loss: 0.5042\n",
      "9/19, train_loss: 0.5042\n",
      "10/19, train_loss: 0.5042\n",
      "11/19, train_loss: 0.5042\n",
      "12/19, train_loss: 0.5042\n",
      "13/19, train_loss: 0.5042\n",
      "14/19, train_loss: 0.5042\n",
      "15/19, train_loss: 0.5042\n",
      "16/19, train_loss: 0.5042\n",
      "17/19, train_loss: 0.5042\n",
      "18/19, train_loss: 0.5042\n",
      "19/19, train_loss: 0.5041\n",
      "Epoch 114 average loss: 0.5042\n",
      "\n",
      "Epoch 115/300\n",
      "1/19, train_loss: 0.5041\n",
      "2/19, train_loss: 0.5041\n",
      "3/19, train_loss: 0.5041\n",
      "4/19, train_loss: 0.5041\n",
      "5/19, train_loss: 0.5041\n",
      "6/19, train_loss: 0.5041\n",
      "7/19, train_loss: 0.5041\n",
      "8/19, train_loss: 0.5041\n",
      "9/19, train_loss: 0.5041\n",
      "10/19, train_loss: 0.5041\n",
      "11/19, train_loss: 0.5041\n",
      "12/19, train_loss: 0.5041\n",
      "13/19, train_loss: 0.5041\n",
      "14/19, train_loss: 0.5041\n",
      "15/19, train_loss: 0.5041\n",
      "16/19, train_loss: 0.5041\n",
      "17/19, train_loss: 0.5041\n",
      "18/19, train_loss: 0.5041\n",
      "19/19, train_loss: 0.5041\n",
      "Epoch 115 average loss: 0.5041\n",
      "Current epoch: 115, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 116/300\n",
      "1/19, train_loss: 0.5041\n",
      "2/19, train_loss: 0.5041\n",
      "3/19, train_loss: 0.5041\n",
      "4/19, train_loss: 0.5041\n",
      "5/19, train_loss: 0.5041\n",
      "6/19, train_loss: 0.5041\n",
      "7/19, train_loss: 0.5041\n",
      "8/19, train_loss: 0.5040\n",
      "9/19, train_loss: 0.5040\n",
      "10/19, train_loss: 0.5040\n",
      "11/19, train_loss: 0.5040\n",
      "12/19, train_loss: 0.5040\n",
      "13/19, train_loss: 0.5040\n",
      "14/19, train_loss: 0.5040\n",
      "15/19, train_loss: 0.5040\n",
      "16/19, train_loss: 0.5040\n",
      "17/19, train_loss: 0.5040\n",
      "18/19, train_loss: 0.5040\n",
      "19/19, train_loss: 0.5040\n",
      "Epoch 116 average loss: 0.5040\n",
      "\n",
      "Epoch 117/300\n",
      "1/19, train_loss: 0.5040\n",
      "2/19, train_loss: 0.5040\n",
      "3/19, train_loss: 0.5040\n",
      "4/19, train_loss: 0.5040\n",
      "5/19, train_loss: 0.5040\n",
      "6/19, train_loss: 0.5040\n",
      "7/19, train_loss: 0.5040\n",
      "8/19, train_loss: 0.5040\n",
      "9/19, train_loss: 0.5040\n",
      "10/19, train_loss: 0.5040\n",
      "11/19, train_loss: 0.5040\n",
      "12/19, train_loss: 0.5040\n",
      "13/19, train_loss: 0.5040\n",
      "14/19, train_loss: 0.5040\n",
      "15/19, train_loss: 0.5040\n",
      "16/19, train_loss: 0.5040\n",
      "17/19, train_loss: 0.5040\n",
      "18/19, train_loss: 0.5040\n",
      "19/19, train_loss: 0.5039\n",
      "Epoch 117 average loss: 0.5040\n",
      "\n",
      "Epoch 118/300\n",
      "1/19, train_loss: 0.5039\n",
      "2/19, train_loss: 0.5039\n",
      "3/19, train_loss: 0.5039\n",
      "4/19, train_loss: 0.5039\n",
      "5/19, train_loss: 0.5039\n",
      "6/19, train_loss: 0.5039\n",
      "7/19, train_loss: 0.5039\n",
      "8/19, train_loss: 0.5039\n",
      "9/19, train_loss: 0.5039\n",
      "10/19, train_loss: 0.5039\n",
      "11/19, train_loss: 0.5039\n",
      "12/19, train_loss: 0.5039\n",
      "13/19, train_loss: 0.5039\n",
      "14/19, train_loss: 0.5039\n",
      "15/19, train_loss: 0.5039\n",
      "16/19, train_loss: 0.5039\n",
      "17/19, train_loss: 0.5039\n",
      "18/19, train_loss: 0.5039\n",
      "19/19, train_loss: 0.5039\n",
      "Epoch 118 average loss: 0.5039\n",
      "\n",
      "Epoch 119/300\n",
      "1/19, train_loss: 0.5039\n",
      "2/19, train_loss: 0.5039\n",
      "3/19, train_loss: 0.5039\n",
      "4/19, train_loss: 0.5039\n",
      "5/19, train_loss: 0.5039\n",
      "6/19, train_loss: 0.5039\n",
      "7/19, train_loss: 0.5039\n",
      "8/19, train_loss: 0.5039\n",
      "9/19, train_loss: 0.5039\n",
      "10/19, train_loss: 0.5039\n",
      "11/19, train_loss: 0.5039\n",
      "12/19, train_loss: 0.5039\n",
      "13/19, train_loss: 0.5038\n",
      "14/19, train_loss: 0.5038\n",
      "15/19, train_loss: 0.5038\n",
      "16/19, train_loss: 0.5038\n",
      "17/19, train_loss: 0.5038\n",
      "18/19, train_loss: 0.5038\n",
      "19/19, train_loss: 0.5038\n",
      "Epoch 119 average loss: 0.5039\n",
      "\n",
      "Epoch 120/300\n",
      "1/19, train_loss: 0.5038\n",
      "2/19, train_loss: 0.5038\n",
      "3/19, train_loss: 0.5038\n",
      "4/19, train_loss: 0.5038\n",
      "5/19, train_loss: 0.5038\n",
      "6/19, train_loss: 0.5038\n",
      "7/19, train_loss: 0.5038\n",
      "8/19, train_loss: 0.5038\n",
      "9/19, train_loss: 0.5038\n",
      "10/19, train_loss: 0.5038\n",
      "11/19, train_loss: 0.5038\n",
      "12/19, train_loss: 0.5038\n",
      "13/19, train_loss: 0.5038\n",
      "14/19, train_loss: 0.5038\n",
      "15/19, train_loss: 0.5038\n",
      "16/19, train_loss: 0.5038\n",
      "17/19, train_loss: 0.5038\n",
      "18/19, train_loss: 0.5038\n",
      "19/19, train_loss: 0.5038\n",
      "Epoch 120 average loss: 0.5038\n",
      "Current epoch: 120, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 121/300\n",
      "1/19, train_loss: 0.5038\n",
      "2/19, train_loss: 0.5038\n",
      "3/19, train_loss: 0.5038\n",
      "4/19, train_loss: 0.5038\n",
      "5/19, train_loss: 0.5037\n",
      "6/19, train_loss: 0.5037\n",
      "7/19, train_loss: 0.5037\n",
      "8/19, train_loss: 0.5037\n",
      "9/19, train_loss: 0.5037\n",
      "10/19, train_loss: 0.5037\n",
      "11/19, train_loss: 0.5037\n",
      "12/19, train_loss: 0.5037\n",
      "13/19, train_loss: 0.5037\n",
      "14/19, train_loss: 0.5037\n",
      "15/19, train_loss: 0.5037\n",
      "16/19, train_loss: 0.5037\n",
      "17/19, train_loss: 0.5037\n",
      "18/19, train_loss: 0.5037\n",
      "19/19, train_loss: 0.5037\n",
      "Epoch 121 average loss: 0.5037\n",
      "\n",
      "Epoch 122/300\n",
      "1/19, train_loss: 0.5037\n",
      "2/19, train_loss: 0.5037\n",
      "3/19, train_loss: 0.5037\n",
      "4/19, train_loss: 0.5037\n",
      "5/19, train_loss: 0.5037\n",
      "6/19, train_loss: 0.5037\n",
      "7/19, train_loss: 0.5037\n",
      "8/19, train_loss: 0.5037\n",
      "9/19, train_loss: 0.5037\n",
      "10/19, train_loss: 0.5037\n",
      "11/19, train_loss: 0.5037\n",
      "12/19, train_loss: 0.5037\n",
      "13/19, train_loss: 0.5037\n",
      "14/19, train_loss: 0.5037\n",
      "15/19, train_loss: 0.5037\n",
      "16/19, train_loss: 0.5037\n",
      "17/19, train_loss: 0.5036\n",
      "18/19, train_loss: 0.5036\n",
      "19/19, train_loss: 0.5036\n",
      "Epoch 122 average loss: 0.5037\n",
      "\n",
      "Epoch 123/300\n",
      "1/19, train_loss: 0.5036\n",
      "2/19, train_loss: 0.5036\n",
      "3/19, train_loss: 0.5036\n",
      "4/19, train_loss: 0.5036\n",
      "5/19, train_loss: 0.5036\n",
      "6/19, train_loss: 0.5036\n",
      "7/19, train_loss: 0.5036\n",
      "8/19, train_loss: 0.5036\n",
      "9/19, train_loss: 0.5036\n",
      "10/19, train_loss: 0.5036\n",
      "11/19, train_loss: 0.5036\n",
      "12/19, train_loss: 0.5036\n",
      "13/19, train_loss: 0.5036\n",
      "14/19, train_loss: 0.5036\n",
      "15/19, train_loss: 0.5036\n",
      "16/19, train_loss: 0.5036\n",
      "17/19, train_loss: 0.5036\n",
      "18/19, train_loss: 0.5036\n",
      "19/19, train_loss: 0.5036\n",
      "Epoch 123 average loss: 0.5036\n",
      "\n",
      "Epoch 124/300\n",
      "1/19, train_loss: 0.5036\n",
      "2/19, train_loss: 0.5036\n",
      "3/19, train_loss: 0.5036\n",
      "4/19, train_loss: 0.5036\n",
      "5/19, train_loss: 0.5036\n",
      "6/19, train_loss: 0.5036\n",
      "7/19, train_loss: 0.5036\n",
      "8/19, train_loss: 0.5036\n",
      "9/19, train_loss: 0.5036\n",
      "10/19, train_loss: 0.5036\n",
      "11/19, train_loss: 0.5035\n",
      "12/19, train_loss: 0.5035\n",
      "13/19, train_loss: 0.5035\n",
      "14/19, train_loss: 0.5035\n",
      "15/19, train_loss: 0.5035\n",
      "16/19, train_loss: 0.5035\n",
      "17/19, train_loss: 0.5035\n",
      "18/19, train_loss: 0.5035\n",
      "19/19, train_loss: 0.5035\n",
      "Epoch 124 average loss: 0.5036\n",
      "\n",
      "Epoch 125/300\n",
      "1/19, train_loss: 0.5035\n",
      "2/19, train_loss: 0.5035\n",
      "3/19, train_loss: 0.5035\n",
      "4/19, train_loss: 0.5035\n",
      "5/19, train_loss: 0.5035\n",
      "6/19, train_loss: 0.5035\n",
      "7/19, train_loss: 0.5035\n",
      "8/19, train_loss: 0.5035\n",
      "9/19, train_loss: 0.5035\n",
      "10/19, train_loss: 0.5035\n",
      "11/19, train_loss: 0.5035\n",
      "12/19, train_loss: 0.5035\n",
      "13/19, train_loss: 0.5035\n",
      "14/19, train_loss: 0.5035\n",
      "15/19, train_loss: 0.5035\n",
      "16/19, train_loss: 0.5035\n",
      "17/19, train_loss: 0.5035\n",
      "18/19, train_loss: 0.5035\n",
      "19/19, train_loss: 0.5035\n",
      "Epoch 125 average loss: 0.5035\n",
      "Current epoch: 125, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 126/300\n",
      "1/19, train_loss: 0.5035\n",
      "2/19, train_loss: 0.5035\n",
      "3/19, train_loss: 0.5035\n",
      "4/19, train_loss: 0.5035\n",
      "5/19, train_loss: 0.5035\n",
      "6/19, train_loss: 0.5035\n",
      "7/19, train_loss: 0.5035\n",
      "8/19, train_loss: 0.5034\n",
      "9/19, train_loss: 0.5034\n",
      "10/19, train_loss: 0.5034\n",
      "11/19, train_loss: 0.5034\n",
      "12/19, train_loss: 0.5034\n",
      "13/19, train_loss: 0.5034\n",
      "14/19, train_loss: 0.5034\n",
      "15/19, train_loss: 0.5034\n",
      "16/19, train_loss: 0.5034\n",
      "17/19, train_loss: 0.5034\n",
      "18/19, train_loss: 0.5034\n",
      "19/19, train_loss: 0.5034\n",
      "Epoch 126 average loss: 0.5034\n",
      "\n",
      "Epoch 127/300\n",
      "1/19, train_loss: 0.5034\n",
      "2/19, train_loss: 0.5034\n",
      "3/19, train_loss: 0.5034\n",
      "4/19, train_loss: 0.5034\n",
      "5/19, train_loss: 0.5034\n",
      "6/19, train_loss: 0.5034\n",
      "7/19, train_loss: 0.5034\n",
      "8/19, train_loss: 0.5034\n",
      "9/19, train_loss: 0.5034\n",
      "10/19, train_loss: 0.5034\n",
      "11/19, train_loss: 0.5034\n",
      "12/19, train_loss: 0.5034\n",
      "13/19, train_loss: 0.5034\n",
      "14/19, train_loss: 0.5034\n",
      "15/19, train_loss: 0.5034\n",
      "16/19, train_loss: 0.5034\n",
      "17/19, train_loss: 0.5034\n",
      "18/19, train_loss: 0.5034\n",
      "19/19, train_loss: 0.5034\n",
      "Epoch 127 average loss: 0.5034\n",
      "\n",
      "Epoch 128/300\n",
      "1/19, train_loss: 0.5034\n",
      "2/19, train_loss: 0.5034\n",
      "3/19, train_loss: 0.5034\n",
      "4/19, train_loss: 0.5034\n",
      "5/19, train_loss: 0.5034\n",
      "6/19, train_loss: 0.5033\n",
      "7/19, train_loss: 0.5033\n",
      "8/19, train_loss: 0.5033\n",
      "9/19, train_loss: 0.5033\n",
      "10/19, train_loss: 0.5033\n",
      "11/19, train_loss: 0.5033\n",
      "12/19, train_loss: 0.5033\n",
      "13/19, train_loss: 0.5033\n",
      "14/19, train_loss: 0.5033\n",
      "15/19, train_loss: 0.5033\n",
      "16/19, train_loss: 0.5033\n",
      "17/19, train_loss: 0.5033\n",
      "18/19, train_loss: 0.5033\n",
      "19/19, train_loss: 0.5033\n",
      "Epoch 128 average loss: 0.5033\n",
      "\n",
      "Epoch 129/300\n",
      "1/19, train_loss: 0.5033\n",
      "2/19, train_loss: 0.5033\n",
      "3/19, train_loss: 0.5033\n",
      "4/19, train_loss: 0.5033\n",
      "5/19, train_loss: 0.5033\n",
      "6/19, train_loss: 0.5033\n",
      "7/19, train_loss: 0.5033\n",
      "8/19, train_loss: 0.5033\n",
      "9/19, train_loss: 0.5033\n",
      "10/19, train_loss: 0.5033\n",
      "11/19, train_loss: 0.5033\n",
      "12/19, train_loss: 0.5033\n",
      "13/19, train_loss: 0.5033\n",
      "14/19, train_loss: 0.5033\n",
      "15/19, train_loss: 0.5033\n",
      "16/19, train_loss: 0.5033\n",
      "17/19, train_loss: 0.5033\n",
      "18/19, train_loss: 0.5033\n",
      "19/19, train_loss: 0.5033\n",
      "Epoch 129 average loss: 0.5033\n",
      "\n",
      "Epoch 130/300\n",
      "1/19, train_loss: 0.5033\n",
      "2/19, train_loss: 0.5033\n",
      "3/19, train_loss: 0.5033\n",
      "4/19, train_loss: 0.5033\n",
      "5/19, train_loss: 0.5032\n",
      "6/19, train_loss: 0.5032\n",
      "7/19, train_loss: 0.5033\n",
      "8/19, train_loss: 0.5032\n",
      "9/19, train_loss: 0.5032\n",
      "10/19, train_loss: 0.5032\n",
      "11/19, train_loss: 0.5032\n",
      "12/19, train_loss: 0.5032\n",
      "13/19, train_loss: 0.5032\n",
      "14/19, train_loss: 0.5032\n",
      "15/19, train_loss: 0.5032\n",
      "16/19, train_loss: 0.5032\n",
      "17/19, train_loss: 0.5032\n",
      "18/19, train_loss: 0.5032\n",
      "19/19, train_loss: 0.5032\n",
      "Epoch 130 average loss: 0.5032\n",
      "Current epoch: 130, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 131/300\n",
      "1/19, train_loss: 0.5032\n",
      "2/19, train_loss: 0.5032\n",
      "3/19, train_loss: 0.5032\n",
      "4/19, train_loss: 0.5032\n",
      "5/19, train_loss: 0.5032\n",
      "6/19, train_loss: 0.5032\n",
      "7/19, train_loss: 0.5032\n",
      "8/19, train_loss: 0.5032\n",
      "9/19, train_loss: 0.5032\n",
      "10/19, train_loss: 0.5032\n",
      "11/19, train_loss: 0.5032\n",
      "12/19, train_loss: 0.5032\n",
      "13/19, train_loss: 0.5032\n",
      "14/19, train_loss: 0.5032\n",
      "15/19, train_loss: 0.5032\n",
      "16/19, train_loss: 0.5032\n",
      "17/19, train_loss: 0.5032\n",
      "18/19, train_loss: 0.5032\n",
      "19/19, train_loss: 0.5032\n",
      "Epoch 131 average loss: 0.5032\n",
      "\n",
      "Epoch 132/300\n",
      "1/19, train_loss: 0.5032\n",
      "2/19, train_loss: 0.5032\n",
      "3/19, train_loss: 0.5032\n",
      "4/19, train_loss: 0.5032\n",
      "5/19, train_loss: 0.5032\n",
      "6/19, train_loss: 0.5031\n",
      "7/19, train_loss: 0.5031\n",
      "8/19, train_loss: 0.5031\n",
      "9/19, train_loss: 0.5031\n",
      "10/19, train_loss: 0.5031\n",
      "11/19, train_loss: 0.5031\n",
      "12/19, train_loss: 0.5031\n",
      "13/19, train_loss: 0.5031\n",
      "14/19, train_loss: 0.5031\n",
      "15/19, train_loss: 0.5031\n",
      "16/19, train_loss: 0.5031\n",
      "17/19, train_loss: 0.5031\n",
      "18/19, train_loss: 0.5031\n",
      "19/19, train_loss: 0.5031\n",
      "Epoch 132 average loss: 0.5031\n",
      "\n",
      "Epoch 133/300\n",
      "1/19, train_loss: 0.5031\n",
      "2/19, train_loss: 0.5031\n",
      "3/19, train_loss: 0.5031\n",
      "4/19, train_loss: 0.5031\n",
      "5/19, train_loss: 0.5031\n",
      "6/19, train_loss: 0.5031\n",
      "7/19, train_loss: 0.5031\n",
      "8/19, train_loss: 0.5031\n",
      "9/19, train_loss: 0.5031\n",
      "10/19, train_loss: 0.5031\n",
      "11/19, train_loss: 0.5031\n",
      "12/19, train_loss: 0.5031\n",
      "13/19, train_loss: 0.5031\n",
      "14/19, train_loss: 0.5031\n",
      "15/19, train_loss: 0.5031\n",
      "16/19, train_loss: 0.5031\n",
      "17/19, train_loss: 0.5031\n",
      "18/19, train_loss: 0.5031\n",
      "19/19, train_loss: 0.5031\n",
      "Epoch 133 average loss: 0.5031\n",
      "\n",
      "Epoch 134/300\n",
      "1/19, train_loss: 0.5031\n",
      "2/19, train_loss: 0.5031\n",
      "3/19, train_loss: 0.5031\n",
      "4/19, train_loss: 0.5031\n",
      "5/19, train_loss: 0.5031\n",
      "6/19, train_loss: 0.5031\n",
      "7/19, train_loss: 0.5031\n",
      "8/19, train_loss: 0.5031\n",
      "9/19, train_loss: 0.5031\n",
      "10/19, train_loss: 0.5030\n",
      "11/19, train_loss: 0.5030\n",
      "12/19, train_loss: 0.5031\n",
      "13/19, train_loss: 0.5030\n",
      "14/19, train_loss: 0.5030\n",
      "15/19, train_loss: 0.5030\n",
      "16/19, train_loss: 0.5030\n",
      "17/19, train_loss: 0.5030\n",
      "18/19, train_loss: 0.5030\n",
      "19/19, train_loss: 0.5030\n",
      "Epoch 134 average loss: 0.5030\n",
      "\n",
      "Epoch 135/300\n",
      "1/19, train_loss: 0.5030\n",
      "2/19, train_loss: 0.5030\n",
      "3/19, train_loss: 0.5030\n",
      "4/19, train_loss: 0.5030\n",
      "5/19, train_loss: 0.5030\n",
      "6/19, train_loss: 0.5030\n",
      "7/19, train_loss: 0.5030\n",
      "8/19, train_loss: 0.5030\n",
      "9/19, train_loss: 0.5030\n",
      "10/19, train_loss: 0.5030\n",
      "11/19, train_loss: 0.5030\n",
      "12/19, train_loss: 0.5030\n",
      "13/19, train_loss: 0.5030\n",
      "14/19, train_loss: 0.5030\n",
      "15/19, train_loss: 0.5030\n",
      "16/19, train_loss: 0.5030\n",
      "17/19, train_loss: 0.5030\n",
      "18/19, train_loss: 0.5030\n",
      "19/19, train_loss: 0.5030\n",
      "Epoch 135 average loss: 0.5030\n",
      "Current epoch: 135, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 136/300\n",
      "1/19, train_loss: 0.5030\n",
      "2/19, train_loss: 0.5030\n",
      "3/19, train_loss: 0.5030\n",
      "4/19, train_loss: 0.5030\n",
      "5/19, train_loss: 0.5030\n",
      "6/19, train_loss: 0.5030\n",
      "7/19, train_loss: 0.5030\n",
      "8/19, train_loss: 0.5030\n",
      "9/19, train_loss: 0.5030\n",
      "10/19, train_loss: 0.5030\n",
      "11/19, train_loss: 0.5030\n",
      "12/19, train_loss: 0.5029\n",
      "13/19, train_loss: 0.5029\n",
      "14/19, train_loss: 0.5029\n",
      "15/19, train_loss: 0.5029\n",
      "16/19, train_loss: 0.5029\n",
      "17/19, train_loss: 0.5029\n",
      "18/19, train_loss: 0.5029\n",
      "19/19, train_loss: 0.5029\n",
      "Epoch 136 average loss: 0.5030\n",
      "\n",
      "Epoch 137/300\n",
      "1/19, train_loss: 0.5029\n",
      "2/19, train_loss: 0.5029\n",
      "3/19, train_loss: 0.5029\n",
      "4/19, train_loss: 0.5029\n",
      "5/19, train_loss: 0.5029\n",
      "6/19, train_loss: 0.5029\n",
      "7/19, train_loss: 0.5029\n",
      "8/19, train_loss: 0.5029\n",
      "9/19, train_loss: 0.5029\n",
      "10/19, train_loss: 0.5029\n",
      "11/19, train_loss: 0.5029\n",
      "12/19, train_loss: 0.5029\n",
      "13/19, train_loss: 0.5029\n",
      "14/19, train_loss: 0.5029\n",
      "15/19, train_loss: 0.5029\n",
      "16/19, train_loss: 0.5029\n",
      "17/19, train_loss: 0.5028\n",
      "18/19, train_loss: 0.5028\n",
      "19/19, train_loss: 0.5028\n",
      "Epoch 137 average loss: 0.5029\n",
      "\n",
      "Epoch 138/300\n",
      "1/19, train_loss: 0.5028\n",
      "2/19, train_loss: 0.5028\n",
      "3/19, train_loss: 0.5028\n",
      "4/19, train_loss: 0.5028\n",
      "5/19, train_loss: 0.5027\n",
      "6/19, train_loss: 0.5027\n",
      "7/19, train_loss: 0.5027\n",
      "8/19, train_loss: 0.5027\n",
      "9/19, train_loss: 0.5027\n",
      "10/19, train_loss: 0.5027\n",
      "11/19, train_loss: 0.5027\n",
      "12/19, train_loss: 0.5027\n",
      "13/19, train_loss: 0.5027\n",
      "14/19, train_loss: 0.5027\n",
      "15/19, train_loss: 0.5027\n",
      "16/19, train_loss: 0.5027\n",
      "17/19, train_loss: 0.5027\n",
      "18/19, train_loss: 0.5027\n",
      "19/19, train_loss: 0.5027\n",
      "Epoch 138 average loss: 0.5027\n",
      "\n",
      "Epoch 139/300\n",
      "1/19, train_loss: 0.5027\n",
      "2/19, train_loss: 0.5027\n",
      "3/19, train_loss: 0.5027\n",
      "4/19, train_loss: 0.5027\n",
      "5/19, train_loss: 0.5027\n",
      "6/19, train_loss: 0.5027\n",
      "7/19, train_loss: 0.5027\n",
      "8/19, train_loss: 0.5026\n",
      "9/19, train_loss: 0.5026\n",
      "10/19, train_loss: 0.5026\n",
      "11/19, train_loss: 0.5026\n",
      "12/19, train_loss: 0.5026\n",
      "13/19, train_loss: 0.5026\n",
      "14/19, train_loss: 0.5026\n",
      "15/19, train_loss: 0.5026\n",
      "16/19, train_loss: 0.5026\n",
      "17/19, train_loss: 0.5026\n",
      "18/19, train_loss: 0.5026\n",
      "19/19, train_loss: 0.5026\n",
      "Epoch 139 average loss: 0.5026\n",
      "\n",
      "Epoch 140/300\n",
      "1/19, train_loss: 0.5026\n",
      "2/19, train_loss: 0.5026\n",
      "3/19, train_loss: 0.5026\n",
      "4/19, train_loss: 0.5026\n",
      "5/19, train_loss: 0.5026\n",
      "6/19, train_loss: 0.5026\n",
      "7/19, train_loss: 0.5026\n",
      "8/19, train_loss: 0.5026\n",
      "9/19, train_loss: 0.5026\n",
      "10/19, train_loss: 0.5026\n",
      "11/19, train_loss: 0.5026\n",
      "12/19, train_loss: 0.5026\n",
      "13/19, train_loss: 0.5026\n",
      "14/19, train_loss: 0.5026\n",
      "15/19, train_loss: 0.5026\n",
      "16/19, train_loss: 0.5026\n",
      "17/19, train_loss: 0.5026\n",
      "18/19, train_loss: 0.5026\n",
      "19/19, train_loss: 0.5026\n",
      "Epoch 140 average loss: 0.5026\n",
      "Current epoch: 140, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 141/300\n",
      "1/19, train_loss: 0.5026\n",
      "2/19, train_loss: 0.5026\n",
      "3/19, train_loss: 0.5026\n",
      "4/19, train_loss: 0.5026\n",
      "5/19, train_loss: 0.5026\n",
      "6/19, train_loss: 0.5026\n",
      "7/19, train_loss: 0.5026\n",
      "8/19, train_loss: 0.5026\n",
      "9/19, train_loss: 0.5026\n",
      "10/19, train_loss: 0.5026\n",
      "11/19, train_loss: 0.5026\n",
      "12/19, train_loss: 0.5025\n",
      "13/19, train_loss: 0.5025\n",
      "14/19, train_loss: 0.5025\n",
      "15/19, train_loss: 0.5025\n",
      "16/19, train_loss: 0.5025\n",
      "17/19, train_loss: 0.5025\n",
      "18/19, train_loss: 0.5025\n",
      "19/19, train_loss: 0.5025\n",
      "Epoch 141 average loss: 0.5026\n",
      "\n",
      "Epoch 142/300\n",
      "1/19, train_loss: 0.5025\n",
      "2/19, train_loss: 0.5025\n",
      "3/19, train_loss: 0.5025\n",
      "4/19, train_loss: 0.5025\n",
      "5/19, train_loss: 0.5025\n",
      "6/19, train_loss: 0.5025\n",
      "7/19, train_loss: 0.5025\n",
      "8/19, train_loss: 0.5025\n",
      "9/19, train_loss: 0.5025\n",
      "10/19, train_loss: 0.5025\n",
      "11/19, train_loss: 0.5025\n",
      "12/19, train_loss: 0.5025\n",
      "13/19, train_loss: 0.5025\n",
      "14/19, train_loss: 0.5025\n",
      "15/19, train_loss: 0.5025\n",
      "16/19, train_loss: 0.5025\n",
      "17/19, train_loss: 0.5025\n",
      "18/19, train_loss: 0.5025\n",
      "19/19, train_loss: 0.5025\n",
      "Epoch 142 average loss: 0.5025\n",
      "\n",
      "Epoch 143/300\n",
      "1/19, train_loss: 0.5025\n",
      "2/19, train_loss: 0.5025\n",
      "3/19, train_loss: 0.5025\n",
      "4/19, train_loss: 0.5025\n",
      "5/19, train_loss: 0.5025\n",
      "6/19, train_loss: 0.5025\n",
      "7/19, train_loss: 0.5025\n",
      "8/19, train_loss: 0.5025\n",
      "9/19, train_loss: 0.5025\n",
      "10/19, train_loss: 0.5025\n",
      "11/19, train_loss: 0.5025\n",
      "12/19, train_loss: 0.5025\n",
      "13/19, train_loss: 0.5025\n",
      "14/19, train_loss: 0.5025\n",
      "15/19, train_loss: 0.5025\n",
      "16/19, train_loss: 0.5025\n",
      "17/19, train_loss: 0.5025\n",
      "18/19, train_loss: 0.5024\n",
      "19/19, train_loss: 0.5024\n",
      "Epoch 143 average loss: 0.5025\n",
      "\n",
      "Epoch 144/300\n",
      "1/19, train_loss: 0.5024\n",
      "2/19, train_loss: 0.5024\n",
      "3/19, train_loss: 0.5024\n",
      "4/19, train_loss: 0.5024\n",
      "5/19, train_loss: 0.5024\n",
      "6/19, train_loss: 0.5024\n",
      "7/19, train_loss: 0.5024\n",
      "8/19, train_loss: 0.5024\n",
      "9/19, train_loss: 0.5024\n",
      "10/19, train_loss: 0.5024\n",
      "11/19, train_loss: 0.5024\n",
      "12/19, train_loss: 0.5024\n",
      "13/19, train_loss: 0.5024\n",
      "14/19, train_loss: 0.5024\n",
      "15/19, train_loss: 0.5024\n",
      "16/19, train_loss: 0.5024\n",
      "17/19, train_loss: 0.5024\n",
      "18/19, train_loss: 0.5024\n",
      "19/19, train_loss: 0.5024\n",
      "Epoch 144 average loss: 0.5024\n",
      "\n",
      "Epoch 145/300\n",
      "1/19, train_loss: 0.5024\n",
      "2/19, train_loss: 0.5024\n",
      "3/19, train_loss: 0.5024\n",
      "4/19, train_loss: 0.5024\n",
      "5/19, train_loss: 0.5024\n",
      "6/19, train_loss: 0.5024\n",
      "7/19, train_loss: 0.5024\n",
      "8/19, train_loss: 0.5024\n",
      "9/19, train_loss: 0.5024\n",
      "10/19, train_loss: 0.5024\n",
      "11/19, train_loss: 0.5024\n",
      "12/19, train_loss: 0.5024\n",
      "13/19, train_loss: 0.5024\n",
      "14/19, train_loss: 0.5024\n",
      "15/19, train_loss: 0.5024\n",
      "16/19, train_loss: 0.5024\n",
      "17/19, train_loss: 0.5024\n",
      "18/19, train_loss: 0.5024\n",
      "19/19, train_loss: 0.5024\n",
      "Epoch 145 average loss: 0.5024\n",
      "Current epoch: 145, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 146/300\n",
      "1/19, train_loss: 0.5024\n",
      "2/19, train_loss: 0.5024\n",
      "3/19, train_loss: 0.5024\n",
      "4/19, train_loss: 0.5024\n",
      "5/19, train_loss: 0.5024\n",
      "6/19, train_loss: 0.5024\n",
      "7/19, train_loss: 0.5024\n",
      "8/19, train_loss: 0.5024\n",
      "9/19, train_loss: 0.5024\n",
      "10/19, train_loss: 0.5024\n",
      "11/19, train_loss: 0.5024\n",
      "12/19, train_loss: 0.5024\n",
      "13/19, train_loss: 0.5024\n",
      "14/19, train_loss: 0.5024\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5023\n",
      "17/19, train_loss: 0.5023\n",
      "18/19, train_loss: 0.5023\n",
      "19/19, train_loss: 0.5023\n",
      "Epoch 146 average loss: 0.5024\n",
      "\n",
      "Epoch 147/300\n",
      "1/19, train_loss: 0.5023\n",
      "2/19, train_loss: 0.5023\n",
      "3/19, train_loss: 0.5023\n",
      "4/19, train_loss: 0.5023\n",
      "5/19, train_loss: 0.5023\n",
      "6/19, train_loss: 0.5023\n",
      "7/19, train_loss: 0.5023\n",
      "8/19, train_loss: 0.5023\n",
      "9/19, train_loss: 0.5023\n",
      "10/19, train_loss: 0.5023\n",
      "11/19, train_loss: 0.5023\n",
      "12/19, train_loss: 0.5023\n",
      "13/19, train_loss: 0.5023\n",
      "14/19, train_loss: 0.5023\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5023\n",
      "17/19, train_loss: 0.5023\n",
      "18/19, train_loss: 0.5023\n",
      "19/19, train_loss: 0.5023\n",
      "Epoch 147 average loss: 0.5023\n",
      "\n",
      "Epoch 148/300\n",
      "1/19, train_loss: 0.5023\n",
      "2/19, train_loss: 0.5023\n",
      "3/19, train_loss: 0.5023\n",
      "4/19, train_loss: 0.5023\n",
      "5/19, train_loss: 0.5023\n",
      "6/19, train_loss: 0.5023\n",
      "7/19, train_loss: 0.5023\n",
      "8/19, train_loss: 0.5023\n",
      "9/19, train_loss: 0.5023\n",
      "10/19, train_loss: 0.5023\n",
      "11/19, train_loss: 0.5023\n",
      "12/19, train_loss: 0.5023\n",
      "13/19, train_loss: 0.5023\n",
      "14/19, train_loss: 0.5023\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5023\n",
      "17/19, train_loss: 0.5023\n",
      "18/19, train_loss: 0.5023\n",
      "19/19, train_loss: 0.5023\n",
      "Epoch 148 average loss: 0.5023\n",
      "\n",
      "Epoch 149/300\n",
      "1/19, train_loss: 0.5023\n",
      "2/19, train_loss: 0.5023\n",
      "3/19, train_loss: 0.5023\n",
      "4/19, train_loss: 0.5023\n",
      "5/19, train_loss: 0.5023\n",
      "6/19, train_loss: 0.5023\n",
      "7/19, train_loss: 0.5023\n",
      "8/19, train_loss: 0.5023\n",
      "9/19, train_loss: 0.5023\n",
      "10/19, train_loss: 0.5023\n",
      "11/19, train_loss: 0.5023\n",
      "12/19, train_loss: 0.5023\n",
      "13/19, train_loss: 0.5023\n",
      "14/19, train_loss: 0.5023\n",
      "15/19, train_loss: 0.5023\n",
      "16/19, train_loss: 0.5022\n",
      "17/19, train_loss: 0.5022\n",
      "18/19, train_loss: 0.5022\n",
      "19/19, train_loss: 0.5022\n",
      "Epoch 149 average loss: 0.5023\n",
      "\n",
      "Epoch 150/300\n",
      "1/19, train_loss: 0.5022\n",
      "2/19, train_loss: 0.5022\n",
      "3/19, train_loss: 0.5022\n",
      "4/19, train_loss: 0.5022\n",
      "5/19, train_loss: 0.5022\n",
      "6/19, train_loss: 0.5022\n",
      "7/19, train_loss: 0.5022\n",
      "8/19, train_loss: 0.5022\n",
      "9/19, train_loss: 0.5022\n",
      "10/19, train_loss: 0.5022\n",
      "11/19, train_loss: 0.5022\n",
      "12/19, train_loss: 0.5022\n",
      "13/19, train_loss: 0.5022\n",
      "14/19, train_loss: 0.5022\n",
      "15/19, train_loss: 0.5022\n",
      "16/19, train_loss: 0.5022\n",
      "17/19, train_loss: 0.5022\n",
      "18/19, train_loss: 0.5022\n",
      "19/19, train_loss: 0.5022\n",
      "Epoch 150 average loss: 0.5022\n",
      "Current epoch: 150, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 151/300\n",
      "1/19, train_loss: 0.5022\n",
      "2/19, train_loss: 0.5022\n",
      "3/19, train_loss: 0.5022\n",
      "4/19, train_loss: 0.5022\n",
      "5/19, train_loss: 0.5022\n",
      "6/19, train_loss: 0.5022\n",
      "7/19, train_loss: 0.5022\n",
      "8/19, train_loss: 0.5022\n",
      "9/19, train_loss: 0.5022\n",
      "10/19, train_loss: 0.5022\n",
      "11/19, train_loss: 0.5022\n",
      "12/19, train_loss: 0.5022\n",
      "13/19, train_loss: 0.5022\n",
      "14/19, train_loss: 0.5022\n",
      "15/19, train_loss: 0.5022\n",
      "16/19, train_loss: 0.5022\n",
      "17/19, train_loss: 0.5022\n",
      "18/19, train_loss: 0.5022\n",
      "19/19, train_loss: 0.5022\n",
      "Epoch 151 average loss: 0.5022\n",
      "\n",
      "Epoch 152/300\n",
      "1/19, train_loss: 0.5022\n",
      "2/19, train_loss: 0.5022\n",
      "3/19, train_loss: 0.5022\n",
      "4/19, train_loss: 0.5022\n",
      "5/19, train_loss: 0.5022\n",
      "6/19, train_loss: 0.5022\n",
      "7/19, train_loss: 0.5022\n",
      "8/19, train_loss: 0.5022\n",
      "9/19, train_loss: 0.5022\n",
      "10/19, train_loss: 0.5022\n",
      "11/19, train_loss: 0.5022\n",
      "12/19, train_loss: 0.5022\n",
      "13/19, train_loss: 0.5022\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 152 average loss: 0.5022\n",
      "\n",
      "Epoch 153/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5021\n",
      "3/19, train_loss: 0.5021\n",
      "4/19, train_loss: 0.5021\n",
      "5/19, train_loss: 0.5021\n",
      "6/19, train_loss: 0.5021\n",
      "7/19, train_loss: 0.5021\n",
      "8/19, train_loss: 0.5021\n",
      "9/19, train_loss: 0.5021\n",
      "10/19, train_loss: 0.5021\n",
      "11/19, train_loss: 0.5021\n",
      "12/19, train_loss: 0.5021\n",
      "13/19, train_loss: 0.5021\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 153 average loss: 0.5021\n",
      "\n",
      "Epoch 154/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5021\n",
      "3/19, train_loss: 0.5021\n",
      "4/19, train_loss: 0.5021\n",
      "5/19, train_loss: 0.5021\n",
      "6/19, train_loss: 0.5021\n",
      "7/19, train_loss: 0.5021\n",
      "8/19, train_loss: 0.5021\n",
      "9/19, train_loss: 0.5021\n",
      "10/19, train_loss: 0.5021\n",
      "11/19, train_loss: 0.5021\n",
      "12/19, train_loss: 0.5021\n",
      "13/19, train_loss: 0.5021\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 154 average loss: 0.5021\n",
      "\n",
      "Epoch 155/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5021\n",
      "3/19, train_loss: 0.5021\n",
      "4/19, train_loss: 0.5021\n",
      "5/19, train_loss: 0.5021\n",
      "6/19, train_loss: 0.5021\n",
      "7/19, train_loss: 0.5021\n",
      "8/19, train_loss: 0.5021\n",
      "9/19, train_loss: 0.5021\n",
      "10/19, train_loss: 0.5021\n",
      "11/19, train_loss: 0.5021\n",
      "12/19, train_loss: 0.5021\n",
      "13/19, train_loss: 0.5021\n",
      "14/19, train_loss: 0.5021\n",
      "15/19, train_loss: 0.5021\n",
      "16/19, train_loss: 0.5021\n",
      "17/19, train_loss: 0.5021\n",
      "18/19, train_loss: 0.5021\n",
      "19/19, train_loss: 0.5021\n",
      "Epoch 155 average loss: 0.5021\n",
      "Current epoch: 155, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 156/300\n",
      "1/19, train_loss: 0.5021\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5020\n",
      "19/19, train_loss: 0.5020\n",
      "Epoch 156 average loss: 0.5020\n",
      "\n",
      "Epoch 157/300\n",
      "1/19, train_loss: 0.5020\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5020\n",
      "19/19, train_loss: 0.5020\n",
      "Epoch 157 average loss: 0.5020\n",
      "\n",
      "Epoch 158/300\n",
      "1/19, train_loss: 0.5020\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5020\n",
      "19/19, train_loss: 0.5020\n",
      "Epoch 158 average loss: 0.5020\n",
      "\n",
      "Epoch 159/300\n",
      "1/19, train_loss: 0.5020\n",
      "2/19, train_loss: 0.5020\n",
      "3/19, train_loss: 0.5020\n",
      "4/19, train_loss: 0.5020\n",
      "5/19, train_loss: 0.5020\n",
      "6/19, train_loss: 0.5020\n",
      "7/19, train_loss: 0.5020\n",
      "8/19, train_loss: 0.5020\n",
      "9/19, train_loss: 0.5020\n",
      "10/19, train_loss: 0.5020\n",
      "11/19, train_loss: 0.5020\n",
      "12/19, train_loss: 0.5020\n",
      "13/19, train_loss: 0.5020\n",
      "14/19, train_loss: 0.5020\n",
      "15/19, train_loss: 0.5020\n",
      "16/19, train_loss: 0.5020\n",
      "17/19, train_loss: 0.5020\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 159 average loss: 0.5020\n",
      "\n",
      "Epoch 160/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 160 average loss: 0.5019\n",
      "Current epoch: 160, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 161/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 161 average loss: 0.5019\n",
      "\n",
      "Epoch 162/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 162 average loss: 0.5019\n",
      "\n",
      "Epoch 163/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5019\n",
      "3/19, train_loss: 0.5019\n",
      "4/19, train_loss: 0.5019\n",
      "5/19, train_loss: 0.5019\n",
      "6/19, train_loss: 0.5019\n",
      "7/19, train_loss: 0.5019\n",
      "8/19, train_loss: 0.5019\n",
      "9/19, train_loss: 0.5019\n",
      "10/19, train_loss: 0.5019\n",
      "11/19, train_loss: 0.5019\n",
      "12/19, train_loss: 0.5019\n",
      "13/19, train_loss: 0.5019\n",
      "14/19, train_loss: 0.5019\n",
      "15/19, train_loss: 0.5019\n",
      "16/19, train_loss: 0.5019\n",
      "17/19, train_loss: 0.5019\n",
      "18/19, train_loss: 0.5019\n",
      "19/19, train_loss: 0.5019\n",
      "Epoch 163 average loss: 0.5019\n",
      "\n",
      "Epoch 164/300\n",
      "1/19, train_loss: 0.5019\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 164 average loss: 0.5018\n",
      "\n",
      "Epoch 165/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 165 average loss: 0.5018\n",
      "Current epoch: 165, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 166/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 166 average loss: 0.5018\n",
      "\n",
      "Epoch 167/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5018\n",
      "10/19, train_loss: 0.5018\n",
      "11/19, train_loss: 0.5018\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5018\n",
      "14/19, train_loss: 0.5018\n",
      "15/19, train_loss: 0.5018\n",
      "16/19, train_loss: 0.5018\n",
      "17/19, train_loss: 0.5018\n",
      "18/19, train_loss: 0.5018\n",
      "19/19, train_loss: 0.5018\n",
      "Epoch 167 average loss: 0.5018\n",
      "\n",
      "Epoch 168/300\n",
      "1/19, train_loss: 0.5018\n",
      "2/19, train_loss: 0.5018\n",
      "3/19, train_loss: 0.5018\n",
      "4/19, train_loss: 0.5018\n",
      "5/19, train_loss: 0.5018\n",
      "6/19, train_loss: 0.5018\n",
      "7/19, train_loss: 0.5018\n",
      "8/19, train_loss: 0.5018\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5018\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 168 average loss: 0.5017\n",
      "\n",
      "Epoch 169/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 169 average loss: 0.5017\n",
      "\n",
      "Epoch 170/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 170 average loss: 0.5017\n",
      "Current epoch: 170, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 171/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 171 average loss: 0.5017\n",
      "\n",
      "Epoch 172/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5017\n",
      "5/19, train_loss: 0.5017\n",
      "6/19, train_loss: 0.5017\n",
      "7/19, train_loss: 0.5017\n",
      "8/19, train_loss: 0.5017\n",
      "9/19, train_loss: 0.5017\n",
      "10/19, train_loss: 0.5017\n",
      "11/19, train_loss: 0.5017\n",
      "12/19, train_loss: 0.5017\n",
      "13/19, train_loss: 0.5017\n",
      "14/19, train_loss: 0.5017\n",
      "15/19, train_loss: 0.5017\n",
      "16/19, train_loss: 0.5017\n",
      "17/19, train_loss: 0.5017\n",
      "18/19, train_loss: 0.5017\n",
      "19/19, train_loss: 0.5017\n",
      "Epoch 172 average loss: 0.5017\n",
      "\n",
      "Epoch 173/300\n",
      "1/19, train_loss: 0.5017\n",
      "2/19, train_loss: 0.5017\n",
      "3/19, train_loss: 0.5017\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 173 average loss: 0.5016\n",
      "\n",
      "Epoch 174/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 174 average loss: 0.5016\n",
      "\n",
      "Epoch 175/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 175 average loss: 0.5016\n",
      "Current epoch: 175, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 176/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 176 average loss: 0.5016\n",
      "\n",
      "Epoch 177/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5016\n",
      "8/19, train_loss: 0.5016\n",
      "9/19, train_loss: 0.5016\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5016\n",
      "12/19, train_loss: 0.5016\n",
      "13/19, train_loss: 0.5016\n",
      "14/19, train_loss: 0.5016\n",
      "15/19, train_loss: 0.5016\n",
      "16/19, train_loss: 0.5016\n",
      "17/19, train_loss: 0.5016\n",
      "18/19, train_loss: 0.5016\n",
      "19/19, train_loss: 0.5016\n",
      "Epoch 177 average loss: 0.5016\n",
      "\n",
      "Epoch 178/300\n",
      "1/19, train_loss: 0.5016\n",
      "2/19, train_loss: 0.5016\n",
      "3/19, train_loss: 0.5016\n",
      "4/19, train_loss: 0.5016\n",
      "5/19, train_loss: 0.5016\n",
      "6/19, train_loss: 0.5016\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5016\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 178 average loss: 0.5015\n",
      "\n",
      "Epoch 179/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 179 average loss: 0.5015\n",
      "\n",
      "Epoch 180/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 180 average loss: 0.5015\n",
      "Current epoch: 180, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 181/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 181 average loss: 0.5015\n",
      "\n",
      "Epoch 182/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5015\n",
      "Epoch 182 average loss: 0.5015\n",
      "\n",
      "Epoch 183/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5015\n",
      "3/19, train_loss: 0.5015\n",
      "4/19, train_loss: 0.5015\n",
      "5/19, train_loss: 0.5015\n",
      "6/19, train_loss: 0.5015\n",
      "7/19, train_loss: 0.5015\n",
      "8/19, train_loss: 0.5015\n",
      "9/19, train_loss: 0.5015\n",
      "10/19, train_loss: 0.5015\n",
      "11/19, train_loss: 0.5015\n",
      "12/19, train_loss: 0.5015\n",
      "13/19, train_loss: 0.5015\n",
      "14/19, train_loss: 0.5015\n",
      "15/19, train_loss: 0.5015\n",
      "16/19, train_loss: 0.5015\n",
      "17/19, train_loss: 0.5015\n",
      "18/19, train_loss: 0.5015\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 183 average loss: 0.5015\n",
      "\n",
      "Epoch 184/300\n",
      "1/19, train_loss: 0.5015\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 184 average loss: 0.5014\n",
      "\n",
      "Epoch 185/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 185 average loss: 0.5014\n",
      "Current epoch: 185, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 186/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 186 average loss: 0.5014\n",
      "\n",
      "Epoch 187/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 187 average loss: 0.5014\n",
      "\n",
      "Epoch 188/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 188 average loss: 0.5014\n",
      "\n",
      "Epoch 189/300\n",
      "1/19, train_loss: 0.5014\n",
      "2/19, train_loss: 0.5014\n",
      "3/19, train_loss: 0.5014\n",
      "4/19, train_loss: 0.5014\n",
      "5/19, train_loss: 0.5014\n",
      "6/19, train_loss: 0.5014\n",
      "7/19, train_loss: 0.5014\n",
      "8/19, train_loss: 0.5014\n",
      "9/19, train_loss: 0.5014\n",
      "10/19, train_loss: 0.5014\n",
      "11/19, train_loss: 0.5014\n",
      "12/19, train_loss: 0.5014\n",
      "13/19, train_loss: 0.5014\n",
      "14/19, train_loss: 0.5014\n",
      "15/19, train_loss: 0.5014\n",
      "16/19, train_loss: 0.5014\n",
      "17/19, train_loss: 0.5014\n",
      "18/19, train_loss: 0.5014\n",
      "19/19, train_loss: 0.5014\n",
      "Epoch 189 average loss: 0.5014\n",
      "\n",
      "Epoch 190/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 190 average loss: 0.5013\n",
      "Current epoch: 190, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 191/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 191 average loss: 0.5013\n",
      "\n",
      "Epoch 192/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 192 average loss: 0.5013\n",
      "\n",
      "Epoch 193/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 193 average loss: 0.5013\n",
      "\n",
      "Epoch 194/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 194 average loss: 0.5013\n",
      "\n",
      "Epoch 195/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5013\n",
      "4/19, train_loss: 0.5013\n",
      "5/19, train_loss: 0.5013\n",
      "6/19, train_loss: 0.5013\n",
      "7/19, train_loss: 0.5013\n",
      "8/19, train_loss: 0.5013\n",
      "9/19, train_loss: 0.5013\n",
      "10/19, train_loss: 0.5013\n",
      "11/19, train_loss: 0.5013\n",
      "12/19, train_loss: 0.5013\n",
      "13/19, train_loss: 0.5013\n",
      "14/19, train_loss: 0.5013\n",
      "15/19, train_loss: 0.5013\n",
      "16/19, train_loss: 0.5013\n",
      "17/19, train_loss: 0.5013\n",
      "18/19, train_loss: 0.5013\n",
      "19/19, train_loss: 0.5013\n",
      "Epoch 195 average loss: 0.5013\n",
      "Current epoch: 195, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 196/300\n",
      "1/19, train_loss: 0.5013\n",
      "2/19, train_loss: 0.5013\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 196 average loss: 0.5012\n",
      "\n",
      "Epoch 197/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 197 average loss: 0.5012\n",
      "\n",
      "Epoch 198/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 198 average loss: 0.5012\n",
      "\n",
      "Epoch 199/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 199 average loss: 0.5012\n",
      "\n",
      "Epoch 200/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 200 average loss: 0.5012\n",
      "Current epoch: 200, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 201/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 201 average loss: 0.5012\n",
      "\n",
      "Epoch 202/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5012\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5012\n",
      "11/19, train_loss: 0.5012\n",
      "12/19, train_loss: 0.5012\n",
      "13/19, train_loss: 0.5012\n",
      "14/19, train_loss: 0.5012\n",
      "15/19, train_loss: 0.5012\n",
      "16/19, train_loss: 0.5012\n",
      "17/19, train_loss: 0.5012\n",
      "18/19, train_loss: 0.5012\n",
      "19/19, train_loss: 0.5012\n",
      "Epoch 202 average loss: 0.5012\n",
      "\n",
      "Epoch 203/300\n",
      "1/19, train_loss: 0.5012\n",
      "2/19, train_loss: 0.5012\n",
      "3/19, train_loss: 0.5012\n",
      "4/19, train_loss: 0.5012\n",
      "5/19, train_loss: 0.5012\n",
      "6/19, train_loss: 0.5012\n",
      "7/19, train_loss: 0.5012\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5012\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 203 average loss: 0.5011\n",
      "\n",
      "Epoch 204/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 204 average loss: 0.5011\n",
      "\n",
      "Epoch 205/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 205 average loss: 0.5011\n",
      "Current epoch: 205, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 206/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 206 average loss: 0.5011\n",
      "\n",
      "Epoch 207/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 207 average loss: 0.5011\n",
      "\n",
      "Epoch 208/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 208 average loss: 0.5011\n",
      "\n",
      "Epoch 209/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 209 average loss: 0.5011\n",
      "\n",
      "Epoch 210/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5011\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5011\n",
      "15/19, train_loss: 0.5011\n",
      "16/19, train_loss: 0.5011\n",
      "17/19, train_loss: 0.5011\n",
      "18/19, train_loss: 0.5011\n",
      "19/19, train_loss: 0.5011\n",
      "Epoch 210 average loss: 0.5011\n",
      "Current epoch: 210, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 211/300\n",
      "1/19, train_loss: 0.5011\n",
      "2/19, train_loss: 0.5011\n",
      "3/19, train_loss: 0.5011\n",
      "4/19, train_loss: 0.5011\n",
      "5/19, train_loss: 0.5011\n",
      "6/19, train_loss: 0.5011\n",
      "7/19, train_loss: 0.5011\n",
      "8/19, train_loss: 0.5011\n",
      "9/19, train_loss: 0.5011\n",
      "10/19, train_loss: 0.5011\n",
      "11/19, train_loss: 0.5011\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5011\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 211 average loss: 0.5011\n",
      "\n",
      "Epoch 212/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 212 average loss: 0.5010\n",
      "\n",
      "Epoch 213/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 213 average loss: 0.5010\n",
      "\n",
      "Epoch 214/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 214 average loss: 0.5010\n",
      "\n",
      "Epoch 215/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 215 average loss: 0.5010\n",
      "Current epoch: 215, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 216/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 216 average loss: 0.5010\n",
      "\n",
      "Epoch 217/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 217 average loss: 0.5010\n",
      "\n",
      "Epoch 218/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 218 average loss: 0.5010\n",
      "\n",
      "Epoch 219/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5010\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5010\n",
      "18/19, train_loss: 0.5010\n",
      "19/19, train_loss: 0.5010\n",
      "Epoch 219 average loss: 0.5010\n",
      "\n",
      "Epoch 220/300\n",
      "1/19, train_loss: 0.5010\n",
      "2/19, train_loss: 0.5010\n",
      "3/19, train_loss: 0.5010\n",
      "4/19, train_loss: 0.5010\n",
      "5/19, train_loss: 0.5010\n",
      "6/19, train_loss: 0.5010\n",
      "7/19, train_loss: 0.5010\n",
      "8/19, train_loss: 0.5010\n",
      "9/19, train_loss: 0.5010\n",
      "10/19, train_loss: 0.5010\n",
      "11/19, train_loss: 0.5010\n",
      "12/19, train_loss: 0.5010\n",
      "13/19, train_loss: 0.5010\n",
      "14/19, train_loss: 0.5010\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5010\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 220 average loss: 0.5010\n",
      "Current epoch: 220, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 221/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 221 average loss: 0.5009\n",
      "\n",
      "Epoch 222/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 222 average loss: 0.5009\n",
      "\n",
      "Epoch 223/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 223 average loss: 0.5009\n",
      "\n",
      "Epoch 224/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 224 average loss: 0.5009\n",
      "\n",
      "Epoch 225/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 225 average loss: 0.5009\n",
      "Current epoch: 225, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 226/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 226 average loss: 0.5009\n",
      "\n",
      "Epoch 227/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 227 average loss: 0.5009\n",
      "\n",
      "Epoch 228/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 228 average loss: 0.5009\n",
      "\n",
      "Epoch 229/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 229 average loss: 0.5009\n",
      "\n",
      "Epoch 230/300\n",
      "1/19, train_loss: 0.5009\n",
      "2/19, train_loss: 0.5009\n",
      "3/19, train_loss: 0.5009\n",
      "4/19, train_loss: 0.5009\n",
      "5/19, train_loss: 0.5009\n",
      "6/19, train_loss: 0.5009\n",
      "7/19, train_loss: 0.5009\n",
      "8/19, train_loss: 0.5009\n",
      "9/19, train_loss: 0.5009\n",
      "10/19, train_loss: 0.5009\n",
      "11/19, train_loss: 0.5009\n",
      "12/19, train_loss: 0.5009\n",
      "13/19, train_loss: 0.5009\n",
      "14/19, train_loss: 0.5009\n",
      "15/19, train_loss: 0.5009\n",
      "16/19, train_loss: 0.5009\n",
      "17/19, train_loss: 0.5009\n",
      "18/19, train_loss: 0.5009\n",
      "19/19, train_loss: 0.5009\n",
      "Epoch 230 average loss: 0.5009\n",
      "Current epoch: 230, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 231/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 231 average loss: 0.5008\n",
      "\n",
      "Epoch 232/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 232 average loss: 0.5008\n",
      "\n",
      "Epoch 233/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 233 average loss: 0.5008\n",
      "\n",
      "Epoch 234/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 234 average loss: 0.5008\n",
      "\n",
      "Epoch 235/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 235 average loss: 0.5008\n",
      "Current epoch: 235, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 236/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 236 average loss: 0.5008\n",
      "\n",
      "Epoch 237/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 237 average loss: 0.5008\n",
      "\n",
      "Epoch 238/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 238 average loss: 0.5008\n",
      "\n",
      "Epoch 239/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 239 average loss: 0.5008\n",
      "\n",
      "Epoch 240/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 240 average loss: 0.5008\n",
      "Current epoch: 240, current mean dice: 0.0000, best mean dice: 0.0000 at epoch 5\n",
      "\n",
      "Epoch 241/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n",
      "8/19, train_loss: 0.5008\n",
      "9/19, train_loss: 0.5008\n",
      "10/19, train_loss: 0.5008\n",
      "11/19, train_loss: 0.5008\n",
      "12/19, train_loss: 0.5008\n",
      "13/19, train_loss: 0.5008\n",
      "14/19, train_loss: 0.5008\n",
      "15/19, train_loss: 0.5008\n",
      "16/19, train_loss: 0.5008\n",
      "17/19, train_loss: 0.5008\n",
      "18/19, train_loss: 0.5008\n",
      "19/19, train_loss: 0.5008\n",
      "Epoch 241 average loss: 0.5008\n",
      "\n",
      "Epoch 242/300\n",
      "1/19, train_loss: 0.5008\n",
      "2/19, train_loss: 0.5008\n",
      "3/19, train_loss: 0.5008\n",
      "4/19, train_loss: 0.5008\n",
      "5/19, train_loss: 0.5008\n",
      "6/19, train_loss: 0.5008\n",
      "7/19, train_loss: 0.5008\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import monai\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.data import (\n",
    "    decollate_batch,\n",
    "    CacheDataset,\n",
    "    list_data_collate,\n",
    ")\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    EnsureTyped,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ConcatItemsd,\n",
    "    SaveImaged,\n",
    "    Invertd,\n",
    "    SpatialPadd\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set paths for your data\n",
    "# Set paths for your data\n",
    "t1_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/t1_masks\"\n",
    "t2_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/t2_masks\"\n",
    "labels_dir = \"/projectnb/cs585bp/students/econlin/VS_Seg-master/label_masks\"\n",
    "output_dir = \"./output\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create data dictionaries for MONAI\n",
    "def create_data_dicts(t1_dir, t2_dir, labels_dir):\n",
    "    t1_files = sorted([os.path.join(t1_dir, f) for f in os.listdir(t1_dir) if f.endswith('.nii.gz')])\n",
    "    t2_files = sorted([os.path.join(t2_dir, f) for f in os.listdir(t2_dir) if f.endswith('.nii.gz')])\n",
    "    label_files = sorted([os.path.join(labels_dir, f) for f in os.listdir(labels_dir) if f.endswith('.nii.gz')])\n",
    "    \n",
    "    # Ensure consistent file counts\n",
    "    assert len(t1_files) == len(t2_files) == len(label_files), \"Mismatch in file counts\"\n",
    "    \n",
    "    data_dicts = [\n",
    "        {\n",
    "            \"t1\": t1_file,\n",
    "            \"t2\": t2_file,\n",
    "            \"label\": label_file\n",
    "        }\n",
    "        for t1_file, t2_file, label_file in zip(t1_files, t2_files, label_files)\n",
    "    ]\n",
    "    return data_dicts\n",
    "\n",
    "# Define transforms for NIfTI files\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t1\", \"t2\", \"label\"]),  # Load NIfTI files\n",
    "    EnsureChannelFirstd(keys=[\"t1\", \"t2\", \"label\"]),\n",
    "    Orientationd(keys=[\"t1\", \"t2\", \"label\"], axcodes=\"RAS\"),  # Standardize orientation\n",
    "    Spacingd(keys=[\"t1\", \"t2\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\", \"nearest\")),\n",
    "    SpatialPadd(keys=[\"t1\", \"t2\", \"label\"], spatial_size=(96, 96, 96), mode=(\"constant\", \"constant\", \"constant\")),\n",
    "    ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "    ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),  # Concatenate T1 and T2 along channel dimension\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys=[\"t1\", \"t2\", \"label\"]),  # Load NIfTI files\n",
    "    EnsureChannelFirstd(keys=[\"t1\", \"t2\", \"label\"]),\n",
    "    Orientationd(keys=[\"t1\", \"t2\", \"label\"], axcodes=\"RAS\"),  # Standardize orientation\n",
    "    Spacingd(keys=[\"t1\", \"t2\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\", \"nearest\")),\n",
    "    SpatialPadd(keys=[\"t1\", \"t2\", \"label\"], spatial_size=(96, 96, 96), mode=(\"constant\", \"constant\", \"constant\")),\n",
    "    ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "    ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),  # Concatenate T1 and T2 along channel dimension\n",
    "    EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "])\n",
    "\n",
    "# Split dataset into training and validation\n",
    "def train_val_split(data_dicts, val_ratio=0.2):\n",
    "    n_val = int(len(data_dicts) * val_ratio)\n",
    "    n_train = len(data_dicts) - n_val\n",
    "    \n",
    "    train_files = data_dicts[:n_train]\n",
    "    val_files = data_dicts[n_train:]\n",
    "    \n",
    "    return train_files, val_files\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "data_dicts = create_data_dicts(t1_dir, t2_dir, labels_dir)\n",
    "train_files, val_files = train_val_split(data_dicts)\n",
    "\n",
    "# Using CacheDataset for better performance\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=list_data_collate)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4, collate_fn=list_data_collate)\n",
    "\n",
    "# Define model\n",
    "# Assuming you have n classes (including background)\n",
    "num_classes = 2  # Change to the number of segmentation classes + background\n",
    "\n",
    "# Create Swin-UNETR model (with 2 input channels for T1 and T2 concatenated)\n",
    "model = SwinUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=2,  # 2 channels for T1 and T2\n",
    "    out_channels=num_classes,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function, optimizer, and metrics\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "# Post-processing transforms\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=num_classes)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=num_classes)])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "val_interval = 5\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "writer = SummaryWriter(os.path.join(output_dir, 'logs'))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_loader)}, train_loss: {loss.item():.4f}\")\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    print(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    writer.add_scalar(\"train_loss\", epoch_loss, epoch + 1)\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            metric_sum = 0.0\n",
    "            metric_count = 0\n",
    "            \n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = val_data[\"image\"].to(device), val_data[\"label\"].to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                \n",
    "                # Compute metric\n",
    "                val_outputs_list = decollate_batch(val_outputs)\n",
    "                val_labels_list = decollate_batch(val_labels)\n",
    "                \n",
    "                val_outputs_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "                val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "                \n",
    "                dice_metric(y_pred=val_outputs_convert, y=val_labels_convert)\n",
    "                \n",
    "            metric = dice_metric.aggregate().item()\n",
    "            dice_metric.reset()\n",
    "            \n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n",
    "                print(\"Saved new best model\")\n",
    "                \n",
    "            print(f\"Current epoch: {epoch + 1}, current mean dice: {metric:.4f}, best mean dice: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "            writer.add_scalar(\"val_mean_dice\", metric, epoch + 1)\n",
    "            \n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()\n",
    "\n",
    "# Function to infer and save predictions as NIfTI files\n",
    "def infer_and_save(model, data_dicts, output_dir):\n",
    "    # Define inference transforms\n",
    "    infer_transforms = Compose([\n",
    "        LoadImaged(keys=[\"t1\", \"t2\"]),\n",
    "        EnsureChannelFirstd(keys=[\"t1\", \"t2\"]),\n",
    "        Orientationd(keys=[\"t1\", \"t2\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"t1\", \"t2\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"bilinear\")),\n",
    "        ScaleIntensityd(keys=[\"t1\", \"t2\"]),\n",
    "        ConcatItemsd(keys=[\"t1\", \"t2\"], name=\"image\", dim=0),\n",
    "        EnsureTyped(keys=[\"image\"]),\n",
    "    ])\n",
    "    \n",
    "    # Create a new dataset for inference (without labels)\n",
    "    infer_ds = CacheDataset(data=data_dicts, transform=infer_transforms, cache_rate=1.0, num_workers=4)\n",
    "    infer_loader = DataLoader(infer_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate)\n",
    "    \n",
    "    # Create output directory for predictions\n",
    "    pred_dir = os.path.join(output_dir, \"predictions\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    \n",
    "    # Post-transform to convert predictions back to NIfTI\n",
    "    post_transforms = Compose([\n",
    "        EnsureTyped(keys=\"pred\"),\n",
    "        AsDiscrete(argmax=True, to_onehot=num_classes),\n",
    "        # Add transforms to map predictions back to original space if needed\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"t1_meta_dict\", output_dir=pred_dir, output_postfix=\"seg\", resample=False, output_dtype=None),\n",
    "    ])\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(infer_loader):\n",
    "            print(f\"Processing case {i+1}/{len(infer_loader)}\")\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            \n",
    "            # Get the filename for saving\n",
    "            t1_path = batch_data[\"t1_meta_dict\"][\"filename_or_obj\"][0]\n",
    "            base_name = os.path.basename(t1_path)\n",
    "            \n",
    "            # Run inference\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Process outputs and save as NIfTI\n",
    "            outputs = outputs.argmax(dim=1, keepdim=True)\n",
    "            batch_data[\"pred\"] = outputs\n",
    "            \n",
    "            # Save the prediction as NIfTI\n",
    "            post_transforms(batch_data)\n",
    "            \n",
    "            print(f\"Saved prediction for {base_name}\")\n",
    "\n",
    "# Load the best model and run inference\n",
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n",
    "infer_and_save(model, val_files, output_dir)\n",
    "print(f\"Inference completed. Predictions saved to {os.path.join(output_dir, 'predictions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c914823d-8a34-4d47-9df6-909b521f4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from monai.networks.nets import SwinUNETR\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ddd8a22-d41f-4fa6-94af-87941426a279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff5b38b-8654-4473-b2a0-1caeff9e2801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ipykernel in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (6.29.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (1.8.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (8.26.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Installed kernelspec torch_env in /usr4/cs585bp/econlin/.local/share/jupyter/kernels/torch_env\n"
     ]
    }
   ],
   "source": [
    "# Install ipykernel in the environment where PyTorch is installed\n",
    "!pip install ipykernel\n",
    "# Then add it to Jupyter\n",
    "!python -m ipykernel install --user --name=torch_env --display-name \"Python (torch_env)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4872a111-37ce-4ddf-9e36-c5b06d16c58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: torch 2.6.0\n",
      "Uninstalling torch-2.6.0:\n",
      "  Successfully uninstalled torch-2.6.0\n",
      "Found existing installation: torchvision 0.21.0\n",
      "Uninstalling torchvision-0.21.0:\n",
      "  Successfully uninstalled torchvision-0.21.0\n",
      "Found existing installation: torchaudio 2.6.0\n",
      "Uninstalling torchaudio-2.6.0:\n",
      "  Successfully uninstalled torchaudio-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319b99ae-f8f5-471b-aafe-1ba220c2d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m547.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m567.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m451.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m575.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m448.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m531.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m721.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m508.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m525.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m555.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m583.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.3.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /share/pkg.8/python3/3.12.4/install/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (955.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m955.5/955.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m462.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m374.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/usr4/cs585bp/econlin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: sympy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/usr4/cs585bp/econlin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/usr4/cs585bp/econlin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr4/cs585bp/econlin/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589ff28c-db1b-4af3-9825-4392067560e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57dff12-40b5-4026-ab75-adf3bf916da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
